# Correlation and simple regression

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
library(tidyverse)
regression_data <- read.csv("Datasets/regression_data.csv")
```

<iframe src="https://teesside.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=c13b5ceb-47c7-4001-8a56-adaa008f19a2&amp;autoplay=false&amp;offerviewer=true&amp;showtitle=true&amp;showbrand=false&amp;captions=false&amp;interactivity=all" height="405" width="100%" style="border: 1px solid #464646;" allowfullscreen allow="autoplay">

</iframe>

## What is Correlation?

Correlation is a measure of the strength and direction of a relationship between two variables. It is most commonly used when we want to see if there is a relationship between two *continuous* variables. When we calculate correlation, we get a value between -1 and 1. The closer the value is to 1, the stronger the relationship. The closer the value is to 0, the weaker the relationship. We also often calculate the significance of the correlation, which tests against a null hypothesis that the correlation is 0.

## How is correlation calculated?

Correlation can be thought of as covariance divided by individual variance. Covariance is a measure of how much two variables change together. Variance is a measure of how much a variable changes on its own. When we divide covariance by variance, we get a value that is standardised and can be compared across different data sets.

If the changes are consistent with both variables (i.e. the covariance is higher and the individual variance is lower), then the final correlation value will be higher.

```{r out.width = "100%", echo=F}

knitr::include_graphics("img/correlation.png") 
```

```{r out.height = "100%", echo=F}

knitr::include_graphics("img/corcalc.png") 
```

## Which correlation to use?

When we run correlation in R, we use the *cor.test()* command. This command will give us the correlation value, the p value and the confidence intervals.

We can specify a Pearson correlation (the default) or a Spearman correlation (for non-parametric data).


### Running correlation in R

-   R can run correlations using the *cor.test()* command

```{r}
cor.test(regression_data$treatment_duration,regression_data$aggression_level) 
```

In the above example, we are testing the correlation between treatment duration and aggression level. Each variable is separated by a comma.

### Interpreting the output

-   The r value tells us the strength and direction of the relationship
-   In the output it is labelled as "cor" (short for correlation)

Correlation values can range from -1 to 1. The closer the value is to 1, the stronger the relationship. The closer the value is to 0, the weaker the relationship. Positive values indicate a positive relationship (i.e. as one variable increases, so does the other). Negative values indicate a negative relationship (i.e. as one variable increases, the other decreases).

```{r}
cor.test(regression_data$treatment_duration,regression_data$aggression_level)
```

### Check the significance of the correlation

-   We can see that the significance by looking at the p value
    -   The significance is 1.146\^-15
    -   This means: 0.0000000000000001146
-   Therefore p value \< 0.05

```{r}
cor.test(regression_data$treatment_duration,regression_data$aggression_level)
```

::: {.note .important}

## Exponent values

When we see a value like 1.146e-15, this is a shorthand way of writing a very small number. The e-15 means that we move the decimal point 15 places to the left. So 1.146e-15 is the same as 0.000000000000001146

:::

# Simple Regression

<iframe src="https://teesside.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=5e657c9f-3a06-4a74-b259-adaa0090242b&amp;autoplay=false&amp;offerviewer=true&amp;showtitle=true&amp;showbrand=false&amp;captions=false&amp;interactivity=all" height="405" width="100%" style="border: 1px solid #464646;" allowfullscreen allow="autoplay">

</iframe>

## What is regression?

-   Testing to see if we can make predictions based on data that are correlated

> We found a strong correlation between treatment duration and agression levels. Can we use this data to predict aggression levels of other clients, based on their treatment duration?

-   When we carry out regression, we get a information about:
    -   How much variance in the **outcome** is explained by the **predictor**
    -   How confident we can be about these results generalising (i.e. **significance**)
    -   How much error we can expect from anu predictions that we make (i.e. **standard error of the estimate**)
    -   The figures we need to calculate a predicted outcome value (i.e. **coefficient values**)

## How is regression calculated?

```{r echo=F}

knitr::include_graphics("img/bestfit.png") 
```

-   When we run a regression analysis, a calculation is done to select the "line of best fit"
-   This is a "prediction line" that minimises the overall amount of error
    -   Error = difference between the data points and the line

## The regression equation

```{r echo=F}

knitr::include_graphics("img/bestfit.png") 
```

-   Once the line of best fit is calculated, predictions are based on this line

-   To make predictions we need the **intercept** and **slope** of the line

    -   **Intercept** or **constant**= where the line crosses the y axis
    -   **Slope** or **beta** = the angle of the line

-   Predictions are made using the calculation for a line: **Y = bX + c**

-   You can think of the equation like this:

**predicted outcome value = beta coefficient \* value of predictor + constant**

## Running regression in R

-   Step 1: Run regression
-   Step 2: Check assumptions
    -   Data
    -   Distribution
    -   Linearity
    -   Homogeneity of variance
    -   Uncorrelated predictors
    -   Indpendence of residuals
    -   No influental cases / outliers
-   Step 3: Check R\^2 value
-   Step 4: Check model significance
-   Step 5: Check coefficient values

## Run regression

-   We use the *lm()* command to run regression while saving the results
-   We then use the *summary()* function to check the results

```{r}
model1 <- lm(formula= aggression_level ~ treatment_duration ,data=regression_data)
summary(model1)

```

## What are residuals?

-   In regression, the assumptions apply to the residuals, not the data themselves
-   Residual just means the difference between the data point and the regression line

```{r echo=F, width= '100%'}

knitr::include_graphics("img/residuals1.png") 
```

## Check assumptions: distribution

-   Using the *plot()* command on our regression model will give us some useful diagnostic plots
-   The second plot that it outputs shows the normality

```{r}

plot(model1, which=2)

```

-   We could also use a histogram to check the distribution
-   Notice how we can use the \$ sign to get the residuals from the model

```{r}
hist(model1$residuals)
```

## Check assumptions: linearity

-   Using the *plot()* command on our regression model will give us some useful diagnostic plots
-   The first plot that it outputs shows the residuals vs the fitted values
-   Here, we want to see them spread out, with the line being horizontal and straight

```{r}

plot(model1, which=1)

```

-   There is a slight amount of curvilinearity here but nothing to be worried about

## Check assumptions: Homogeneity of Variance #1

-   We can use the sample plot to check Homogeneity of Variance
-   We want the variance to be constant across the data set. We do not want the variance to change at different points in the data

```{r}

plot(model1, which=1)

```

-   A violation of Homogeneity of Variance would usually look like a funnel, with the data narrowing

```{r echo=F, out.height= "100%"}

knitr::include_graphics("img/biasedresiduals.png") 
```

## Check assumptions: Influential cases

-   We need to check that there are no extreme outliers - they could throw off our predictions
-   We are looking for participants that have high rediduals + high leverage
    -   Some guidance suggests anything higher than 1 is an influential case
    -   Others suggest 4/n is the cut off point (4 divided by number of participants)

```{r  out.width= "30%"}

plot(model1, which=4)

```

-   We are looking for participants that have high rediduals + high leverage
    -   No cases over 1
    -   Many are over 0.04 (4/n = 0.04)

```{r  out.width= "40%"}

plot(model1, which=5)

```

## Check the r squared value

-   r\^2 = the amount of variance in the **outcome** that is explained by the **predictor(s)**
-   The closer this value is to 1, the more useful our regression model is for predicting the outcome

```{r}
modelSummary <- summary(model1)
modelSummary

```

-   The r\^2 of `r modelSummary$r.squared` means that 48% of the variance in **aggression level** is explained by **treatment duration**

## Check model significance

-   The model significance is displayed at the very end of the output
    -   *p-value: 1.146e-15*
    -   As p \< 0.05, the model is significant

```{r}

modelSummary

```

## Check coefficient values

-   The coefficient values are displayed in the coefficients table
-   If we have more than one predictor, they are all listed here

```{r}

modelSummary$coefficients

```

-   The **beta coefficient** for treatment duration is in the *Estimate* column
-   For every unit increase in treatment duration, aggression level decreases by 0.69

## The regression equation

-   The regression equation is:

Outcome = predictor value \* beta coefficient + constant

-   For this model, that is:

Aggression level = treatment duration \* -0.69 + 12.33

```{r}

modelSummary$coefficients

```

## Accounting for error in predictions

-   We also know that the accuracy of predictions will be within a certain margin of error
-   This is known as **standard error of the estimate** or **residual standard error**

```{r}

modelSummary

```

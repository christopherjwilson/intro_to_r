% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tikz}{}{\usepackage{tikz}}
\makeatother
        \newcommand*\circled[1]{\tikz[baseline=(char.base)]{
          \node[shape=circle,draw,inner sep=1pt] (char) {{\scriptsize#1}};}}  
                  
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Introduction to R for Clinical Psychology},
  pdfauthor={Christopher J Wilson},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Introduction to R for Clinical Psychology}
\author{Christopher J Wilson}
\date{2024-12-12}
\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter*{Welcome}\label{welcome}
\addcontentsline{toc}{chapter}{Welcome}

\markboth{Welcome}{Welcome}

The purpose of this site/book is to introduce you to R and R Studio for
use in Clinical Psychology Research. Before you begin, there are some
important things to know:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  This site is not a comprehensive guide to everything that can be done
  in R. There are many resources available for learning R, and this site
  is just a starting point. It will cover the basics of R and R Studio,
  and will provide examples of how to use R for common tasks in clinical
  psychology research.
\item
  This site is not a comprehensive guide to statistics. It will cover
  some key statistical concepts and how to perform them in R, but it is
  not a compete substitute for a statistics textbook or course. The goal
  of this content is to cover some key statistical concepts that are
  pertinent to clinical psychology research. It is expected that you
  have some background in statistics, and that you are familiar with
  basic concepts such as hypothesis testing, p-values, and confidence
  intervals.
\item
  It is important to note that R is a powerful tool, but it can be
  complex and challenging to learn. It is normal to feel overwhelmed at
  times, and it is important to be patient with yourself as you learn.
  The more you practice and use R, the more comfortable you will become
  with it. Set your expectations accordingly, and remember that learning
  R is a process that takes a long time and should continue throughout
  your career. This is just your starting point.
\item
  This site is a work in progress. I am constantly updating, adding new
  content and removing some less useful elements, so please check back
  regularly for updates. If you have any feedback or suggestions for
  content, please let me know.
\end{enumerate}

\section*{The tidyverse}\label{tidyverse}
\addcontentsline{toc}{section}{The tidyverse}

\markright{The tidyverse}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-important-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-important-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{This site/book uses the tidyverse set of packages}, rightrule=.15mm, left=2mm]

The tidyverse is a collection of R packages designed for to make data
manipulation and visualization. easier in R. The tidyverse is a powerful
set of tools for data analysis, and it is widely used in the R
community. It is assumed that you will have the tidyverse installed and
loaded for the examples in this site/book. If you do not have the
tidyverse installed, you can install it by running the following code in
the R console:

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You only need to install the package on to your machine once. Once you
have installed the tidyverse, you can load it by running the following
code in the R console:

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

The \texttt{library()} function is used to load packages in R. You will
need to load the tidyverse package at the beginning of each R session in
which you want to use it.

\end{tcolorbox}

\section*{Textbooks that can be accessed
online}\label{textbooks-that-can-be-accessed-online}
\addcontentsline{toc}{section}{Textbooks that can be accessed online}

\markright{Textbooks that can be accessed online}

e-books can be accessed from the library website:
\url{https://www.tees.ac.uk/depts/lis/}

\subsection*{Research Methods and
Statistics}\label{research-methods-and-statistics}
\addcontentsline{toc}{subsection}{Research Methods and Statistics}

\href{https://ebookcentral.proquest.com/lib/tees/detail.action?docID=5584239}{Coolican,
2019. Research Methods and Statistics in Psychology. Taylor \& Francis
Group}

\href{https://ebookcentral.proquest.com/lib/tees/reader.action?docID=4038573}{Barker,
C., Pistrang, N., \& Elliott, R. (2015). Research methods in clinical
psychology: An introduction for students and practitioners (3rd ed.).
Chichester, West Sussex: Wiley Blackwell.}

\href{https://ebookcentral.proquest.com/lib/tees/detail.action?docID=918179}{Weiner,
I. B., Schinka, J. A., \& Velicer, W. F. (2012). Handbook of psychology,
research methods in psychology (2. Aufl. ed.). Somerset: Wiley.}

\subsection*{Working with R and RStudio to do
analysis}\label{working-with-r-and-rstudio-to-do-analysis}
\addcontentsline{toc}{subsection}{Working with R and RStudio to do
analysis}

\href{https://learningstatisticswithr.com/book/}{Navarro, D. (2017)
Learning statistics with R.}

\href{https://bookdown.org/ndphillips/YaRrr/}{Phillips N. D. (2018)
YaRrr! The Pirate's Guide to R}

\href{https://cran.r-project.org/doc/contrib/Horton+Pruim+Kaplan_MOSAIC-StudentGuide.pdf}{Horton,
Pruim and Kaplan (2015) A Student's Guide to R}

\href{https://bookdown.org/marius_mather/Rad/}{Mather, M. (2019) R for
Academics}

\href{https://r4ds.had.co.nz/}{Wickham and Grolemund (2019). R For Data
Science}

\href{https://bookdown.org/yihui/rmarkdown/}{Allaire and Grolemund
(2019). R Markdown: The Definitive Guide}

\href{https://github.com/rstudio/cheatsheets/raw/master/rstudio-ide.pdf}{Basics
of RStudio}

\href{https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf}{Data
Import}

\href{https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf}{Data
Transformation}

\href{https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf}{Data
Visualisation with GGPlot}

\bookmarksetup{startatroot}

\chapter{Introduction to R and R
Studio}\label{introduction-to-r-and-r-studio}

\section{What are R and R Studio?}\label{what-are-r-and-r-studio}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{At the end of this section, you will be able to:}, rightrule=.15mm, left=2mm]

\begin{itemize}
\tightlist
\item
  Download and install R and R Studio
\item
  Understand the basic layout of R Studio
\item
  Describe some of the differences between SPSS and R
\end{itemize}

\end{tcolorbox}

\subsection{Downloading and installing R and R
Studio}\label{downloading-and-installing-r-and-r-studio}

To get started with R Studio, you need to download and install two
pieces of software:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{R}: The base software that you will use to write and run code.
\item
  \textbf{R Studio}: An integrated development environment (IDE) that
  makes it easier to write and run code in R.
\end{enumerate}

Click on these links to download:

\begin{itemize}
\tightlist
\item
  \href{https://cran.r-project.org/}{R project}
\item
  \href{https://rstudio.com/}{RStudio}
\end{itemize}

\subsection{The R Studio layout}\label{the-r-studio-layout}

When you open R Studio, you will see a screen that looks like this:

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{images/rstudio_ide.png}}

}

\caption{R Studio IDE}

\end{figure}%

Briefly, the different panes in R Studio are:

\begin{itemize}
\tightlist
\item
  \textbf{Console}: You can write and run code in this pane. However, it
  is best practice to write code in a script. You will see output from
  your code in the console.
\item
  \textbf{Environment/History}: This pane shows you the objects that you
  have created in R, and the history of the commands that you have run.
\item
  \textbf{Files/Plots/Packages/Help}: These panes allow you to navigate
  your files, view plots, manage packages, and access help
  documentation.
\end{itemize}

You will learn more about these panes as you work through the course.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{images/rstudio1.png}}

}

\caption{R Studio IDE}

\end{figure}%

\subsection{Differences between SPSS and
R}\label{differences-between-spss-and-r}

R is a statistical programming language, while SPSS is a point-and-click
software package. This means that in R, you write code to perform tasks,
while in SPSS, you click buttons and select options from menus.

This can take some getting used to, but there are many advantages to
using R:

\begin{itemize}
\tightlist
\item
  \textbf{Reproducibility}: You can save your code and rerun it at any
  time, ensuring that your analysis is reproducible.
\item
  \textbf{Flexibility}: You can write code to perform any task you like,
  rather than being limited to the options available in a menu.
\item
  \textbf{Community}: R has a large and active community of users who
  share code and help each other to solve problems.
\end{itemize}

With R, you won't manipulate your source data files. Instead, you load
the data into R and manipulate it in R. This means that you can always
go back to your original data and start again if you need to.

\section{No more ``point and click''! - the R
workflow.}\label{no-more-point-and-click---the-r-workflow.}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{At the end of this section, you will be able to:}, rightrule=.15mm, left=2mm]

\begin{itemize}
\tightlist
\item
  Open a new script in R Studio
\item
  Write and run code in a script
\item
  Save a script for later use
\end{itemize}

\end{tcolorbox}

\subsection{Using scripts in R Studio}\label{using-scripts-in-r-studio}

When you work in R, you will write code in a script. This is a text file
that contains the code that you want to run. You can write and run code
in the console, but it is best practice to write code in a script. This
allows you to save your code and run it again later IT also makes it
easier to see what you have done.

To open a new script in R Studio, click on
\texttt{File\ \textgreater{}\ New\ File\ \textgreater{}\ R\ Script}.
This will open a new script in the top-left pane of R Studio.

You can write code in the script, and then run it by selecting the code
that you want to run and clicking the \texttt{Run} button at the top of
the script pane. You can also run code by pressing
\texttt{Ctrl\ +\ Enter} on your keyboard.

To save your script, click on \texttt{File\ \textgreater{}\ Save\ As...}
and save the file with a \texttt{.R} extension.

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-important-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-important-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Organising your work}, rightrule=.15mm, left=2mm]

It is good practice to keep your work organised by putting your scripts,
data, and other files in a folder on your computer, for each project
that you work on.

RStudio also allows the creation of projects. You can create a new
project in R Studio by clicking on
\texttt{File\ \textgreater{}\ New\ Project...}. This will create a new
folder on your computer where you can save your scripts, data, and other
files. If you save your script in the project folder, you can easily
access it by opening the project in R Studio. If you use projects, be
aware that R Studio will load the last project you worked on when you
open the software.

\end{tcolorbox}

\section{Objects, functions and packages in
R}\label{objects-functions-and-packages-in-r}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{At the end of this section, you will be able to:}, rightrule=.15mm, left=2mm]

\begin{itemize}
\tightlist
\item
  Create objects in R
\item
  Use functions in R to perform tasks
\item
  Install and load packages in R
\end{itemize}

\end{tcolorbox}

\subsection{What are objects?}\label{what-are-objects}

In R, you can create objects to store data. For example, you can create
an object called \texttt{numbers} that contains a set of numbers like
this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{numbers }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Breaking this code down:

\begin{itemize}
\item
  \texttt{numbers} is the name of the object that you are creating.
\item
  \texttt{\textless{}-} is the assignment operator. It assigns the value
  on the right-hand side of the operator to the object on the left-hand
  side.
\item
  \texttt{c(1,\ 2,\ 3,\ 4,\ 5)} is the data that you are assigning to
  the object. In this case, it is a set of numbers.
\end{itemize}

When you run this code, R will create an object called \texttt{numbers}
that contains the numbers 1, 2, 3, 4, and 5. You will be ab` le to see
the object in the Environment pane in R Studio.

You can then use the object in your code, instead of typing out the data
each time (see Section~\ref{sec-functions} for example).

\subsection{What are functions?}\label{sec-functions}

Functions are code that have been written to perform a specific task.
You can use functions in R to perform tasks like reading data into R,
summarising data, and creating plots.

For example, the \texttt{mean()} function calculates the mean of a set
of numbers. You can use the \texttt{mean()} function like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{numbers }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{)}

\FunctionTok{mean}\NormalTok{(numbers)}
\end{Highlighting}
\end{Shaded}

Functions in R have a name, followed by parentheses. You can pass
arguments to the function inside the parentheses. In this case, the
\texttt{mean()} function takes a set of numbers as an argument, and
returns the mean of those numbers.

To learn more about a function, you can use the \texttt{help()}
function. For example, to learn more about the \texttt{mean()} function,
you can run the following code:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{help}\NormalTok{(mean)}
\end{Highlighting}
\end{Shaded}

You can also use the \texttt{?} operator to get help on a function. For
example, to get help on the \texttt{mean()} function, you can run the
following code:

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{?mean}
\end{Highlighting}
\end{Shaded}

\subsection{What are packages?}\label{what-are-packages}

R has many built-in functions that you can use to perform tasks.
However, there are also many packages available that contain additional
functions. You can install these packages onto your conputer and then
load them into your R session whenever you want to use them.

To install a package, you can use the \texttt{install.packages()}
function. For example, to install the \texttt{tidyverse} package, you
would run the following code:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To load a package into your R session, you can use the
\texttt{library()} function. For example, to load the \texttt{tidyverse}
package, you would run the following code:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

Once you have loaded a package, you can use the functions in that
package in your code. For example, the \texttt{tidyverse} package
contains functions for data manipulation and visualisation.

\bookmarksetup{startatroot}

\chapter{Working with data in R
Studio}\label{working-with-data-in-r-studio}

Working with your data in RStudio is a little bit different from working
with data in a spreadsheet, for example. One crucial difference is that
you need to be explicit about what you want to do with your data. In a
spreadsheet, you can simply click on a cell and start typing. In R, you
need to tell the software what you want to do with your data. This can
be a bit intimidating at first, but it is also one of the most powerful
features of R. It allows you to automate repetitive tasks and perform
complex analyses with just a few lines of code.

\section{Importing data into R}\label{importing-data-into-r}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{At the end of this section, you will be able to:}, rightrule=.15mm, left=2mm]

\begin{itemize}
\tightlist
\item
  Load data into R from different file types
\item
  Understand the structure of data in R
\end{itemize}

\end{tcolorbox}

There are a few different ways to load data into R. You can load data
from a file on your computer, from a URL, or from a package. You can
load data in different file types, such as CSV, Excel, and SPSS files.

Using RStudio, you can load data by clicking on
\texttt{File\ \textgreater{}\ Import\ Dataset}. This will open a window
where you can select the file that you want to load.

However, you can also load data using code. For example, you can use the
\texttt{read\_csv()} function from the \texttt{readr} package to load a
CSV file into R. You can use the \texttt{readxl} package to load an
Excel file, and the \texttt{haven} package to load an SPSS file.

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Load the readr package}

\FunctionTok{library}\NormalTok{(readr)}

\CommentTok{\# Load a CSV file into R}

\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's break this code down:

\begin{itemize}
\item
  \texttt{library(readr)} loads the \texttt{readr} package into your R
  session. This package contains the \texttt{read\_csv()} function,
  which you can use to load a CSV file into R.
\item
  \texttt{read\_csv("data.csv")} reads the CSV file called
  \texttt{data.csv} into R. The data will be stored in an object called
  \texttt{data}.
\end{itemize}

When you load data into R, it will be stored as a data frame. A data
frame is a type of object in R that is used to store tabular data. It is
similar to a spreadsheet in Excel, with rows and columns.

\section{How are data stored in R?}\label{how-are-data-stored-in-r}

If you worked through the previous section, you should already have some
idea how to load data into R. But how are data stored in R? In R, data
are stored in objects. An object is a container that holds data. There
are several types of objects in R, but the most common ones are:

\begin{itemize}
\tightlist
\item
  Vectors (e.g., a sequence of numbers)
\item
  Matrices (e.g., a table of rows and colummns, all of the same data
  type)
\item
  Data frames (e.g., a table of data where each column represents a
  variable and each row represents an observation)
\item
  Lists (e.g., a collection of objects)
\end{itemize}

In this section, we will focus on data frames, which are the most common
way to store data in R. A data frame is a table of data where each
column represents a variable and each row represents an observation. You
can think of a data frame as having a structure similar to a
spreadsheet.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{images/dataframe.png}}

}

\caption{Data frame with 3 variables/columns}

\end{figure}%

\section{How do we use data frames in
R?}\label{how-do-we-use-data-frames-in-r}

To view the data in a data frame, you can simply type the name of the
data frame in the console and press Enter. For example, if you have a
data frame called \texttt{my\_data}, you can view the data in the data
frame by typing \texttt{my\_data} in the console and pressing Enter.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# load the tidyverse package}

\FunctionTok{library}\NormalTok{(tidyverse)}

\CommentTok{\# Create a data frame}
\NormalTok{my\_data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{name =} \FunctionTok{c}\NormalTok{(}\StringTok{"Alice"}\NormalTok{, }\StringTok{"Bob"}\NormalTok{, }\StringTok{"Charlie"}\NormalTok{, }\StringTok{"David"}\NormalTok{, }\StringTok{"Eve"}\NormalTok{, }\StringTok{"Frank"}\NormalTok{),}
  \AttributeTok{age =} \FunctionTok{c}\NormalTok{(}\DecValTok{25}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{45}\NormalTok{, }\DecValTok{50}\NormalTok{),}
  \AttributeTok{height =} \FunctionTok{c}\NormalTok{(}\DecValTok{160}\NormalTok{, }\DecValTok{175}\NormalTok{, }\DecValTok{180}\NormalTok{, }\DecValTok{165}\NormalTok{, }\DecValTok{170}\NormalTok{, }\DecValTok{190}\NormalTok{),}
  \AttributeTok{car =} \FunctionTok{c}\NormalTok{(}\StringTok{"Electric"}\NormalTok{, }\StringTok{"Petrol"}\NormalTok{, }\StringTok{"Electric"}\NormalTok{, }\StringTok{"Petrol"}\NormalTok{, }\StringTok{"Petrol"}\NormalTok{, }\StringTok{"Electric"}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# View or refer to the data in the data frame}

\NormalTok{my\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     name age height      car
1   Alice  25    160 Electric
2     Bob  30    175   Petrol
3 Charlie  35    180 Electric
4   David  40    165   Petrol
5     Eve  45    170   Petrol
6   Frank  50    190 Electric
\end{verbatim}

In the code above, we created a data frame called \texttt{my\_data} with
four variables: \texttt{name}, \texttt{age}, \texttt{height}, and
\texttt{car}. We then used the \texttt{my\_data} object to view the data
in the data frame.

\section{View or refer to a specific variable in a data
frame}\label{view-or-refer-to-a-specific-variable-in-a-data-frame}

To view or refer to a specific variable in a data frame, you can use the
\texttt{\$} operator. For example, if you want to view the \texttt{age}
variable in the \texttt{my\_data} data frame, you can type
\texttt{my\_data\$age} in the console and press Enter.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# View or refer to a specific variable in a data frame}

\NormalTok{my\_data}\SpecialCharTok{$}\NormalTok{age}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 25 30 35 40 45 50
\end{verbatim}

\section{Data types in R}\label{data-types-in-r}

In R, each variable in a data frame has a data type. The most common
data types in R are:

\begin{itemize}
\tightlist
\item
  Numeric: for continuous variables (e.g., age, height)
\item
  Factor: for categorical variables
\item
  Logical: for binary variables (TRUE or FALSE)
\end{itemize}

You can use the \texttt{str()} function to view the structure of a data
frame, including the data types of each variable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# View the structure of a data frame}

\FunctionTok{str}\NormalTok{(my\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   6 obs. of  4 variables:
 $ name  : chr  "Alice" "Bob" "Charlie" "David" ...
 $ age   : num  25 30 35 40 45 50
 $ height: num  160 175 180 165 170 190
 $ car   : chr  "Electric" "Petrol" "Electric" "Petrol" ...
\end{verbatim}

In the code above, we used the \texttt{str()} function to view the
structure of the \texttt{my\_data} data frame. The output shows the data
types of each variable in the data frame.

\section{Convert data types in R}\label{convert-data-types-in-r}

You can convert the data type of a variable in R using the \texttt{as.}
functions. For example, you can convert a character variable to a factor
variable using the \texttt{as.factor()} function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Convert a character variable to a factor variable}

\NormalTok{my\_data}\SpecialCharTok{$}\NormalTok{name }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{name)}

\NormalTok{my\_data}\SpecialCharTok{$}\NormalTok{car }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{car)}
\end{Highlighting}
\end{Shaded}

In the code above, we converted the \texttt{name} variable in the
\texttt{my\_data} data frame from a character variable to a factor
variable using the \texttt{as.factor()} function.

\section{Subsetting data in R}\label{subsetting-data-in-r}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{At the end of this section, you will be able to:}, rightrule=.15mm, left=2mm]

\begin{itemize}
\tightlist
\item
  Filter data in R
\item
  Create subsets of data in R
\end{itemize}

\end{tcolorbox}

Subsetting data in R means selecting a subset of the data based on
certain criteria. For example, you might want to select only the rows
where a certain variable is greater than a certain value, or only the
columns that contain certain variables.

If we use the \texttt{my\_data} data frame from the previous section, we
can subset the data to select only the rows where the \texttt{age}
variable is greater than 30.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter the data frame to select only the rows where the age variable is greater than 30}

\CommentTok{\# this method uses the dplyr package, which is a part of the tidyverse. Be sure to load the tidyverse package if you haven\textquotesingle{}t already.}

\NormalTok{my\_data }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(age }\SpecialCharTok{\textgreater{}} \DecValTok{30}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     name age height      car
1 Charlie  35    180 Electric
2   David  40    165   Petrol
3     Eve  45    170   Petrol
4   Frank  50    190 Electric
\end{verbatim}

Let's break this code down:

\begin{itemize}
\tightlist
\item
  \texttt{my\_data} is the data frame that we want to subset.
\item
  \texttt{\%\textgreater{}\%} is the pipe operator, which is used to
  pass the data frame to the next function. This allows us to link
  multiple steps together in a single line of code.
\item
  \texttt{filter(age\ \textgreater{}\ 30)} is the function that filters
  the data frame to select only the rows where the \texttt{age} variable
  is greater than 30.
\end{itemize}

The output of this code will be a new data frame that contains only the
rows where the \texttt{age} variable is greater than 30. However, this
new data frame will not be saved anywhere, so if you want to save it,
you need to assign it to a new object. Tp do this, you can use the
assignment operator \texttt{\textless{}-}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter the data frame to select only the rows where the age variable is greater than 30 and save the result to a new data frame called new\_data}

\NormalTok{new\_data }\OtherTok{\textless{}{-}}\NormalTok{ my\_data }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(age }\SpecialCharTok{\textgreater{}} \DecValTok{30}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In this code, on the left side of the assignment operator
\texttt{\textless{}-}, we have \texttt{new\_data}, which is the name of
the new data frame that will contain only the filtered subset of the
data (i.e., the values where the \texttt{age} variable is greater than
30). The difference between this code and the previous code is that we
are now saving the result to a new data frame called \texttt{new\_data},
instead of just printing it to the console.

We can also combine multiple conditions when subsetting data. For
example, we can select only the rows where the \texttt{age} variable is
greater than 25 and the \texttt{height} variable is greater than 175.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter the data frame to select only the rows where the age variable is greater than 25 and the height variable is greater than 175}

\NormalTok{my\_data }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(age }\SpecialCharTok{\textgreater{}} \DecValTok{25} \SpecialCharTok{\&}\NormalTok{ height }\SpecialCharTok{\textgreater{}} \DecValTok{175}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     name age height      car
1 Charlie  35    180 Electric
2   Frank  50    190 Electric
\end{verbatim}

There are many other ways to subset data in R, depending on the criteria
you want to use. For example, you can use the \texttt{select()} function
to select specific columns, the \texttt{arrange()} function to sort the
data, and the \texttt{mutate()} function to create new variables. We
will cover some of these functions in later sections.

\section{Grouping and summarising data in
R}\label{grouping-and-summarising-data-in-r}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{At the end of this section, you will be able to:}, rightrule=.15mm, left=2mm]

\begin{itemize}
\tightlist
\item
  Group data in R
\item
  Summarise data in R
\end{itemize}

\end{tcolorbox}

Grouping and summarising data in R means grouping the data by one or
more variables and then calculating summary statistics for each group.
For example, you might want to calculate the mean age for each group of
people based on their height.

If we use the \texttt{my\_data} data frame from the previous section, we
can group the data by the \texttt{car} variable and then calculate the
mean age for each group.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Group the data frame by the car variable and calculate the mean age for each group}

\NormalTok{my\_data }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{(car) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean\_age =} \FunctionTok{mean}\NormalTok{(age)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 2
  car      mean_age
  <fct>       <dbl>
1 Electric     36.7
2 Petrol       38.3
\end{verbatim}

Let's break this code down:

\begin{itemize}
\tightlist
\item
  \texttt{my\_data} is the data frame that we want to group and
  summarise.
\item
  \texttt{\%\textgreater{}\%} is the pipe operator, which is used to
  pass the data frame to the next function. This allows us to link
  multiple steps together in a single line of code.
\item
  \texttt{group\_by(car)} is the function that groups the data frame by
  the \texttt{car} variable.
\item
  \texttt{summarise(mean\_age\ =\ mean(age))} is the function that
  calculates the mean age for each group of cars. The \texttt{mean\_age}
  variable is the name of the new variable that will contain the mean
  age for each group.
\item
  \texttt{ungroup()} is the function that removes the grouping from the
  data frame. This is optional, but it is good practice to ungroup the
  data frame after you have finished summarising it.
\end{itemize}

The output of this code will be a new data frame that contains the mean
age for each group of cars. The \texttt{car} variable is the grouping
variable, and the \texttt{mean\_age} variable is the summary statistic
that we calculated for each group.

You can also calculate other summary statistics, such as the median,
standard deviation, minimum, and maximum, using the \texttt{summarise()}
function. You can also calculate multiple summary statistics at the same
time by specifying multiple variables inside the \texttt{summarise()}
function. For example, you can calculate the mean and standard deviation
of the age variable for each group of cars.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Group the data frame by the car variable and calculate the mean and standard deviation of the age variable for each group}

\NormalTok{my\_data }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{(car) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean\_age =} \FunctionTok{mean}\NormalTok{(age), }\AttributeTok{sd\_age =} \FunctionTok{sd}\NormalTok{(age)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 3
  car      mean_age sd_age
  <fct>       <dbl>  <dbl>
1 Electric     36.7  12.6 
2 Petrol       38.3   7.64
\end{verbatim}

In this code, we calculated the mean and standard deviation of the
\texttt{age} variable for each group of cars. The \texttt{mean\_age} and
\texttt{sd\_age} variables are the names of the new variables that will
contain the mean and standard deviation for each group.

\bookmarksetup{startatroot}

\chapter{Exploratory and descriptive
analysis}\label{exploratory-and-descriptive-analysis}

In this section, we will cover the basics of exploratory and descriptive
analysis in R. We will learn how to conduct some of the most common
descriptive statistics, such as mean, median, mode, standard deviation,
and variance. We will also look at basic distribution plots, such as
histograms and box plots, to visualize the data.

\section{Mean, median, and mode}\label{mean-median-and-mode}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{At the end of this section, you will be able to:}, rightrule=.15mm, left=2mm]

\begin{itemize}
\tightlist
\item
  Calculate the mean, median, and mode of a dataset
\end{itemize}

\end{tcolorbox}

For this section we will use the \texttt{album\_sales} dataset, which we
have already loaded in some of the videos. Let's start by loading the
dataset and displaying the first few rows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\CommentTok{\# Load the album\_sales dataset. The location of the dataset will be different based on where you saved it on your computer.}

\NormalTok{album\_sales }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"Datasets/album\_sales.csv"}\NormalTok{)}

\CommentTok{\# Display the first few rows of the dataset}

\FunctionTok{head}\NormalTok{(album\_sales)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   Adverts Sales Airplay Attract   Genre
1   10.256   330      43      10 Country
2  985.685   120      28       7     Pop
3 1445.563   360      35       7  HipHop
4 1188.193   270      33       7  HipHop
5  574.513   220      44       5   Metal
6  568.954   170      19       5 Country
\end{verbatim}

The dataset contains 5 variables: Adverts, Sales, Airplay, Attract and
Genre. We will focus on the \texttt{Sales} variable for this section.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the mean of the Sales variable}

\NormalTok{mean\_sales }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(album\_sales}\SpecialCharTok{$}\NormalTok{Sales)}

\NormalTok{mean\_sales}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 193.2
\end{verbatim}

Let's break down the code above:

\begin{itemize}
\item
  We used the \texttt{mean()} function to calculate the mean of the
  \texttt{Sales} variable in the \texttt{album\_sales} dataset. To do
  this, we specified the dataset \texttt{album\_sales} and the variable
  \texttt{Sales} using the \texttt{\$} operator.
\item
  The mean sales value is stored in the \texttt{mean\_sales} variable.
\end{itemize}

Next, let's calculate the median of the \texttt{Sales} variable:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the median of the Sales variable}

\NormalTok{median\_sales }\OtherTok{\textless{}{-}} \FunctionTok{median}\NormalTok{(album\_sales}\SpecialCharTok{$}\NormalTok{Sales)}

\NormalTok{median\_sales}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 200
\end{verbatim}

The code above calculates the median of the \texttt{Sales} variable in
the \texttt{album\_sales} dataset. The median sales value is stored in
the \texttt{median\_sales} variable.

Finally, let's calculate the mode of the \texttt{Sales} variable.
Unfortunately, R does not have a built-in function to calculate the
mode. However, we do this in the following way:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the mode of the Sales variable}

\NormalTok{album\_sales}\SpecialCharTok{$}\NormalTok{Sales }\SpecialCharTok{\%\textgreater{}\%}
 \FunctionTok{table}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
.
 10  30  40  50  60  70  80  90 100 110 120 130 140 150 160 170 180 190 200 210 
  1   1   3   1   5   6   3   4   8   5  10   3  11  12   5   4   8   8   7  13 
220 230 240 250 260 270 280 290 300 310 320 330 340 360 
  6  17   7  10   3   3   5   8   6   2   4   2   3   6 
\end{verbatim}

We can see from the output that the mode of the \texttt{Sales} variable
is 210, since that value appears most frequently in the dataset (13
times).

\section{Standard deviation and
variance}\label{standard-deviation-and-variance}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{At the end of this section, you will be able to:}, rightrule=.15mm, left=2mm]

\begin{itemize}
\tightlist
\item
  Calculate the standard deviation
\item
  Calculate the variance
\item
  Calculate the range of a dataset
\item
  Calculate the interquartile range (IQR)
\end{itemize}

\end{tcolorbox}

Next, let's calculate the standard deviation and variance of the
\texttt{Sales} variable in the \texttt{album\_sales} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the standard deviation of the Sales variable}

\NormalTok{sd\_sales }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(album\_sales}\SpecialCharTok{$}\NormalTok{Sales)}

\NormalTok{sd\_sales}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 80.69896
\end{verbatim}

The code above calculates the standard deviation of the \texttt{Sales}
variable in the \texttt{album\_sales} dataset. The standard deviation
value is stored in the \texttt{sd\_sales} variable.

Next, let's calculate the variance of the \texttt{Sales} variable:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the variance of the Sales variable}

\NormalTok{var\_sales }\OtherTok{\textless{}{-}} \FunctionTok{var}\NormalTok{(album\_sales}\SpecialCharTok{$}\NormalTok{Sales)}

\NormalTok{var\_sales}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 6512.322
\end{verbatim}

The code above calculates the variance of the \texttt{Sales} variable in
the \texttt{album\_sales} dataset. The variance value is stored in the
\texttt{var\_sales} variable.

\section{Range and interquartile
range}\label{range-and-interquartile-range}

The range of a dataset is the difference between the maximum and minimum
values. Let's calculate the range of the \texttt{Sales} variable in the
\texttt{album\_sales} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the range of the Sales variable}

\NormalTok{range\_sales }\OtherTok{\textless{}{-}} \FunctionTok{range}\NormalTok{(album\_sales}\SpecialCharTok{$}\NormalTok{Sales)}

\NormalTok{range\_sales}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1]  10 360
\end{verbatim}

The code above calculates the range of the \texttt{Sales} variable in
the \texttt{album\_sales} dataset. The range of the sales values is
stored in the \texttt{range\_sales} variable.

The interquartile range (IQR) is the difference between the 75th
percentile (Q3) and the 25th percentile (Q1) of a dataset. Let's
calculate the IQR of the \texttt{Sales} variable in the
\texttt{album\_sales} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the interquartile range of the Sales variable}

\NormalTok{IQR\_sales }\OtherTok{\textless{}{-}} \FunctionTok{IQR}\NormalTok{(album\_sales}\SpecialCharTok{$}\NormalTok{Sales)}

\NormalTok{IQR\_sales}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 112.5
\end{verbatim}

The code above calculates the interquartile range (IQR) of the
\texttt{Sales} variable in the \texttt{album\_sales} dataset. The IQR
value is stored in the \texttt{IQR\_sales} variable.

\section{Distribution plots}\label{distribution-plots}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{At the end of this section, you will be able to:}, rightrule=.15mm, left=2mm]

\begin{itemize}
\item
  Create a histogram to visualize the distribution of a dataset
\item
  Create a box plot to visualize the distribution of a dataset
\end{itemize}

\end{tcolorbox}

Next, let's create a histogram to visualize the distribution of the
\texttt{Sales} variable in the \texttt{album\_sales} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a histogram of the Sales variable}

\FunctionTok{hist}\NormalTok{(album\_sales}\SpecialCharTok{$}\NormalTok{Sales, }\AttributeTok{main =} \StringTok{"Histogram of Sales"}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"Sales"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Frequency"}\NormalTok{, }\AttributeTok{col =} \StringTok{"lightblue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{descriptive_stats_files/figure-pdf/unnamed-chunk-9-1.pdf}}

The code above creates a histogram of the \texttt{Sales} variable in the
\texttt{album\_sales} dataset. The histogram displays the frequency of
sales values in the dataset. The only required argument for the
\texttt{hist()} function is the variable you want to plot. The
\texttt{main}, \texttt{xlab}, \texttt{ylab}, and \texttt{col} arguments
are optional and allow you to customize the appearance of the histogram.

Finally, let's create a box plot to visualize the distribution of the
\texttt{Sales} variable in the \texttt{album\_sales} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a box plot of the Sales variable}

\FunctionTok{boxplot}\NormalTok{(album\_sales}\SpecialCharTok{$}\NormalTok{Sales, }\AttributeTok{main =} \StringTok{"Boxplot of Sales"}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"Sales"}\NormalTok{, }\AttributeTok{col =} \StringTok{"lightblue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{descriptive_stats_files/figure-pdf/unnamed-chunk-10-1.pdf}}

The code above creates a box plot of the \texttt{Sales} variable in the
\texttt{album\_sales} dataset. The box plot displays the distribution of
sales values, including the median, quartiles, and outliers. The only
required argument for the \texttt{boxplot()} function is the variable
you want to plot. The \texttt{main}, \texttt{xlab}, and \texttt{col}
arguments are optional and allow you to customize the appearance of the
box plot.

We will learn more about plotting data with ggplot2 in another section.
However, for now, we have used the base R functions \texttt{hist()} and
\texttt{boxplot()} to create simple distribution plots.

\section{Assessing the normality of
data}\label{assessing-the-normality-of-data}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{At the end of this section, you will be able to:}, rightrule=.15mm, left=2mm]

\begin{itemize}
\tightlist
\item
  Assess the skewness and kurtosis of a dataset
\item
  Test the normality of a dataset using the Shapiro-Wilk test
\end{itemize}

\end{tcolorbox}

Many statistical tests assume that the data is normally distributed.
When your data sample size is small, violation of the normaility
assumption could be an issue. Let's assess the normality of the
\texttt{Sales} variable in the \texttt{album\_sales} dataset by
calculating the skewness and kurtosis. In order to do this, we will use
the \texttt{psych} package, which provides functions for calculating
skewness and kurtosis. If you haven't installed the \texttt{psych}
package yet, you can do so by running the following code:

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Install the psych package if you haven\textquotesingle{}t already}

\FunctionTok{install.packages}\NormalTok{(}\StringTok{"psych"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now, let's calculate the skewness and kurtosis of the \texttt{Sales}
variable in the \texttt{album\_sales} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the psych package}

\FunctionTok{library}\NormalTok{(psych)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Attaching package: 'psych'
\end{verbatim}

\begin{verbatim}
The following objects are masked from 'package:ggplot2':

    %+%, alpha
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the skewness of the Sales variable}

\NormalTok{skew\_sales }\OtherTok{\textless{}{-}} \FunctionTok{skew}\NormalTok{(album\_sales}\SpecialCharTok{$}\NormalTok{Sales)}

\NormalTok{skew\_sales}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0432729
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the kurtosis of the Sales variable}

\NormalTok{kurt\_sales }\OtherTok{\textless{}{-}} \FunctionTok{kurtosi}\NormalTok{(album\_sales}\SpecialCharTok{$}\NormalTok{Sales)}

\NormalTok{kurt\_sales}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -0.7157339
\end{verbatim}

When interpreting the skewness and kurtosis values, remember that values
of 0 indicate a normal distribution.

We can also test the normality of the \texttt{Sales} variable using the
Shapiro-Wilk test. The null hypothesis of the Shapiro-Wilk test is that
the data is normally distributed. Let's perform the Shapiro-Wilk test on
the \texttt{Sales} variable in the \texttt{album\_sales} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Perform the Shapiro{-}Wilk test on the Sales variable}

\FunctionTok{shapiro.test}\NormalTok{(album\_sales}\SpecialCharTok{$}\NormalTok{Sales)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Shapiro-Wilk normality test

data:  album_sales$Sales
W = 0.98479, p-value = 0.02965
\end{verbatim}

The output of the Shapiro-Wilk test includes the test statistic and the
p-value. If the p-value is less than 0.05, we reject the null hypothesis
and conclude that the data is not normally distributed. If the p-value
is greater than 0.05, we fail to reject the null hypothesis and conclude
that the data is normally distributed.

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-warning-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Assessing normality}, rightrule=.15mm, left=2mm]

The shapiro-wilk test is sensitive to sample size. For small sample
sizes, the test may be too conservative and reject the null hypothesis
too often. For large sample sizes, the test may be too lenient and fail
to reject the null hypothesis too often. Therefore, you should not rely
solely on the Shapiro-Wilk test to assess the normality of your data.
Visual inspection of the data using histograms and Q-Q plots is also
recommended. Also remember that the central limit theorem states that
the sampling distribution of the mean will be approximately normally
distributed for large sample sizes, regardless of the distribution of
the original data.

We could also use non-parametric bootstrapping methods to deal with
non-normal data. We will cover this in a later section.

\end{tcolorbox}

\bookmarksetup{startatroot}

\chapter{Sampling, power and effect
size}\label{sampling-power-and-effect-size}

In this chapter, you will learn how to conduct some analysis on the
related concepts of sampling, power, and effect size.

Effect size is a measure of the strength of the relationship between two
variables in a statistical population. It is used to quantify the size
of the difference between two groups or the strength of an association
between two variables. In this section, we will learn how to calculate
the effect size for different types of data.

The size of an effect is important when planning a study and trying to
determine the sample size required. If an effect is small, we need a
larger sample size to detect it. If an effect is large, we can detect it
with a smaller sample size.

The ability of a study to detect an effect, using its sample, is called
statistical power. Power is the probability that a study will correctly
reject a false null hypothesis. In other words, power is the probability
that a study will find a true effect when there is one (avoiding a Type
2 error).

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, rightrule=.15mm, left=2mm]

The power of a study is influenced by the sample size, the effect size,
and the significance level. A larger sample size, a larger effect size,
and a higher significance level all increase the power of a study.

However, since our significance level is usually set at 0.05 and the
effect size is determined by the data, the sample size is the only
factor that we can control to increase the power of a study.

\end{tcolorbox}

\section{Different measures of effect
size}\label{different-measures-of-effect-size}

There are different ways to calculate effect size depending on the type
of data and the statistical test used. Here are some common effect size
measures:

\begin{itemize}
\item
  \textbf{Cohen's d}: This is a measure of the difference between two
  means in standard deviation units. It is commonly used in t-tests and
  ANOVA tests.
\item
  \textbf{Eta-squared (}\(\eta^2\)): This is a measure of the proportion
  of variance in the dependent variable that is explained by the
  independent variable. It is commonly used in ANOVA tests.
\item
  \textbf{Phi coefficient (}\(\phi\)): This is a measure of the
  association between two binary variables. It is commonly used in
  chi-square tests.
\item
  \textbf{Correlation coefficient (}\(r\)): This is a measure of the
  strength and direction of the relationship between two continuous
  variables. It is commonly used in correlation tests.
\end{itemize}

In the following sections, we will calculate the effect size for
different types of data using some of these measures.

\section{Calculating Cohen's d}\label{calculating-cohens-d}

Cohen's d is a measure of the difference between two means in standard
deviation units. It is calculated as the difference between the means
divided by the pooled standard deviation. The formula for Cohen's d is:

\[ d = \frac{{\bar{X}_1 - \bar{X}_2}}{{s_p}} \]

where:

\begin{itemize}
\item
  \(\bar{X}_1\) and \(\bar{X}_2\) are the means of the two groups.
\item
  \(s_p\) is the pooled standard deviation, calculated as:
\end{itemize}

\[s_p = \sqrt{\frac{{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}}{{n_1 + n_2 - 2}}} \]

where:

\begin{itemize}
\item
  \(n_1\) and \(n_2\) are the sample sizes of the two groups.
\item
  \(s_1\) and \(s_2\) are the standard deviations of the two groups.
\end{itemize}

Let's calculate Cohen's d for a hypothetical dataset with two groups.
The dataset contains the following information:

\begin{itemize}
\tightlist
\item
  Group 1: Mean = 10, Standard deviation = 2, Sample size = 30
\item
  Group 2: Mean = 12, Standard deviation = 3, Sample size = 30
\end{itemize}

To calculate Cohen's d, we first need to calculate the pooled standard
deviation (\(s_p\)) using the formula above. Then, we can calculate
Cohen's d using the formula for Cohen's d.

Let's calculate Cohen's d for this dataset using R:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate Cohen\textquotesingle{}s d}

\CommentTok{\# Group 1}

\NormalTok{mean1 }\OtherTok{\textless{}{-}} \DecValTok{10}

\NormalTok{sd1 }\OtherTok{\textless{}{-}} \DecValTok{2}

\NormalTok{n1 }\OtherTok{\textless{}{-}} \DecValTok{30}

\CommentTok{\# Group 2}

\NormalTok{mean2 }\OtherTok{\textless{}{-}} \DecValTok{12}

\NormalTok{sd2 }\OtherTok{\textless{}{-}} \DecValTok{3}

\NormalTok{n2 }\OtherTok{\textless{}{-}} \DecValTok{30}

\CommentTok{\# Calculate pooled standard deviation}

\NormalTok{sp }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(((n1 }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ sd1}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{ (n2 }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ sd2}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (n1 }\SpecialCharTok{+}\NormalTok{ n2 }\SpecialCharTok{{-}} \DecValTok{2}\NormalTok{))}

\CommentTok{\# Calculate Cohen\textquotesingle{}s d}

\NormalTok{d }\OtherTok{\textless{}{-}}\NormalTok{ (mean1 }\SpecialCharTok{{-}}\NormalTok{ mean2) }\SpecialCharTok{/}\NormalTok{ sp}


\NormalTok{d}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -0.7844645
\end{verbatim}

The calculated value of Cohen's d is -0.7844645. This negative value
indicates that the mean of Group 1 is smaller than the mean of Group 2
by approximately 0.78 standard deviations.

\section{Calculating Eta-squared and other effect size
measures}\label{calculating-eta-squared-and-other-effect-size-measures}

It is possible to calculate other effect size measures such as
Eta-squared, Phi coefficient. However, these measures are most commonly
calculated using the output of statistical tests such as ANOVA and
chi-square tests etc. To obtain these measures for the purpose of sample
size calculation, you would usually look at previous studies or meta
analyses to determine the expected effect size. For clinical research,
you may also use the minimal clinically important difference (MCID) as a
guide to determine the effect size.

\section{Power analysis}\label{power-analysis}

Power analysis is a method used to determine the sample size required to
detect an effect of a given size with a certain level of confidence. It
is important to conduct a power analysis before conducting a study to
ensure that the sample size is adequate to detect the effect of
interest.

To conduct a power analysis, you need to specify the following
parameters:

\begin{itemize}
\item
  The effect size: The size of the effect you want to detect. This is
  usually determined based on previous studies, meta-analyses or MCID.
\item
  The significance level (\(\alpha\)): The probability of rejecting the
  null hypothesis when it is true (Type 1 error rate). This is commonly
  set at 0.05.
\item
  The power (\(1 - \beta\)): The probability of correctly rejecting the
  null hypothesis when it is false (1 - Type 2 error rate). This is
  commonly set at 0.80 or 0.90.
\item
  The number of groups or conditions: The number of groups or conditions
  in the study.
\end{itemize}

The sample size required to achieve a desired power level can be
calculated using power analysis functions in R. There are many packages
in r for power analysis. The package \texttt{pwr} is one such package
that provides functions to calculate the sample size required for
different types of statistical tests.

The functions for some basic research designs are:

\begin{itemize}
\item
  \texttt{pwr.t.test()}: For t-tests
\item
  \texttt{pwr.anova.test()}: For ANOVA tests
\item
  \texttt{pwr.chisq.test()}: For chi-square tests
\item
  \texttt{pwr.f2.test()}: For regression models
\end{itemize}

Let's calculate the sample size required to achieve a power of 0.80 for
a t-test with the example data we used earlier. We will use the
\texttt{pwr.t.test()} function from the \texttt{pwr} package to
calculate the sample size required to achieve a power of 0.80 for a
t-test with the following parameters:

\begin{itemize}
\item
  Effect size (Cohen's d) = -0.7844645
\item
  Significance level (\(\alpha\)) = 0.05
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the pwr package}


\FunctionTok{library}\NormalTok{(pwr)}

\CommentTok{\# Calculate the sample size required for a t{-}test}
\CommentTok{\# using the d value calculated earlier}

\FunctionTok{pwr.t.test}\NormalTok{(}\AttributeTok{d =}\NormalTok{ d, }\AttributeTok{sig.level =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{power =} \FloatTok{0.80}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

     Two-sample t test power calculation 

              n = 26.50429
              d = 0.7844645
      sig.level = 0.05
          power = 0.8
    alternative = two.sided

NOTE: n is number in *each* group
\end{verbatim}

The output of the \texttt{pwr.t.test()} function provides the sample
size required to achieve a power of 0.80 for a t-test with the specified
effect size and significance level. The output includes the following
information:

\begin{itemize}
\item
  n = The sample size required for each group to achieve a power of
  0.80.
\item
  d = The effect size (Cohen's d) used in the power analysis.
\item
  sig.level = The significance level used in the power analysis.
\item
  power = The power level achieved with the specified sample size.
\end{itemize}

The sample size required to achieve a power of 0.80 for a t-test with
the specified effect size and significance level is 26.5 for each group.
Since the sample size must be a whole number, we would need to round up
to the nearest whole number. Therefore, the sample size required for
each group is 27.

\section{More complex power analysis}\label{more-complex-power-analysis}

For more complex designs, different approaches to power analysis might
be necessary, such as using simulation. This is possible to do in R, but
is beyond the scope of this chapter.

\bookmarksetup{startatroot}

\chapter{Basic statistical tests}\label{basic-statistical-tests}

In this chapter, we will learn how to perform basic statistical tests in
R. These are all tests that you should be familiar with already from
your statistics courses. We will cover the following tests:

\begin{itemize}
\tightlist
\item
  Independent t-test
\item
  Paired t-test
\item
  Wilcoxon signed-rank test
\item
  Mann-Whitney U test
\item
  Chi-squared test
\item
  Correlation
\item
  ANOVA
\end{itemize}

These examples will not be exhaustive, but they should give you a good
starting point for performing these tests in R. For theoretical
background, you can refer to any standard statistics textbook.

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{At the end of this chapter, you will be able to:}, rightrule=.15mm, left=2mm]

\begin{itemize}
\tightlist
\item
  Conduct several basic inferential tests with R
\end{itemize}

\end{tcolorbox}

These examples will all use built-in datasets in R. Each example will
include the necessary code to load the dataset and perform the test.
However, you can also use your own datasets by loading them into R
yourself and replacing the dataset name in the examples.

\section{Independent t-test}\label{independent-t-test}

The independent t-test is used to compare the means of two independent
groups. In R, you can use the \texttt{t.test()} function to perform an
independent t-test. Here is an example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This example uses the mtcars dataset, which is a built{-}in dataset in R that contains data on various car models.}

\CommentTok{\# Load the mtcars dataset}

\FunctionTok{data}\NormalTok{(mtcars)}

\CommentTok{\# Independent t{-}test example: Is there a difference in fuel efficiency between automatic and manual cars?}
\CommentTok{\# the variable am is a binary variable indicating the type of transmission (0 = automatic, 1 = manual)}

\CommentTok{\# Perform the independent t{-}test}

\NormalTok{t\_test\_result }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ am, }\AttributeTok{data =}\NormalTok{ mtcars)}

\CommentTok{\# Print the result}

\NormalTok{t\_test\_result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Welch Two Sample t-test

data:  mpg by am
t = -3.7671, df = 18.332, p-value = 0.001374
alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0
95 percent confidence interval:
 -11.280194  -3.209684
sample estimates:
mean in group 0 mean in group 1 
       17.14737        24.39231 
\end{verbatim}

In this example, we are comparing the fuel efficiency (mpg) of automatic
and manual cars in the \texttt{mtcars} dataset. The \texttt{mpg}
variable is the dependent variable, and the \texttt{am} variable is the
independent variable. The \texttt{t.test()} function is used to perform
the independent t-test, and the result is stored in the
\texttt{t\_test\_result} variable.

If we look at the output, we can see the following:

\begin{itemize}
\tightlist
\item
  The t-test result is presented.
\item
  The alternative hypothesis is that the means are not equal.
\item
  The 95\% confidence interval for the difference in means is presented.
\item
  The 2 sample means are presented.
\end{itemize}

\section{Paired t-test}\label{paired-t-test}

The paired t-test is used to compare the means of two related groups. In
R, you can use the \texttt{t.test()} function with the
\texttt{paired\ =\ TRUE} argument to perform a paired t-test. Here is an
example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This example uses the sleep dataset, which is a built{-}in dataset in R that contains data on the effect of two soporific drugs on sleep duration.}

\CommentTok{\# Load the sleep dataset}

\FunctionTok{data}\NormalTok{(sleep)}

\CommentTok{\# Paired t{-}test example: Is there a difference in sleep duration between the two drugs?}

\CommentTok{\# Perform the paired t{-}test}
\CommentTok{\# The paired t{-}test now requires the data to be in wide format, (data from each group in separate columns). If this is not the case, we need to reshape the data first. }

\NormalTok{sleep2 }\OtherTok{\textless{}{-}} \FunctionTok{reshape}\NormalTok{(sleep, }\AttributeTok{direction =} \StringTok{"wide"}\NormalTok{,}
                  \AttributeTok{idvar =} \StringTok{"ID"}\NormalTok{, }\AttributeTok{timevar =} \StringTok{"group"}\NormalTok{)}

\NormalTok{paired\_t\_test\_result }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{(sleep2}\SpecialCharTok{$}\NormalTok{extra}\FloatTok{.1}\NormalTok{, sleep2}\SpecialCharTok{$}\NormalTok{extra}\FloatTok{.2}\NormalTok{, }\AttributeTok{paired =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Print the result}

\NormalTok{paired\_t\_test\_result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Paired t-test

data:  sleep2$extra.1 and sleep2$extra.2
t = -4.0621, df = 9, p-value = 0.002833
alternative hypothesis: true mean difference is not equal to 0
95 percent confidence interval:
 -2.4598858 -0.7001142
sample estimates:
mean difference 
          -1.58 
\end{verbatim}

In this example, we are comparing the sleep duration (\texttt{extra})
between patients who were given different drugs in the \texttt{sleep}
dataset (note: 10 people were each measured twice). The \texttt{extra}
variable is the dependent variable, and the \texttt{group} variable is
the independent variable. The \texttt{t.test()} function is used to
perform the paired t-test, and the result is stored in the
\texttt{paired\_t\_test\_result} variable.

If we look at the output, we can see the following:

\begin{itemize}
\tightlist
\item
  The t-test result is presented.
\item
  The alternative hypothesis is that the means are not equal.
\item
  The 95\% confidence interval for the difference in means is presented.
\item
  The mean difference is presented.
\end{itemize}

\section{Wilcoxon signed-rank test}\label{wilcoxon-signed-rank-test}

The Wilcoxon signed-rank test is a non-parametric test used to compare
two related groups. In R, you can use the \texttt{wilcox.test()}
function to perform a Wilcoxon signed-rank test. Here is an example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This example uses the sleep dataset, which is a built{-}in dataset in R that contains data on the effect of two soporific drugs on sleep duration.}

\CommentTok{\# Load the sleep dataset}

\FunctionTok{data}\NormalTok{(sleep)}

\CommentTok{\# Wilcoxon signed{-}rank test example: Is there a difference in sleep duration between the two drugs?}

\CommentTok{\# Perform the Wilcoxon signed{-}rank test}
\CommentTok{\# The wilcoxin now requires the data to be in wide format, (data from each group in separate columns). If this is not the case, we need to reshape the data first. }

\NormalTok{sleep2 }\OtherTok{\textless{}{-}} \FunctionTok{reshape}\NormalTok{(sleep, }\AttributeTok{direction =} \StringTok{"wide"}\NormalTok{,}
                  \AttributeTok{idvar =} \StringTok{"ID"}\NormalTok{, }\AttributeTok{timevar =} \StringTok{"group"}\NormalTok{)}

\NormalTok{wilcoxon\_test\_result }\OtherTok{\textless{}{-}} \FunctionTok{wilcox.test}\NormalTok{(sleep2}\SpecialCharTok{$}\NormalTok{extra}\FloatTok{.1}\NormalTok{, sleep2}\SpecialCharTok{$}\NormalTok{extra}\FloatTok{.2}\NormalTok{, }\AttributeTok{paired =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in wilcox.test.default(sleep2$extra.1, sleep2$extra.2, paired = TRUE):
cannot compute exact p-value with ties
\end{verbatim}

\begin{verbatim}
Warning in wilcox.test.default(sleep2$extra.1, sleep2$extra.2, paired = TRUE):
cannot compute exact p-value with zeroes
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Print the result}

\NormalTok{wilcoxon\_test\_result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Wilcoxon signed rank test with continuity correction

data:  sleep2$extra.1 and sleep2$extra.2
V = 0, p-value = 0.009091
alternative hypothesis: true location shift is not equal to 0
\end{verbatim}

In this example, we are comparing the sleep duration (\texttt{extra})
between patients who were given different drugs in the \texttt{sleep}
dataset. The \texttt{extra} variable is the dependent variable, and the
\texttt{group} variable is the independent variable. The
\texttt{wilcox.test()} function is used to perform the Wilcoxon
signed-rank test, and the result is stored in the
\texttt{wilcoxon\_test\_result} variable.

If we look at the output, we can see the following:

\begin{itemize}
\tightlist
\item
  The Wilcoxon signed-rank test result is presented ( V is the sum of
  the ranks of the differences between the pairs of observations).
\item
  The alternative hypothesis (true location shift is not equal to 0) is
  presented. This means that the medians of the two groups are not
  equal.
\end{itemize}

Note: if there are ties in the data, the exact p-value is not
calculated. Instead, an approximate p-value is presented.

\section{Mann-Whitney U test}\label{mann-whitney-u-test}

The Mann-Whitney U test is a non-parametric test used to compare the
means of two independent groups. In R, you can use the
\texttt{wilcox.test()} function to perform a Mann-Whitney U test. Note:
we will not include the \texttt{paired\ =\ TRUE} argument that we did in
the previous example. Here is an example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This example uses the mtcars dataset, which is a built{-}in dataset in R that contains data on various car models.}

\CommentTok{\# Load the mtcars dataset}

\FunctionTok{data}\NormalTok{(mtcars)}

\CommentTok{\# Mann{-}Whitney U test example: Is there a difference in fuel efficiency between automatic and manual cars?}

\CommentTok{\# Perform the Mann{-}Whitney U test}

\NormalTok{mann\_whitney\_test\_result }\OtherTok{\textless{}{-}} \FunctionTok{wilcox.test}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ am, }\AttributeTok{data =}\NormalTok{ mtcars)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot
compute exact p-value with ties
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Print the result}

\NormalTok{mann\_whitney\_test\_result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Wilcoxon rank sum test with continuity correction

data:  mpg by am
W = 42, p-value = 0.001871
alternative hypothesis: true location shift is not equal to 0
\end{verbatim}

The output is interpreted in the same way as the Wilcoxon signed-rank
test output.

\section{Chi-squared test}\label{chi-squared-test}

The chi-squared test is used to test the association between two
categorical variables. In R, you can use the \texttt{chisq.test()}
function to perform a chi-squared test. Here is an example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This example uses the mtcars dataset, which is a built{-}in dataset in R that contains data on various car models.}

\CommentTok{\# Load the mtcars dataset}

\FunctionTok{data}\NormalTok{(mtcars)}

\CommentTok{\# Chi{-}squared test example: Is there an association between the number of cylinders and the type of transmission?}

\CommentTok{\# Create a contingency table}

\NormalTok{contingency\_table }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{cyl, mtcars}\SpecialCharTok{$}\NormalTok{am)}

\CommentTok{\# Perform the chi{-}squared test}

\NormalTok{chi\_squared\_test\_result }\OtherTok{\textless{}{-}} \FunctionTok{chisq.test}\NormalTok{(contingency\_table)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in chisq.test(contingency_table): Chi-squared approximation may be
incorrect
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Print the result}

\NormalTok{chi\_squared\_test\_result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's Chi-squared test

data:  contingency_table
X-squared = 8.7407, df = 2, p-value = 0.01265
\end{verbatim}

In this example, we are testing the association between the number of
cylinders (\texttt{cyl}) and the type of transmission (\texttt{am}) in
the \texttt{mtcars} dataset. The \texttt{chisq.test()} function is used
to perform the chi-squared test, and the result is stored in the
\texttt{chi\_squared\_test\_result} variable.

If we look at the output, we can see the following:

\begin{itemize}
\tightlist
\item
  The chi-squared test result is presented.
\end{itemize}

\section{Correlation}\label{correlation}

Correlation is used to test the relationship between two continuous
variables. In R, you can use the \texttt{cor.test()} function to
calculate the correlation coefficient with significance test. Here is an
example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This example uses the mtcars dataset, which is a built{-}in dataset in R that contains data on various car models.}

\CommentTok{\# Load the mtcars dataset}

\FunctionTok{data}\NormalTok{(mtcars)}

\CommentTok{\# Correlation example: Is there a relationship between fuel efficiency and horsepower?}

\CommentTok{\# Calculate the correlation coefficient}

\NormalTok{correlation\_result }\OtherTok{\textless{}{-}} \FunctionTok{cor.test}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg, mtcars}\SpecialCharTok{$}\NormalTok{hp)}

\CommentTok{\# Print the result}

\NormalTok{correlation\_result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's product-moment correlation

data:  mtcars$mpg and mtcars$hp
t = -6.7424, df = 30, p-value = 1.788e-07
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.8852686 -0.5860994
sample estimates:
       cor 
-0.7761684 
\end{verbatim}

In this example, we are testing the relationship between fuel efficiency
(\texttt{mpg}) and horsepower (\texttt{hp}) in the \texttt{mtcars}
dataset. The \texttt{cor.test()} function is used to calculate the
correlation coefficient, and the result is stored in the
\texttt{correlation\_result} variable.

If we look at the output, we can see the following:

\begin{itemize}
\tightlist
\item
  The correlation coefficient is presented in the last line of the
  output.
\item
  The significance level of the correlation is tested using a t-test,
  which is also presented in the output.
\item
  The confidence interval for the correlation coefficient is presented.
\item
  The alternative hypothesis is that the correlation is not equal to 0.
\end{itemize}

\section{One-way ANOVA}\label{one-way-anova}

One-way ANOVA (Analysis of Variance) is used to test the differences
between the means of three or more groups. In R, you can use the
\texttt{aov()} function to perform an ANOVA. Here is an example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This example uses the mtcars dataset, which is a built{-}in dataset in R that contains data on various car models.}

\CommentTok{\# Load the mtcars dataset}

\FunctionTok{data}\NormalTok{(mtcars)}

\CommentTok{\# One{-}way ANOVA example: Is there a difference in fuel efficiency between cars with different numbers of cylinders?}

\CommentTok{\# cyl should be a factor}

\NormalTok{mtcars}\SpecialCharTok{$}\NormalTok{cyl }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{cyl)}

\CommentTok{\# Perform the ANOVA}

\NormalTok{anova\_result }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cyl, }\AttributeTok{data =}\NormalTok{ mtcars)}

\CommentTok{\# Print the result}

\FunctionTok{summary}\NormalTok{(anova\_result)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            Df Sum Sq Mean Sq F value   Pr(>F)    
cyl          2  824.8   412.4    39.7 4.98e-09 ***
Residuals   29  301.3    10.4                     
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

In this example, we are testing the differences in fuel efficiency
(\texttt{mpg}) between cars with different numbers of cylinders
(\texttt{cyl}) in the \texttt{mtcars} dataset. The \texttt{aov()}
function is used to perform the ANOVA, and the result is stored in the
\texttt{anova\_result} variable.

If we look at the output, we can see the following:

\begin{itemize}
\tightlist
\item
  The ANOVA result is within the \texttt{summary()} function. The
  summary includes the F-statistic, the p-value, and the significance
  level of the ANOVA test.
\end{itemize}

\section{Factorial ANOVA}\label{factorial-anova}

Factorial ANOVA is used to test the effects of two or more independent
variables on a dependent variable. In R, you can use the \texttt{aov()}
function with interaction terms to perform a factorial ANOVA. Here is an
example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This example uses the mtcars dataset, which is a built{-}in dataset in R that contains data on various car models.}

\CommentTok{\# Load the mtcars dataset}

\FunctionTok{data}\NormalTok{(mtcars)}

\CommentTok{\# Factorial ANOVA example: Is there an interaction effect between the number of cylinders and the type of transmission on fuel efficiency?}

\CommentTok{\# cyl and am should be factors}

\NormalTok{mtcars}\SpecialCharTok{$}\NormalTok{cyl }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{cyl)}
\NormalTok{mtcars}\SpecialCharTok{$}\NormalTok{am }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{am)}

\CommentTok{\# Perform the factorial ANOVA}

\NormalTok{factorial\_anova\_result }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cyl }\SpecialCharTok{*}\NormalTok{ am, }\AttributeTok{data =}\NormalTok{ mtcars)}

\CommentTok{\# Print the result}

\FunctionTok{summary}\NormalTok{(factorial\_anova\_result)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            Df Sum Sq Mean Sq F value   Pr(>F)    
cyl          2  824.8   412.4  44.852 3.73e-09 ***
am           1   36.8    36.8   3.999   0.0561 .  
cyl:am       2   25.4    12.7   1.383   0.2686    
Residuals   26  239.1     9.2                     
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

In this example, we are testing the interaction effect between the
number of cylinders (\texttt{cyl}) and the type of transmission
(\texttt{am}) on fuel efficiency (\texttt{mpg}) in the \texttt{mtcars}
dataset. The \texttt{aov()} function is used to perform the factorial
ANOVA, and the result is stored in the \texttt{factorial\_anova\_result}
variable.

The interaction term is specified using the \texttt{*} operator in the
formula.

If we look at the output, we can see the following:

\begin{itemize}
\item
  The ANOVA result is within the \texttt{summary()} function. The
  summary includes the F-statistic, the p-value, and the significance
  level of each term tested in the ANOVA.
\item
  Each independent variable is tested separately (main effects), and the
  interaction effect is also tested.
\item
  The interaction effect is denoted by the \texttt{cyl:am} term in the
  output.
\end{itemize}

\section{Conclusion}\label{conclusion}

In this chapter, we have covered several basic statistical tests that
you can perform in R. These are included for reference and to help you
get started with performing statistical tests in R. However, we will be
focusing on modelling our data, using different types of regression
models, rather than re-learning these tests.

\bookmarksetup{startatroot}

\chapter{Correlation in R}\label{correlation-in-r}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{At the end of this chapter, you will be able to:}, rightrule=.15mm, left=2mm]

\begin{itemize}
\tightlist
\item
  Conduct and interpret a correlation analysis in R
\end{itemize}

\end{tcolorbox}

\section{What is Correlation?}\label{what-is-correlation}

Correlation is a measure of the strength and direction of a relationship
between two variables. It is most commonly used when we want to see if
there is a relationship between two \emph{continuous} variables.
However, it is possible to run correlations between a continuous and a
categorical variable (this is known as point-biserial correlation) or
between two categorical variables (this is known as phi coefficient).

Sticking to the more conventional form of correlation; when we calculate
correlation, we get an r value of between -1 and 1. This tells us two
things:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The closer the value is to 1, the stronger the relationship. The
  closer the value is to 0, the weaker the relationship.
\item
  The sign of the value tells us the direction of the relationship.
  Positive values indicate a positive relationship (i.e.~as one variable
  increases, so does the other). Negative values indicate a negative
  relationship (i.e.~as one variable increases, the other decreases).
\end{enumerate}

We also often calculate the significance of the correlation, which tests
against a null hypothesis that the correlation is 0. This is not part of
the correlation per se, but it is often part of correlational research
questions.

\section{Visualising correlation}\label{visualising-correlation}

We can visualise correlation using a scatterplot. This is a graph where
each data point is plotted on a graph, with one variable on the x axis
and the other on the y axis. If the data points form something
resembling a straight line, with all of the data points in a consistent
pattern, then we have a strong correlation. If the data points are
scattered, then we have a weaker correlation. However, if the data
points are inconsistent or more diffuse, the correlation is weaker. The
direction of the line tells us the direction of the correlation.

\begin{center}
\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{correlation_files/figure-pdf/unnamed-chunk-1-1.pdf}
\end{center}

Visualising the data in this way can give us a good idea of the strength
and direction of the correlation. However, it is not a substitute for
running the correlation itself. At the same time, correlation
coefficients can be misleading on their own. It is always a good idea to
visualise the data as well.

\section{How is correlation
calculated?}\label{how-is-correlation-calculated}

Correlation can be thought of as covariance divided by individual
variance. Covariance is a measure of how much two variables change
together. Variance is a measure of how much a variable changes on its
own. When we divide covariance by variance, we get a value that is
standardised and can be compared across different data sets.

If the changes are consistent with both variables (i.e.~the covariance
is higher and the individual variance is lower), then the final
correlation value will be higher. However, if the changes are
inconsistent (i.e.~the covariance is lower and the individual variance
is higher), then the final correlation value will be lower.

\begin{center}
\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{img/correlation.png}
\end{center}

\begin{center}
\includegraphics[width=\linewidth,height=1\textheight,keepaspectratio]{img/corcalc.png}
\end{center}

\section{Which correlation to use?}\label{which-correlation-to-use}

When we run correlation in R, we use the \emph{cor.test()} command. This
command will give us the correlation value, the p value and the
confidence intervals.

We can specify a Pearson correlation (the default) or a Spearman
correlation (for non-parametric data).

\subsection{Running correlation in R}\label{running-correlation-in-r}

\begin{itemize}
\tightlist
\item
  R can run correlations using the \emph{cor.test()} command
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor.test}\NormalTok{(regression\_data}\SpecialCharTok{$}\NormalTok{treatment\_duration,regression\_data}\SpecialCharTok{$}\NormalTok{aggression\_level) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's product-moment correlation

data:  regression_data$treatment_duration and regression_data$aggression_level
t = -9.5503, df = 98, p-value = 1.146e-15
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.7838251 -0.5765006
sample estimates:
       cor 
-0.6942996 
\end{verbatim}

In the above example, we are testing the correlation between treatment
duration and aggression level. Each variable is separated by a comma.

\subsection{Interpreting the output}\label{interpreting-the-output}

\begin{itemize}
\tightlist
\item
  The r value tells us the strength and direction of the relationship
\item
  In the output it is labelled as ``cor'' (short for correlation)
\end{itemize}

Correlation values can range from -1 to 1. The closer the value is to 1,
the stronger the relationship. The closer the value is to 0, the weaker
the relationship. Positive values indicate a positive relationship
(i.e.~as one variable increases, so does the other). Negative values
indicate a negative relationship (i.e.~as one variable increases, the
other decreases).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor.test}\NormalTok{(regression\_data}\SpecialCharTok{$}\NormalTok{treatment\_duration,regression\_data}\SpecialCharTok{$}\NormalTok{aggression\_level)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's product-moment correlation

data:  regression_data$treatment_duration and regression_data$aggression_level
t = -9.5503, df = 98, p-value = 1.146e-15
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.7838251 -0.5765006
sample estimates:
       cor 
-0.6942996 
\end{verbatim}

\subsection{Check the significance of the
correlation}\label{check-the-significance-of-the-correlation}

\begin{itemize}
\tightlist
\item
  We can see that the significance by looking at the p value

  \begin{itemize}
  \tightlist
  \item
    The significance is 1.146\^{}-15
  \item
    This means: 0.0000000000000001146
  \end{itemize}
\item
  Therefore p value \textless{} 0.05
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor.test}\NormalTok{(regression\_data}\SpecialCharTok{$}\NormalTok{treatment\_duration,regression\_data}\SpecialCharTok{$}\NormalTok{aggression\_level)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's product-moment correlation

data:  regression_data$treatment_duration and regression_data$aggression_level
t = -9.5503, df = 98, p-value = 1.146e-15
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.7838251 -0.5765006
sample estimates:
       cor 
-0.6942996 
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Exponent values}, rightrule=.15mm, left=2mm]

When we see a value like 1.146e-15, this is a shorthand way of writing a
very small number. The e-15 means that we move the decimal point 15
places to the left. So 1.146e-15 is the same as 0.000000000000001146

\end{tcolorbox}

\bookmarksetup{startatroot}

\chapter{Simple Regression in R}\label{simple-regression-in-r}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{At the end of this chapter, you will be able to:}, rightrule=.15mm, left=2mm]

\begin{itemize}
\tightlist
\item
  Conduct and interpret a regression analysis in R
\end{itemize}

\end{tcolorbox}

\bookmarksetup{startatroot}

\chapter{What is regression
analysis?}\label{what-is-regression-analysis}

Regression analysis is a statistical technique used to model the
relationship between an outcome (dependent) variable and one or more
predictor (criterion/independent) variables. The goal of regression
analysis is to understand how the outcome variable changes as the
predictor variable changes.

When we say simple regression, we mean that there is only one predictor
variable. This is to distinguish from multiple regression, where there
are two or more predictor variables.

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-important-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-important-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Regression does not imply causation}, rightrule=.15mm, left=2mm]

Regression analysis can show that two variables are related, but it does
not prove that one variable causes the other. To establish causation,
you need a research design that can rule out alternative explanations,
i.e., an experiment.

\end{tcolorbox}

\bookmarksetup{startatroot}

\chapter{Simple regression in R}\label{simple-regression-in-r-1}

In R, we can conduct a simple regression analysis using the
\texttt{lm()} function. The \texttt{lm()} function stands for linear
model and is used to fit linear models. The syntax for the \texttt{lm()}
function is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, data)}
\end{Highlighting}
\end{Shaded}

Where \texttt{y} is the outcome variable, \texttt{x} is the predictor
variable, and \texttt{data} is the dataset containing the variables.

Let's conduct a simple regression analysis using the \texttt{lm()}
function in R. We will use the \texttt{regression\_data} dataset, which
contains \texttt{trust\_score} as the outcome variable and
\texttt{treatment\_duration} as the predictor variable.

\phantomsection\label{annotated-cell-49}%
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit a simple regression model}

\NormalTok{regression\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(trust\_score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treatment\_duration, }\AttributeTok{data =}\NormalTok{ regression\_data) }\hspace*{\fill}\NormalTok{\circled{1}}
\end{Highlighting}
\end{Shaded}

\begin{description}
\tightlist
\item[\circled{1}]
We fit a simple regression model using the \texttt{lm()} function. The
outcome variable is \texttt{trust\_score}, and the predictor variable is
\texttt{treatment\_duration}. The data are specified using the
\texttt{data} argument.
\end{description}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{What is the model?}, rightrule=.15mm, left=2mm]

The terms \textbf{model} or \textbf{fit}, are commonly used in
regression analysis.

Model refers to the variables that you included in the model as well as
the relationship between them. For example, our model above includes the
predictor variable \textbf{depression} and the outcome variable
\textbf{avoidance}. The model is the relationship between these
variables. In this case, we are saying that \textbf{avoidance} is
predicted by \textbf{depression}.

A different model would be one that included different predictor
variables. For example, we could have a model that included the
predictor variable \textbf{treatment} and the outcome variable
\textbf{avoidance}, or model that includes both \textbf{depression} and
\textbf{treatment} as predictor variables and \textbf{avoidance} as the
outcome variable.

Fit refers to the idea that we want to know if this model is a good fit
for the data. In other words, does the model explain the data well?

We might find that our first model is not a good fit for the data. In
this case, we might try a different model. We will learn next week how
to compare different models to see which one is the best fit for the
data.

\end{tcolorbox}

\section{Checking model assumptions}\label{checking-model-assumptions}

Before interpreting the results of a regression analysis, it is
essential to check the assumptions of the model. The key assumptions of
linear regression are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Linearity}: The relationship between the predictor and outcome
  variables is linear.
\item
  \textbf{Independence}: The residuals (errors) are independent of each
  other.
\item
  \textbf{Homoscedasticity}: The residuals have constant variance.
\item
  \textbf{Normality}: The residuals are normally distributed.
\end{enumerate}

We can check these assumptions using diagnostic plots. The
\texttt{plot()} function in R can be used to create diagnostic plots for
the regression model.

\phantomsection\label{annotated-cell-50}%
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create diagnostic plots}

\FunctionTok{plot}\NormalTok{(regression\_model) }\hspace*{\fill}\NormalTok{\circled{1}}
\end{Highlighting}
\end{Shaded}

\begin{description}
\tightlist
\item[\circled{1}]
We create diagnostic plots for the regression model using the
\texttt{plot()} function. The diagnostic plots include a scatterplot of
the residuals against the fitted values, a Q-Q plot of the residuals, a
scale-location plot, and a plot of the residuals against the predictor
variable.
\end{description}

\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{regression_files/figure-pdf/unnamed-chunk-2-1.pdf}}
\end{center}

\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{regression_files/figure-pdf/unnamed-chunk-2-2.pdf}}
\end{center}

\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{regression_files/figure-pdf/unnamed-chunk-2-3.pdf}}
\end{center}

\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{regression_files/figure-pdf/unnamed-chunk-2-4.pdf}}
\end{center}

\section{Check the assumptions}\label{check-the-assumptions}

We can check the assumptions of linear regression using the plot()
function.

\phantomsection\label{annotated-cell-51}%
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# check the assumptions}

\FunctionTok{plot}\NormalTok{(model) }\hspace*{\fill}\NormalTok{\circled{1}}
\end{Highlighting}
\end{Shaded}

\begin{description}
\tightlist
\item[\circled{1}]
The \textbf{plot()} function is used to create a plot of the model. It
actually shows several plots in sequence.
\end{description}

\begin{quote}
When you run the plot function, there will be a message in the console
that says ``Hit to see next plot:''. You need to press enter to move
through the plots.
\end{quote}

\subsection{The assumption of
independence}\label{the-assumption-of-independence}

This assumption is the idea that each observation is independent of the
others. In other words, the value of one observation (e.g.~a
participant's score) should not be related to the value of another
participant's score. This is not interpreted from the plots, but is an
assumption that should be considered when collecting data. For example,
you would not use this approach to analyse data from a repeated measures
design, because the observations are not independent.

\subsection{The assumption of
linearity}\label{the-assumption-of-linearity}

The first plot is a plot of the residuals against the fitted values.
This plot is used to check the assumption of linearity. The assumption
of linearity states that the relationship between the predictor variable
and the outcome variable is linear. In other words, the relationship
between the predictor variable and the outcome variable can be described
by a straight line. If the relationship is not linear, then the model is
not a good fit for the data.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{images/linearity1.jpg}}

}

\caption{Linear and non-linear data}

\end{figure}%

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Linear and non-linear residuals}, rightrule=.15mm, left=2mm]

\textbf{Good:} Random scatter around the line. The line is straight.

\textbf{Bad:} Non-random scatter around the line. The line is not
straight.

\textbf{What do we do if this assumption is violated?:} Linearity is a
very important assumption. If the assumption is violated, then the
linear model is likely not a good fit for the data. In this case we
should probably try a different model. Data are never perfect, but we
shouldn't ignore a clear violation of linearity.

\end{tcolorbox}

\subsection{The assumption of
normality}\label{the-assumption-of-normality}

The second plot is a normal Q-Q plot of the residuals. This plot is used
to check the assumption of normality. The assumption of normality states
that the residuals are normally distributed. If the residuals are not
normally distributed, then the model is not a good fit for the data.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{images/qqplot.jpg}}

}

\caption{qqplot of residuals}

\end{figure}%

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Normal and non-normal residuals}, rightrule=.15mm, left=2mm]

\textbf{Good:} The points follow the line.

\textbf{Bad:} The points do not follow the line.

\textbf{What do we do if this assumption is violated?:} Normality
affects the accuracy of beta values, significance tests and confidence
intervals. However, it is most important with small sample sizes. As
sample size increases, the assumption of normality becomes less
important.

\end{tcolorbox}

\subsection{The assumption of
homoscedasticity}\label{the-assumption-of-homoscedasticity}

The third plot is a scale-location plot of the residuals against the
fitted values. This plot is used to check the assumption of
homoscedasticity. The assumption of homoscedasticity states that the
residuals have equal variance. If the residuals do not have equal
variance, then the model is not a good fit for the data.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{images/homoscedasticity.png}}

}

\caption{homoscedasticity}

\end{figure}%

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Homoscedastic and heteroscedastic residuals}, rightrule=.15mm, left=2mm]

\textbf{Good:} The points are randomly scattered around the line. and
the line is horizontal.

\textbf{Bad:} The points are not randomly scattered around the line. and
the line is not horizontal.

\textbf{What do we do if this assumption is violated?:} This will affect
the accuracy of the beta values, significance tests and confidence
intervals. Essentially, it means that conclusions we draw from the model
are less accurate. What we do depends on the situation. Transformation
of the DV (e.g.~log transformation) might help. If not, there are
weighted regression models that can be used.

\end{tcolorbox}

\subsection{Checking for outliers or influential
cases}\label{checking-for-outliers-or-influential-cases}

The fourth plot is a plot of Cook's distance. This plot is used to check
for outliers. An outlier is a data point that is very different from the
rest of the data. If there are outliers, then they could be affecting
the regression model. The threshold for Cook's distance is 1. If a data
point has a Cook's distance greater than 1, then it is considered an
outlier.

The fifth plot is a plot of the residuals against the leverage. The
sixth plot is a plot of the Cook's distance against the leverage. They
are pretty much the same plots as the plot 4.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{images/leverage.jpg}}

}

\caption{Influential Cases and Leverage}

\end{figure}%

On the plot above, Cook's Distance is indicated by a red line. If a data
point is outside the red line, then it is considered an outlier.

Leverage is the idea that a particular outlier might have a lot of
influence on the regression model. Look for data points that are outside
the red line on the top right or bottom right of the plot. These are
data points that have a lot of leverage and might be influencing the
regression model.

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Outliers and influential cases}, rightrule=.15mm, left=2mm]

\textbf{Good:} No data points outside the red lines.

\textbf{Bad:} Data points outside the red lines. Outliers with high
leverage.

\textbf{What do we do if this assumption is violated?:} Outliers will
affect the calculation of variances (e.g.~sum of squares or standard
deviation) that are used in many calculations related to the regression
model. If there are influentual cases, we should consider removing them
from the analysis. When doing so, it is important to explain why you
removed them, and be transparent about how this affected the model
results.

\end{tcolorbox}

\section{Interpret the regression model
results}\label{interpret-the-regression-model-results}

To view the results of the regression model, we use the summary()
function.

\phantomsection\label{annotated-cell-52}%
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# View the results}

\FunctionTok{summary}\NormalTok{(regression\_model) }\hspace*{\fill}\NormalTok{\circled{1}}
\end{Highlighting}
\end{Shaded}

\begin{description}
\tightlist
\item[\circled{1}]
We view the results of the regression model using the \texttt{summary()}
function. The output includes the coefficients, standard errors,
t-values, p-values, and R-squared value of the model.
\end{description}

\begin{verbatim}

Call:
lm(formula = trust_score ~ treatment_duration, data = regression_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-53.585 -24.991   0.304  24.285  46.804 

Coefficients:
                   Estimate Std. Error t value Pr(>|t|)    
(Intercept)          64.754     14.176   4.568 1.44e-05 ***
treatment_duration   -1.130      1.371  -0.824    0.412    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 29.29 on 98 degrees of freedom
Multiple R-squared:  0.006886,  Adjusted R-squared:  -0.003248 
F-statistic: 0.6795 on 1 and 98 DF,  p-value: 0.4118
\end{verbatim}

\subsection{\texorpdfstring{\textbf{Call:} The regression
formula}{Call: The regression formula}}\label{call-the-regression-formula}

The first line of the output is the regression formula. This is the
formula that was used to create the model.

\subsection{\texorpdfstring{\textbf{Residuals:} The
residuals}{Residuals: The residuals}}\label{residuals-the-residuals}

The second line of the output is the residuals. The residuals are the
difference between the actual values of the outcome variable and the
predicted values of the outcome variable. This section is giving us some
summary statistics about the residuals. However, we usually check the
assumptions using the plots.

\subsection{\texorpdfstring{\textbf{Coefficients:} The beta
values}{Coefficients: The beta values}}\label{coefficients-the-beta-values}

The third section of the output is the coefficients. You will see a line
of values for the \textbf{intercept} and another line for each of the
predictor variables in the model. \textbf{Estimate} is the beta value.
\textbf{Std. Error} is the standard error of the beta value.
\textbf{Pr(\textgreater\textbar t\textbar)} is the p value for the beta
value.

\begin{itemize}
\item
  Intercept: We are not usually interested in this line by itself. It is
  the value of the outcome variable when all of the predictor variables
  are equal to zero. In this case, it is the value of avoidance when
  depression is equal to zero. However, it might be the case that
  depression cannot be equal to zero. In this case, the intercept would
  not be meaningful. \emph{If the predictor were a categorical variable,
  then the intercept would be the value of the outcome variable when the
  predictor variable is equal to the reference category (i.e.~The mean
  of the outcome for that group).}
\item
  Depression: This is the beta value for depression. It is the amount
  that avoidance changes when depression increases by one unit. What
  \emph{unit} means, depends on how the variables were measured, so it
  is likely to mean one point in the scale used to measure depression,
  for example.
\end{itemize}

The final section in the output shows:

\begin{itemize}
\item
  \textbf{Residual standard error}. This is the standard deviation of
  the residuals. It is the average amount that the actual values of the
  outcome variable differ from the predicted values of the outcome
  variable.
\item
  \textbf{Multiple R-squared}. This is the R-squared value. It is the
  amount of variance in the outcome variable that is explained by the
  model. We usually talk about this as a percentage value.
\item
  \textbf{Adjusted R-squared}. This is the adjusted R-squared value. It
  is the amount of variance in the outcome variable that is explained by
  the model, adjusted for the number of predictor variables in the
  model. This is to account for the fact that having more predictors in
  the model will always increase the R-squared value, even if the
  predictors are not related to the outcome variable. It is relevant
  when we have more than one predictor variable in the model.
\item
  \textbf{F Statistic}. The F value comes from the ANOVA that is used to
  test the significance of the model. It tests the null hypothesis that
  all of the beta values are equal to zero.
\item
  \textbf{p-value}. This is the p value for the F statistic (the
  significance of the overall regression model, with all of the
  predictors).
\end{itemize}

\section{Regression with a categorical predictor variable}\label{catreg}

So far, we have conducted a regression analysis with a continuous
predictor variable. Now, let's consider a regression analysis with a
categorical predictor variable.

Our next hypothesis is that the level of depression is different for
each treatment group. We can test this hypothesis using a regression
model with a categorical predictor variable. Let's conduct a regression
analysis with the \texttt{treatment} variable as the predictor variable.
If there are 2 levels of the predictor variable, then the model will
compare the two levels. If there are more than 2 levels, then the model
will compare each level to the reference level. By default, R uses the
first level of the predictor variable as the reference level. Howeverm
you can specify a different reference level using the \texttt{relevel()}
function.

\phantomsection\label{annotated-cell-53}%
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# specify the reference level {-} this is more useful when we have more than 2 levels}

\NormalTok{regression\_data}\SpecialCharTok{$}\NormalTok{treatment\_group }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(regression\_data}\SpecialCharTok{$}\NormalTok{treatment\_group) }\hspace*{\fill}\NormalTok{\circled{0}}
\NormalTok{regression\_data}\SpecialCharTok{$}\NormalTok{treatment }\OtherTok{\textless{}{-}} \FunctionTok{relevel}\NormalTok{(regression\_data}\SpecialCharTok{$}\NormalTok{treatment\_group, }\AttributeTok{ref =} \StringTok{"therapy1"}\NormalTok{) }\hspace*{\fill}\NormalTok{\circled{1}}

\CommentTok{\# Fit a regression model with a categorical predictor variable}

\NormalTok{regression\_model2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(aggression\_level }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treatment\_group, }\AttributeTok{data =}\NormalTok{ regression\_data) }\hspace*{\fill}\NormalTok{\circled{2}}

\CommentTok{\# Summary of the model}

\FunctionTok{summary}\NormalTok{(regression\_model2) }\hspace*{\fill}\NormalTok{\circled{3}}
\end{Highlighting}
\end{Shaded}

\begin{description}
\tightlist
\item[\circled{0}]
We convert the \texttt{treatment} variable to a factor variable using
the \texttt{as.factor()} function. This is necessary for R to recognize
the variable as a categorical variable.
\item[\circled{1}]
We specify the reference level for the \texttt{treatment} variable using
the \texttt{relevel()} function. In this case, we set the reference
level to ``therapy1''. This is likely the default level, but we are
specifying it here for clarity. If we had more than 2 levels, we could
specify a different reference level using the \texttt{ref} argument. For
example, `ref = ``control''
\item[\circled{2}]
We fit a regression model with the \texttt{treatment\_group} variable as
the predictor variable. The outcome variable is
\texttt{aggression\_level}. The data are specified using the
\texttt{data} argument.
\item[\circled{3}]
We view the results of the regression model using the \texttt{summary()}
function. The output includes the coefficients, standard errors,
t-values, p-values, and R-squared value of the model.
\end{description}

\begin{verbatim}

Call:
lm(formula = aggression_level ~ treatment_group, data = regression_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-4.6800 -1.4882 -0.0185  1.3460  4.4131 

Coefficients:
                        Estimate Std. Error t value Pr(>|t|)    
(Intercept)               4.6800     0.2843  16.464  < 2e-16 ***
treatment_grouptherapy2   1.3201     0.4103   3.217  0.00175 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.05 on 98 degrees of freedom
Multiple R-squared:  0.09554,   Adjusted R-squared:  0.08631 
F-statistic: 10.35 on 1 and 98 DF,  p-value: 0.001753
\end{verbatim}

The output tells us that the model is significant, and that the beta
values for the treatment\_group therapy2 is different from the reference
group (therapy1) which is the intercept of this model. The beta values
are the difference in the outcome variable between the reference group
and the other groups. The beta value for the reference group is the mean
of the outcome variable (aggression\_level) for that group. The beta
values for therapy2 is the difference in the mean outcome variable for
that group and the reference group.

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Interpreting the results of a regression model with a categorical
predictor variable}, rightrule=.15mm, left=2mm]

When we have a categorical predictor variable, the interpretation of the
beta values changes. The beta values are the difference in the outcome
variable between the reference category and the other categories. The
reference category is the category that is not included in the model.
The beta value for the reference category is the mean of the outcome
variable for that category. The beta values for the other categories are
the difference in the outcome variable between that category and the
reference category.

If you want to see this done another way, if your predictor has 2
levels, you can conduct a t-test and compare the means of the two
groups. The t-test will give you the same results as the regression
model. The t-test is a simpler way to compare the means of two groups,
but the regression model is more flexible and can handle more complex
models (by adding more predictor variables).

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-warning-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Testing assumptions with a categorical predictor variable}, rightrule=.15mm, left=2mm]

When we have a categorical predictor variable, we need to be careful
about how we interpret the assumptions of linear regression. The
assumptions of linearity, normality and homoscedasticity are still
relevant. However, because we have a categorical predictor, the plots
will look different. We need to check the assumptions for each level of
the categorical predictor variable. Some of the plots will change to
reflect this.

\end{tcolorbox}

\section{Comparing multiple levels of predictor variables (estimated
marginal
means)}\label{comparing-multiple-levels-of-predictor-variables-estimated-marginal-means}

If we have more than 2 levels of the predictor variable, we can compare
each level to the reference level. We can also compare all levels to
each other using the \texttt{emmeans} package. The \texttt{emmeans}
package provides estimated marginal means for the levels of a predictor
variable. We can use the \texttt{emmeans()} function to calculate the
estimated marginal means for the levels of the predictor variable.

For this example, let's use the mtcar dataset, which contains
information about cars. We will conduct a regression analysis with the
\texttt{cyl} variable as the predictor variable and the \texttt{mpg}
variable as the outcome variable. The \texttt{cyl} variable has 3 levels
(4, 6, and 8 cylinders). We will compare the mean \texttt{mpg} for each
level of the \texttt{cyl} variable.

\phantomsection\label{annotated-cell-54}%
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the mtcars dataset}

\FunctionTok{data}\NormalTok{(mtcars)}

\CommentTok{\# Convert the cyl variable to a factor variable}

\NormalTok{mtcars}\SpecialCharTok{$}\NormalTok{cyl }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{cyl)}

\CommentTok{\# Fit a regression model with the cyl variable as the predictor variable}

\NormalTok{regression\_model3 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cyl, }\AttributeTok{data =}\NormalTok{ mtcars) }\hspace*{\fill}\NormalTok{\circled{1}}

\CommentTok{\# Summary of the model}

\FunctionTok{summary}\NormalTok{(regression\_model3) }\hspace*{\fill}\NormalTok{\circled{2}}

\CommentTok{\# Compare the levels of the predictor variable with pairwise comparisons}

\FunctionTok{library}\NormalTok{(emmeans)}
\end{Highlighting}
\end{Shaded}

\begin{description}
\tightlist
\item[\circled{3}]
We use the \texttt{emmeans()} function from the \texttt{emmeans} package
to calculate the estimated marginal means for the levels of the
\texttt{cyl} variable. The \texttt{pairwise\ \textasciitilde{}\ cyl}
argument specifies that we want to compare the levels of the
\texttt{cyl} variable with pairwise comparisons.
\end{description}

\begin{verbatim}
Welcome to emmeans.
Caution: You lose important information if you filter this package's results.
See '? untidy'
\end{verbatim}

\phantomsection\label{annotated-cell-55}%
\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{emmeans}\NormalTok{(regression\_model3, pairwise }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cyl) }\hspace*{\fill}\NormalTok{\circled{3}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = mpg ~ cyl, data = mtcars)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.2636 -1.8357  0.0286  1.3893  7.2364 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  26.6636     0.9718  27.437  < 2e-16 ***
cyl6         -6.9208     1.5583  -4.441 0.000119 ***
cyl8        -11.5636     1.2986  -8.905 8.57e-10 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.223 on 29 degrees of freedom
Multiple R-squared:  0.7325,    Adjusted R-squared:  0.714 
F-statistic:  39.7 on 2 and 29 DF,  p-value: 4.979e-09

$emmeans
 cyl emmean    SE df lower.CL upper.CL
 4     26.7 0.972 29     24.7     28.7
 6     19.7 1.220 29     17.3     22.2
 8     15.1 0.861 29     13.3     16.9

Confidence level used: 0.95 

$contrasts
 contrast    estimate   SE df t.ratio p.value
 cyl4 - cyl6     6.92 1.56 29   4.441  0.0003
 cyl4 - cyl8    11.56 1.30 29   8.905  <.0001
 cyl6 - cyl8     4.64 1.49 29   3.112  0.0112

P value adjustment: tukey method for comparing a family of 3 estimates 
\end{verbatim}

The output tells us that the mean \texttt{mpg} for the 4-cylinder cars
is significantly different from the mean \texttt{mpg} for the 6-cylinder
cars and the 8-cylinder cars. The mean \texttt{mpg} for the 6-cylinder
cars is also significantly different from the mean \texttt{mpg} for the
8-cylinder cars. The estimated marginal means provide a way to compare
the levels of the predictor variable and determine if there are
significant differences between them.

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{What are estimated marginal means?}, rightrule=.15mm, left=2mm]

Estimated marginal means are the predicted means of the outcome variable
for each level of the predictor variable. They are calculated by
averaging the predicted values of the outcome variable for each level of
the predictor variable, while holding all other variables constant.
Estimated marginal means provide a way to compare the levels of the
predictor variable and determine if there are significant differences
between them.

\subsection{What is the difference between estimated marginal means and
actual means - how should I report
them?}\label{what-is-the-difference-between-estimated-marginal-means-and-actual-means---how-should-i-report-them}

Estimated marginal means are the predicted means of the outcome variable
for each level of the predictor variable. They are calculated by
averaging the predicted values of the outcome variable for each level of
the predictor variable, while holding all other variables constant.
Actual means are the observed means of the outcome variable for each
level of the predictor variable. The difference between estimated
marginal means and actual means is that estimated marginal means are
based on the regression model, while actual means are based on the
observed data. This allows the estimated marginal means to take into
account other variables in the model, for example.

When reporting the results of a regression analysis, it is common to
report the estimated marginal means rather than the actual means. This
is because the estimated marginal means take into account the effects of
other variables in the model, while the actual means do not. Reporting
the estimated marginal means can give a more accurate representation of
the relationship between the predictor variable and the outcome
variable.

\subsection{What are Tukey corrected
p-values?}\label{what-are-tukey-corrected-p-values}

Tukey corrected p-values are used to adjust for multiple comparisons in
a pairwise comparison analysis. When conducting multiple comparisons
between the levels of a predictor variable, there is an increased risk
of making a Type I error (false positive) due to the number of
comparisons being made. Tukey corrected p-values adjust for this
increased risk. This means that the p-values are adjusted to account for
the number of comparisons being made, reducing the likelihood of making
a false positive conclusion.

\end{tcolorbox}

\bookmarksetup{startatroot}

\chapter{Multiple Regression and Heirarchical
Regression}\label{multiple-regression-and-heirarchical-regression}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, rightrule=.15mm, left=2mm]

By the end of this session, you will be able to:

\begin{itemize}
\tightlist
\item
  Compare multiple regression to simple regression
\item
  Describe the assumptions of multiple regression
\item
  Consider sample size in regression
\item
  Interpret the output of Multiple regression
\item
  Conduct and interpret hierarchical multiple regression
\end{itemize}

\end{tcolorbox}

\section{What is multiple
regression?}\label{what-is-multiple-regression}

Multiple regression ia an extension of simple regression that allows us
to predict an outcome variable (Y) based on multiple predictors (X1, X2
\ldots).

\[ Y = b_1X_1 + b_2X_2 + b_0 \]

(The constant can be referred to in the equation as \textbf{c} or
\textbf{b0} )

\section{What are the assumptions of Multiple
Regression?}\label{what-are-the-assumptions-of-multiple-regression}

They are primarily the same as simple regression, so look back at the
assumptions of simple regression in that section. The additional
assumption of no \textbf{multicollinearity} applies, due to having
multiple predictors. The multicollinearity assumption means that
predictors should not be highly correlated. If they were, it would be
difficult to determine the unique contribution of each predictor to the
model.

\subsection{What is multicollinearity?}\label{what-is-multicollinearity}

Multicollinearity means that predictors correlated highly with each
other. This is not good because:

\begin{itemize}
\tightlist
\item
  It makes it difficult to determine the role of individual predictors
\item
  Increases the error of the model (higher standard errors)
\item
  Difficult to identify significant predictors
\item
  Wider confidence interval
\end{itemize}

\subsection{Testing multicollinearity}\label{testing-multicollinearity}

To test for multicollinearity, we can use the \texttt{mctest()} function
in R. This function is part of the \texttt{mctest} package. It performs
several tests for multicollinearity, including the Variance Inflation
Factor (VIF) and the Condition Index (CI).

To run the test, you pass the regression model to the \texttt{mctest()}
function. The function will then return the results of the tests.

\phantomsection\label{annotated-cell-56}%
\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# use the mctest package}
\CommentTok{\# install.packages(mctest)}

\FunctionTok{library}\NormalTok{(mctest)}

\NormalTok{m1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(aggression\_level }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treatment\_group }\SpecialCharTok{+}\NormalTok{ treatment\_duration }\SpecialCharTok{+}\NormalTok{ trust\_score, }\AttributeTok{data=}\NormalTok{regression\_data) }\hspace*{\fill}\NormalTok{\circled{1}}

\FunctionTok{mctest}\NormalTok{(m1) }\hspace*{\fill}\NormalTok{\circled{2}}
\end{Highlighting}
\end{Shaded}

\begin{description}
\tightlist
\item[\circled{1}]
In this code, we are running a regression model with three predictors
(treatment group, treatment duration, and trust score) and the outcome
variable aggression level.
\item[\circled{2}]
We then pass the regression model to the \texttt{mctest()} function to
test for multicollinearity.
\end{description}

\begin{verbatim}

Call:
omcdiag(mod = mod, Inter = TRUE, detr = detr, red = red, conf = conf, 
    theil = theil, cn = cn)


Overall Multicollinearity Diagnostics

                       MC Results detection
Determinant |X'X|:         0.9229         0
Farrar Chi-Square:         7.7960         0
Red Indicator:             0.1547         0
Sum of Lambda Inverse:     3.1728         0
Theil's Method:           -0.8800         0
Condition Number:         13.6549         0

1 --> COLLINEARITY is detected by the test 
0 --> COLLINEARITY is not detected by the test
\end{verbatim}

The \texttt{mctest()} function also takes an additional argument,
\texttt{type}, which specifies whether you want, the main tests, each
individual predictor, or both. The default is \texttt{type\ =\ "main"},
which will run the main tests.

\section{Sample size for multiple
regression}\label{sample-size-for-multiple-regression}

As the number of predictors increases, the sample size needed to run a
multiple regression analysis also increases. A common rule of thumb is
to have at least 10-15 participants per predictor. However, this is a
loose rule, and the actual number of participants needed will depend on
the complexity of the model and the effect size. Always run a power
analysis to determine the sample size needed for your study.

\section{Approaches to multiple regression: All predictors at
once}\label{approaches-to-multiple-regression-all-predictors-at-once}

There are different ways we could run a multiple regression analysis
depending on our research question and how we conceptualise the
relationship between the predictors and the outcome. One way is to
include all predictors at the same time. This is useful when we want to
know the combined predictive power of all the predictors at the same
time.

\begin{quote}
Research question: Do a client's treatment duration and treatment group
predict aggression level?
\end{quote}

\phantomsection\label{annotated-cell-57}%
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\AttributeTok{data =}\NormalTok{ regression\_data, aggression\_level }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treatment\_duration }\SpecialCharTok{+}\NormalTok{ treatment\_group) }\hspace*{\fill}\NormalTok{\circled{1}}
\end{Highlighting}
\end{Shaded}

\begin{description}
\tightlist
\item[\circled{1}]
Here we are including all of the predictors at the same time. Note that
we are using a plus sign + between each predictor - this means that no
interactions will be tested.
\end{description}

\subsection{Using categorical predictors in
R}\label{using-categorical-predictors-in-r}

\begin{itemize}
\tightlist
\item
  Treatment group is a categorical (also called ``nominal'' or
  ``factor'') variable
\item
  No special ``dummy coding'' is required in R to use categorical
  predictors in regression
\item
  R will use the first group as the reference category and test whether
  being in another group shows a significant difference
\item
  R chooses the reference group based on numerical value or alphabetical
  order
\item
  If you want you can change the reference category or ``force'' it
  using the relevel function:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{regression\_data}\SpecialCharTok{$}\NormalTok{treatment\_group }\OtherTok{\textless{}{-}} \FunctionTok{relevel}\NormalTok{(regression\_data}\SpecialCharTok{$}\NormalTok{treatment\_group, }\AttributeTok{ref =} \StringTok{"therapy1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{More information in categorical predictors in section}
@ref(catreg)

\subsection{Reviewing the output}\label{reviewing-the-output}

The output from this approach will look the same as simple regression,
except there will be an additional row for each predictor in the model
in the coefficients table.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(model1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = aggression_level ~ treatment_duration + treatment_group, 
    data = regression_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.9468 -1.1104  0.0205  0.9621  3.4481 

Coefficients:
                        Estimate Std. Error t value Pr(>|t|)    
(Intercept)             11.58713    0.77331  14.984  < 2e-16 ***
treatment_duration      -0.66024    0.07119  -9.274 4.96e-15 ***
treatment_grouptherapy2  0.85032    0.30449   2.793   0.0063 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.5 on 97 degrees of freedom
Multiple R-squared:  0.5206,    Adjusted R-squared:  0.5107 
F-statistic: 52.67 on 2 and 97 DF,  p-value: 3.267e-16
\end{verbatim}

As a reminder of what the output tells us:

\begin{itemize}
\tightlist
\item
  Multiple \(R^2\) = Total variance in outcome that is explained by the
  model
\item
  p-value = Statistical significance of the model
\item
  Coefficients = Contribution of each predictor to the model

  \begin{itemize}
  \tightlist
  \item
    Pr = Significance of the individual predictor
  \item
    Estimate = Change in the outcome level that occurs when the
    predictor increases by 1 unit of measurement (for continuous
    predictors) or the difference in the mean outcome level between the
    predictor and the reference category (for categorical predictors)
  \end{itemize}
\end{itemize}

\subsection{All predictors at once (testing
interactions)}\label{all-predictors-at-once-testing-interactions}

It might be the case that we are interested in whether the predictors
interact with each other to predict the outcome. For example, we might
want to know if the effect of treatment duration on aggression level
depends on the treatment group.

\begin{quote}
Research questions:
\end{quote}

\begin{itemize}
\tightlist
\item
  Do a client's treatment duration and treatment group predict
  aggression level
\item
  Do the predictors interact?
\end{itemize}

\phantomsection\label{annotated-cell-59}%
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\AttributeTok{data =}\NormalTok{ regression\_data, aggression\_level }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treatment\_duration }\SpecialCharTok{*}\NormalTok{ treatment\_group) }\hspace*{\fill}\NormalTok{\circled{1}}
\end{Highlighting}
\end{Shaded}

\begin{description}
\tightlist
\item[\circled{1}]
Here we are including all of the predictors at the same time. Note that
we are using an asterisk \texttt{*} between each predictor. This means
that interactions will be tested.
\end{description}

Reviewing the output

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(model2) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                                             Estimate Std. Error    t value
(Intercept)                                12.3529190  1.1006127 11.2236751
treatment_duration                         -0.7334435  0.1033086 -7.0995381
treatment_grouptherapy2                    -0.5615517  1.4753596 -0.3806202
treatment_duration:treatment_grouptherapy2  0.1394649  0.1425977  0.9780305
                                               Pr(>|t|)
(Intercept)                                3.599000e-19
treatment_duration                         2.166226e-10
treatment_grouptherapy2                    7.043260e-01
treatment_duration:treatment_grouptherapy2 3.305175e-01
\end{verbatim}

In the output, we get additional information in the coefficients table
about the interaction between variables. We can see from the output that
none of the interactions are significant. This being the case, we would
not interpret the main effects of the predictors in the model, and
instead, we would interpret the results based on the interaction between
the predictors.

There is more information on interactions in regression in the section
on moderation.

\subsection{Hierarchical multiple regression: Theory driven ``blocks''
of
variables}\label{hierarchical-multiple-regression-theory-driven-blocks-of-variables}

When we have multiple predictors, it might be the case that we have
previous research or theory to guide how we run the analysis. For
example, we might know that treatment duration and therapy group are
likely to predict the outcome, based on the results of previous studies.
However, we might also want to check whether client's level of trust in
the clinician has any \textbf{additional} impact on our ability to
predict the outcome (aggression level).

This being the case, we could run a hierarchical multiple regression
analysis. This is where we run the regression in ``blocks'' (groups) of
variables. We start with a baseline model (Model 0) and then add
additional predictors in each subsequent model.

How we group our predictors will depend on our research question and the
theory behind the relationship between the predictors and the outcome.
As such, we should have a clear rationale for how we group our
predictors.

To do this, we run three regression models:

\begin{itemize}
\tightlist
\item
  Model 0: the constant (intercept)
\item
  Model 1: treatment duration and therapy group
\item
  Model 2: treatment duration and therapy group and trust score
\end{itemize}

We then compare the two regression models to see if:

\begin{itemize}
\tightlist
\item
  Model 1 is better than Model 0 (the intercept)
\item
  Model 2 is better than Model 1
\end{itemize}

The intercept (null) model is the simplest model we can run. It is a
model that only includes the constant (intercept) and no predictors.
This model is used as a baseline to compare the other models to.
Therefore, we don't really interpret the null model in isolation,
rather, we examine whether another model (with predictors) is more
useful in predicting the outcome than the null model.

\textbf{Hierarchical multiple regression: Running and comparing 2
models}

\phantomsection\label{annotated-cell-61}%
\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# run regression using the same method as above}
\NormalTok{model0 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\AttributeTok{data =}\NormalTok{ regression\_data, aggression\_level }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{) }\hspace*{\fill}\NormalTok{\circled{1}}
\NormalTok{model1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\AttributeTok{data =}\NormalTok{ regression\_data, aggression\_level }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treatment\_duration }\SpecialCharTok{+}\NormalTok{ treatment\_group) }\hspace*{\fill}\NormalTok{\circled{2}}
\NormalTok{model2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\AttributeTok{data =}\NormalTok{ regression\_data, aggression\_level }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treatment\_duration }\SpecialCharTok{+}\NormalTok{ treatment\_group }\SpecialCharTok{+}\NormalTok{ trust\_score) }\hspace*{\fill}\NormalTok{\circled{3}}


\DocumentationTok{\#\# use the aov() command to compare the models}
\FunctionTok{anova}\NormalTok{(model0,model1,model2) }\hspace*{\fill}\NormalTok{\circled{4}}
\end{Highlighting}
\end{Shaded}

\begin{description}
\tightlist
\item[\circled{1}]
Here we are running the null model (Model 0) with only the intercept
\item[\circled{2}]
Here we are running Model 1 with treatment duration and treatment group
\item[\circled{3}]
Here we are running Model 2 with treatment duration, treatment group,
and trust score
\item[\circled{4}]
We then use the \texttt{anova()} command to compare the models
\end{description}

\begin{verbatim}
Analysis of Variance Table

Model 1: aggression_level ~ 1
Model 2: aggression_level ~ treatment_duration + treatment_group
Model 3: aggression_level ~ treatment_duration + treatment_group + trust_score
  Res.Df    RSS Df Sum of Sq       F    Pr(>F)    
1     99 455.27                                   
2     97 218.26  2   237.013 52.2195 4.507e-16 ***
3     96 217.86  1     0.399  0.1757     0.676    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

When we run the \texttt{anova()} command, we get an output that tells us
whether the additional predictors in each model significantly change the
R\^{}2 value of the model (reducing the residual variance of the model).
We can see from the output that Model 1 is significantly better than the
null model (Model 0), and Model 2 is not significantly better than Model
1.

\subsection{Model performance metrics}\label{model-performance-metrics}

When we run a hierarchical multiple regression analysis, we are
interested in whether the additional predictors in each model
significantly change the R\^{}2 value of the model. However, adding
additional predictors to the model will always increase the R\^{}2 value
of the model somewhat, even if the predictors are not useful This is
because the R\^{}2 value is based on the amount of variance in the
outcome that is explained by the predictors in the model.

Because of this, we should also consider other metrics of model
performance, such as the AIC (Akaike Information Criterion) and BIC
(Bayesian Information Criterion). These metrics take into account the
number of predictors in the model and penalise the model for having too
many predictors. A lower AIC or BIC value indicates a better model. The
\texttt{AIC()} and \texttt{BIC()} functions in R can be used to
calculate these metrics.

\phantomsection\label{annotated-cell-62}%
\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{AIC}\NormalTok{(model0,model1,model2) }\hspace*{\fill}\NormalTok{\circled{1}}

\FunctionTok{BIC}\NormalTok{(model0,model1,model2) }\hspace*{\fill}\NormalTok{\circled{2}}
\end{Highlighting}
\end{Shaded}

\begin{description}
\tightlist
\item[\circled{1}]
Here we are using the \texttt{AIC()} function to calculate the AIC value
for each model
\item[\circled{2}]
Here we are using the \texttt{BIC()} function to calculate the BIC value
for each model
\end{description}

\begin{verbatim}
       df      AIC
model0  2 439.3603
model1  4 369.8394
model2  5 371.6566
       df      BIC
model0  2 444.5707
model1  4 380.2601
model2  5 384.6825
\end{verbatim}

We can see from the output that Model 1 has the lowest AIC and BIC
values, indicating that it is the best model of the three. We phrase
this has having the best fit to the data, given the number of predictors
in the model. Although, in this example, model 2 is not significantly
better than model 1, it could be the case that you have a different
research question or theory where the \texttt{anova()} test would show
that the model with the additional predictors is significantly better.
You could then use the AIC and BIC values to determine which model is
actually the best fit to the data.

Remember that the accuracy of your models will depend on the quality of
your data and the assumptions of regression being met. How you interpret
the results will depend on your research question and the theory behind
the relationship between the predictors and the outcome. The analysis
cannot prove causation, only association. It is up to you to design your
study and analysis to best answer your research question.

\bookmarksetup{startatroot}

\chapter{Moderation}\label{moderation}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{At the end of this chapter, you will be able to:}, rightrule=.15mm, left=2mm]

\begin{itemize}
\tightlist
\item
  Describe moderation
\item
  Check the assumptions of moderation
\item
  Interpret the results of moderation analysis
\item
  Mean centre data for moderation analysis
\item
  Bootstrap moderation analysis
\end{itemize}

\end{tcolorbox}

\section{What is moderation?}\label{what-is-moderation}

Theoretically, we can think of moderation as a relationship between a
predictor (X) and an outcome (Y) that is affected by a third variable
(M). This means that the relationship between X and Y exists, but the
strength of this relationship is affected by the level of M. We could
also phrase this as the relationship between X and Y being different,
depending on the level of M.

A moderated relationship can be visualised as follows:

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{images/moderation_model1.png}}

}

\caption{A moderated relationship}

\end{figure}%

In the above model, we theorise that \texttt{Time\ in\ counselling}
predicts \texttt{General\ Wellbeing} but the strength of the
relationship is affected by the \texttt{level\ of\ Rapport} with
psychologist. As with all of our hypotheses, the moderated relationship
is theorised based on previous research evidence and/or strong
theoretical reasoning.

\section{What packages do we need?}\label{what-packages-do-we-need}

We can do a lot of the moderation analysis with just the \texttt{lm()}
function in R. However, there are some packages that can make the
process easier and help us understand the moderation better. These
include:

\begin{itemize}
\tightlist
\item
  \textbf{gvlma} (for checking assumptions)
\item
  \textbf{interactions} (for generating interaction plot)
\item
  \textbf{Rockchalk} (for testing simple slopes)
\item
  \textbf{car} (includes a \textbf{Boot()} function to bootstrap
  regression models )
\end{itemize}

\section{Moderation as an
interaction}\label{moderation-as-an-interaction}

When we say that a variable moderates the relationship between two other
variables, we are saying that the relationship between the two variables
is not the same at different levels of the moderator. This is similar to
an interaction in standard regression analysis. In fact, we can tell if
there is a moderation effect by looking at the interaction term in a
regression model.

\texttt{lm(Y\ \textasciitilde{}\ X\ +\ M\ +\ X*M)}

In the above formula, the interaction term is \texttt{X*M}. This term is
the product of the two variables (X and M). If the interaction term is
significant, this suggests that there is a moderation effect.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{images/moderation_model1.png}}

}

\caption{A moderated relationship}

\end{figure}%

If the interaction term is significant, this suggests that there is a
moderation effect.

However, a moderator can effect the direction and/or strength of a
relationship between X and Y. The interaction effect might only be
significant when the moderator is at a certain level. The below plot
shows a moderated relationship:

\includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{moderation_files/figure-pdf/unnamed-chunk-3-1.pdf}

In the plot above:

\begin{itemize}
\tightlist
\item
  The blue line is the ``standard'' regression line
\item
  The black line is when the moderator is ``low'' (-1sd)
\item
  The dotted line is when the moderator is ``high'' (+1sd)
\end{itemize}

To understand the moderation effect fully, we need some additional
analysis.

\section{Moderation: step-by-step}\label{moderation-step-by-step}

\subsection{Run your model}\label{run-your-model}

As with our previous models, we start by running a regression model. In
this case, we are interested in the interaction between our predictor
and moderator. The model is named \texttt{fitMod} and is run using the
\texttt{lm()} function. The name is not important, but it is good
practice to name your models in a way that makes sense to you.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitMod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(generalWellbeing }\SpecialCharTok{\textasciitilde{}}\NormalTok{ timeInCounselling }\SpecialCharTok{*}\NormalTok{rapportLevel , }\AttributeTok{data =}\NormalTok{ Moddata) }\CommentTok{\#Model interacts IV \& moderator}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Grand Mean Centering}, rightrule=.15mm, left=2mm]

In many moderation models, people use grand mean centering. This is a
way to make the results of the model easier to interpret. Especially if
you have a continuous moderator, it can be helpful to center the data
around the grand mean. This means that the mean of the data is
subtracted from each value. This makes the mean of the data 0.

This is becuase regression coefficients (b values) are based on
predicting Y when X = 0, but not all measures actually have a zero
value. To make results easier to interpret, we can centre our data
around the grand mean of the data (making the mean 0). The mean of the
full sample is subtracted from the value. This is similar to z-score
(i.e.~a standardised score)

To do this in R, we can use the \textbf{scale()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    timeInCounselling\_centred    }\OtherTok{\textless{}{-}} \FunctionTok{scale}\NormalTok{(timeInCounselling, }\AttributeTok{center=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{scale=}\ConstantTok{FALSE}\NormalTok{) }\CommentTok{\#Centering X; }
\NormalTok{    rapportLevel\_centred    }\OtherTok{\textless{}{-}} \FunctionTok{scale}\NormalTok{(rapportLevel,  }\AttributeTok{center=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{scale=}\ConstantTok{FALSE}\NormalTok{) }\CommentTok{\#Centering M;}
\end{Highlighting}
\end{Shaded}

We then use the centred data in our analysis.

We can see that the difference between the original data is the mean of
the data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  timeInCounselling\_centred    }\OtherTok{\textless{}{-}} \FunctionTok{scale}\NormalTok{(timeInCounselling, }\AttributeTok{center=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{scale=}\ConstantTok{FALSE}\NormalTok{) }\CommentTok{\#Centering X; }
  
\NormalTok{  timeInCounselling}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  [1]  3.7580974  5.0792900 12.2348333  6.2820336  6.5171509 12.8602599
  [7]  7.8436648  0.9397551  3.2525886  4.2173521 10.8963272  7.4392553
 [13]  7.6030858  6.4427309  3.7766355 13.1476525  7.9914019  1.8664686
 [19]  8.8054236  4.1088344  1.7287052  5.1281003  1.8959822  3.0844351
 [25]  3.4998429  0.7467732  9.3511482  6.6134925  1.4474523 11.0152597
 [31]  7.7058569  4.8197141  9.5805026  9.5125340  9.2863243  8.7545610
 [37]  8.2156706  5.7523532  4.7761493  4.4781160  3.2211721  5.1683309
 [43]  0.9384146 14.6758239 10.8318480  1.5075657  4.3884607  4.1333786
 [49]  9.1198605  5.6665237  7.0132741  5.8858130  5.8285182 11.4744091
 [55]  5.0969161 12.0658824  0.1950112  8.3384550  6.4954170  6.8637663
 [61]  7.5185579  3.9907062  4.6671705  1.9256985  1.7128351  7.2141146
 [67]  7.7928391  6.2120169  9.6890699 14.2003387  4.0358753  3.2366755
 [73] 10.0229541  3.1631969  3.2479655 10.1022855  4.8609080  1.1171292
 [79]  6.7252139  5.4444346  6.0230567  7.5411216  4.5173599  8.5775062
 [85]  5.1180538  7.3271279 10.3873561  7.7407260  4.6962737 10.5952305
 [91]  9.9740154  8.1935878  6.9549269  3.4883757 11.4426098  3.5989617
 [97] 14.7493320 12.1304425  5.0571986  1.8943164
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{head}\NormalTok{(timeInCounselling\_centred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            [,1]
[1,] -2.72442479
[2,] -1.40323216
[3,]  5.75231105
[4,] -0.20048864
[5,]  0.03462873
[6,]  6.37773774
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
   \FunctionTok{mean}\NormalTok{(timeInCounselling)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 6.482522
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  timeInCounselling[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{{-}}\NormalTok{timeInCounselling\_centred[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 6.482522
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Centering Data}
\NormalTok{Moddata}\SpecialCharTok{$}\NormalTok{timeInCounselling\_centred    }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{scale}\NormalTok{(timeInCounselling, }\AttributeTok{center=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{scale=}\ConstantTok{FALSE}\NormalTok{)) }

\CommentTok{\#Centering IV; }
\NormalTok{Moddata}\SpecialCharTok{$}\NormalTok{rapportLevel\_centred    }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{scale}\NormalTok{(rapportLevel,  }\AttributeTok{center=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{scale=}\ConstantTok{FALSE}\NormalTok{)) }\CommentTok{\#Centering moderator; }

\CommentTok{\#Moderation "By Hand" with centred data}
\FunctionTok{library}\NormalTok{(gvlma)}
\NormalTok{fitMod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(generalWellbeing }\SpecialCharTok{\textasciitilde{}}\NormalTok{ timeInCounselling\_centred }\SpecialCharTok{*}\NormalTok{rapportLevel\_centred  , }\AttributeTok{data =}\NormalTok{ Moddata) }\CommentTok{\#Model interacts IV \& moderator}

\FunctionTok{library}\NormalTok{(interactions)}
\NormalTok{ ip }\OtherTok{\textless{}{-}} \FunctionTok{interact\_plot}\NormalTok{(fitMod, }\AttributeTok{pred =}\NormalTok{ timeInCounselling\_centred, }\AttributeTok{modx =}\NormalTok{ rapportLevel\_centred)}
\NormalTok{ ip}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{moderation_files/figure-pdf/unnamed-chunk-7-1.pdf}}

\subsubsection{Do I need to mean centre my
data?}\label{do-i-need-to-mean-centre-my-data}

It's really about ease of interpretation. It is worth noting:

\begin{itemize}
\tightlist
\item
  It does not change the results of your interaction (coefficient,
  standard error or significance tests).
\item
  It will change the results of the direct effects (the individual
  predictors in your model).
\item
  It is a step that tries to ensure that the coefficients of the
  predictor and moderator are meaningful to interpret, in relation to
  each other.
\item
  In some cases, it might not be necessary to mean centre at all.
  However, there is no harm in doing so, and it could potentially be
  helpful.
\end{itemize}

Hayes (2013) discusses mean centering, pp.~282-290.

\end{tcolorbox}

\subsection{Step 2: Check assumptions}\label{step-2-check-assumptions}

We can use the \texttt{gvlma} function to check regression assumptions
for moderation analysis.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gvlma)}
\FunctionTok{gvlma}\NormalTok{(fitMod)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = generalWellbeing ~ timeInCounselling_centred * rapportLevel_centred, 
    data = Moddata)

Coefficients:
                                   (Intercept)  
                                       21.1851  
                     timeInCounselling_centred  
                                        0.8971  
                          rapportLevel_centred  
                                        0.5842  
timeInCounselling_centred:rapportLevel_centred  
                                        0.1495  


ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS
USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:
Level of Significance =  0.05 

Call:
 gvlma(x = fitMod) 

                    Value p-value                   Decision
Global Stat        9.6949 0.04589 Assumptions NOT satisfied!
Skewness           7.7571 0.00535 Assumptions NOT satisfied!
Kurtosis           1.2182 0.26972    Assumptions acceptable.
Link Function      0.5287 0.46716    Assumptions acceptable.
Heteroscedasticity 0.1910 0.66207    Assumptions acceptable.
\end{verbatim}

The ``global stat'' is an attempt to check multiple assumptions of
linear model: Pena, E. A., \& Slate, E. H. (2006). Global validation of
linear model assumptions. Journal of the American Statistical
Association, 101(473), 341-354.

Since one of the underlying assumptions is violated, the overall stat is
also not acceptable.

The data looks skewed, we should transform it or perhaps use
bootstrapping (we will do this later).

\subsection{Multicollinearity?}\label{multicollinearity}

For more information on multicollinearity in moderation, see Clelland,
G. H., Irwin, J. R., Disatnik, D., \& Sivan, L. (2017).
Multicollinearity is a red herring in the search for moderator
variables: A guide to interpreting moderated multiple regression models
and a critique of Iacobucci, Schneider, Popovich, and Bakamitsos (2016).
Behavior research methods, 49(1), 394-402.

\subsection{Step 3: Moderation
Analysis}\label{step-3-moderation-analysis}

Now that we have run our model and checked the assumptions, we can
interpret the results. So we run the \texttt{summary()} function on our
model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitMod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(generalWellbeing }\SpecialCharTok{\textasciitilde{}}\NormalTok{ timeInCounselling\_centred }\SpecialCharTok{*}\NormalTok{rapportLevel\_centred  , }\AttributeTok{data =}\NormalTok{ Moddata) }\CommentTok{\#Model interacts IV \& moderator}
 \CommentTok{\#Model interacts IV \& moderator}
\FunctionTok{summary}\NormalTok{(fitMod)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = generalWellbeing ~ timeInCounselling_centred * rapportLevel_centred, 
    data = Moddata)

Residuals:
    Min      1Q  Median      3Q     Max 
-18.121  -8.938  -0.670   5.840  37.396 

Coefficients:
                                               Estimate Std. Error t value
(Intercept)                                    21.18508    1.14115  18.565
timeInCounselling_centred                       0.89707    0.33927   2.644
rapportLevel_centred                            0.58416    0.15117   3.864
timeInCounselling_centred:rapportLevel_centred  0.14948    0.04022   3.716
                                               Pr(>|t|)    
(Intercept)                                     < 2e-16 ***
timeInCounselling_centred                      0.009569 ** 
rapportLevel_centred                           0.000203 ***
timeInCounselling_centred:rapportLevel_centred 0.000340 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 11.33 on 96 degrees of freedom
Multiple R-squared:  0.2737,    Adjusted R-squared:  0.251 
F-statistic: 12.06 on 3 and 96 DF,  p-value: 9.12e-07
\end{verbatim}

The results above show that there is a moderated effect (the interaction
term is significant). When the interaction term is significant, this
suggests that there is a moderation effect (i.e.~the relationship
between the predictor and outcome is different at different levels of
the moderator). So we do not need to interpret the main effects
(timeInCounselling and rapportLevel).

\subsubsection{Visualising the moderation
effect}\label{visualising-the-moderation-effect}

We use an approach called \textbf{simple slopes} to visualise the
moderation effect. This is a way to see how the relationship between the
predictor and outcome changes at different levels of the moderator.

\texttt{interact\_plot(fitMod,\ pred\ =\ timeInCounselling\_centred,\ modx\ =\ rapportLevel\_centred)}

\pandocbounded{\includegraphics[keepaspectratio]{moderation_files/figure-pdf/unnamed-chunk-10-1.pdf}}

In the case of the above plot, we can see that the relationship between
\texttt{timeInCounselling} and \texttt{generalWellbeing} is stronger
when \texttt{rapportLevel} is high (+1sd) compared to when it is low
(-1sd).

The \textbf{rockchalk} package includes useful functions for visualising
simple slopes

\phantomsection\label{annotated-cell-66}%
\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rockchalk)}

\NormalTok{fitMod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(generalWellbeing }\SpecialCharTok{\textasciitilde{}}\NormalTok{ timeInCounselling }\SpecialCharTok{*}\NormalTok{rapportLevel  , }\AttributeTok{data =}\NormalTok{ Moddata)}
\FunctionTok{summary}\NormalTok{(fitMod)}


\NormalTok{slopes }\OtherTok{\textless{}{-}} \FunctionTok{plotSlopes}\NormalTok{(fitMod, }\AttributeTok{modx =} \StringTok{"rapportLevel"}\NormalTok{, }\AttributeTok{plotx =} \StringTok{"timeInCounselling"}\NormalTok{) }\hspace*{\fill}\NormalTok{\circled{1}}
\end{Highlighting}
\end{Shaded}

\begin{description}
\tightlist
\item[\circled{2}]
The \texttt{testSlopes()} function tests the simple slopes. This is to
see if the slopes are significant.
\item[\circled{3}]
The \texttt{plot()} function is used to plot the results of the simple
slopes test. This tells us at which level of the moderator the slopes
are significant.
\end{description}

\pandocbounded{\includegraphics[keepaspectratio]{moderation_files/figure-pdf/unnamed-chunk-11-1.pdf}}

\phantomsection\label{annotated-cell-67}%
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{testSlopes }\OtherTok{\textless{}{-}} \FunctionTok{testSlopes}\NormalTok{(slopes) }\hspace*{\fill}\NormalTok{\circled{2}}
\FunctionTok{plot}\NormalTok{(testSlopes) }\hspace*{\fill}\NormalTok{\circled{3}}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{moderation_files/figure-pdf/unnamed-chunk-11-2.pdf}}

\begin{verbatim}

Call:
lm(formula = generalWellbeing ~ timeInCounselling * rapportLevel, 
    data = Moddata)

Residuals:
    Min      1Q  Median      3Q     Max 
-18.121  -8.938  -0.670   5.840  37.396 

Coefficients:
                               Estimate Std. Error t value Pr(>|t|)    
(Intercept)                    17.28006    3.17944   5.435 4.15e-07 ***
timeInCounselling               0.15510    0.42033   0.369  0.71296    
rapportLevel                   -0.38484    0.29916  -1.286  0.20140    
timeInCounselling:rapportLevel  0.14948    0.04022   3.716  0.00034 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 11.33 on 96 degrees of freedom
Multiple R-squared:  0.2737,    Adjusted R-squared:  0.251 
F-statistic: 12.06 on 3 and 96 DF,  p-value: 9.12e-07

Values of rapportLevel OUTSIDE this interval:
        lo         hi 
-11.580166   3.634439 
cause the slope of (b1 + b2*rapportLevel)timeInCounselling to be statistically significant
\end{verbatim}

In the example above, we can see that the slope of the relationship
between \texttt{timeInCounselling} and \texttt{generalWellbeing} is
significant when the moderator is between -11.58 and 3.63. We can look
at the simple slopes plot to see how the relationship changes at these
levels. We can also check whether -11.58 and 3.63 are within the range
of the moderator variable (\texttt{rapportLevel}) - if this is not the
case, we only consider the values that are within the possible range of
the moderator variable.

\subsection{Step 4: Bootstrapping}\label{step-4-bootstrapping}

We saw in the regression diagnostics that the data was skewed. We can
use bootstrapping to get a more accurate estimate of the confidence
intervals. The \textbf{car} package includes a function to bootstrap any
regression

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(car)}
\end{Highlighting}
\end{Shaded}

\begin{description}
\tightlist
\item[\circled{1}]
The \texttt{Boot()} function is used to bootstrap the model. We save the
results as an object called \texttt{bootstrapModel}.
\item[\circled{2}]
We can use the \texttt{confint()} function to get the confidence
intervals of the original model.
\item[\circled{3}]
We can use the \texttt{confint()} function to get the confidence
intervals of the bootstrapped model.
\item[\circled{4}]
We can use the \texttt{summary()} function to get a summary of the
bootstrapped model.
\item[\circled{5}]
We can use the \texttt{hist()} function to plot a histogram of the
bootstrapped model.
\end{description}

\begin{verbatim}
Loading required package: carData
\end{verbatim}

\phantomsection\label{annotated-cell-69}%
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bootstrapModel }\OtherTok{\textless{}{-}} \FunctionTok{Boot}\NormalTok{(fitMod, }\AttributeTok{R=}\DecValTok{999}\NormalTok{) }\hspace*{\fill}\NormalTok{\circled{1}}

\FunctionTok{confint}\NormalTok{(fitMod) }\hspace*{\fill}\NormalTok{\circled{2}}
\FunctionTok{confint}\NormalTok{(bootstrapModel) }\hspace*{\fill}\NormalTok{\circled{3}}
\FunctionTok{summary}\NormalTok{(bootstrapModel) }\hspace*{\fill}\NormalTok{\circled{4}}
\FunctionTok{hist}\NormalTok{(bootstrapModel) }\hspace*{\fill}\NormalTok{\circled{5}}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{moderation_files/figure-pdf/unnamed-chunk-12-1.pdf}}

\begin{verbatim}
                                     2.5 %     97.5 %
(Intercept)                    10.96891826 23.5912086
timeInCounselling              -0.67926290  0.9894532
rapportLevel                   -0.97866229  0.2089882
timeInCounselling:rapportLevel  0.06963667  0.2293205
Bootstrap bca confidence intervals

                                     2.5 %     97.5 %
(Intercept)                    11.57230420 23.7222700
timeInCounselling              -0.61780918  1.0397199
rapportLevel                   -0.90786799  0.2558502
timeInCounselling:rapportLevel  0.05806412  0.2146814

Number of bootstrap replications R = 999 
                               original    bootBias   bootSE  bootMed
(Intercept)                    17.28006 -0.13667103 3.165301 17.05431
timeInCounselling               0.15510  0.01637117 0.399550  0.15929
rapportLevel                   -0.38484  0.00716631 0.294061 -0.38218
timeInCounselling:rapportLevel  0.14948 -0.00052838 0.038516  0.14974
\end{verbatim}

Comparing the confidence intervals of the original model and the
bootstrapped model can give us an idea of how accurate our original
model is. If the confidence intervals are very different, this suggests
that the original model is not very accurate. In this case, we might
want to use the bootstrapped model instead.

The bootBias and bootSE columns in the summary of the bootstrapped model
can also give us an idea of how much the bootstrapped model differs from
the original model. If the bootBias is very different from 0, this
suggests that the original model is biased. If the bootSE is very
different from the original SE, this suggests that the original model is
not very accurate.

In the data above, we can see that the bootBias and bootSE are very
close to 0 and the original SE, respectively. This suggests that the
deviation from normality is not a big problem in this case.

\bookmarksetup{startatroot}

\chapter{Mediation analysis}\label{mediation-analysis}

This example borrows heavily from Demos \& Salas (2019). \emph{A
Language, not a Letter: Learning Statistics in R} (Chapter 14)

\section{Overview}\label{overview}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{At the end of this chapter, you will be able to:}, rightrule=.15mm, left=2mm]

\begin{itemize}
\tightlist
\item
  Explain the difference between mediation and moderation.
\item
  Apply the Baron and Kenny approach to mediation analysis in R.
\item
  Apply the Preacher \& Hayes (2004) mediation approach in R.
\item
  Interpret the output of mediation analysis.
\end{itemize}

\end{tcolorbox}

\section{What is mediation?}\label{what-is-mediation}

The basic premise of mediation is that the relationship between a
predictor (X) and an outcome (Y) is mediated by another variable (M).
That is to say, X predicts M, which in turn predicts Y.

When thinking about mediation, form a research perspective, we could be
saying that the relationship between X and Y is \emph{explained by} M.
That is to say, M is the mechanism through which X influences Y.

To use an example, consider the relationship between internet usage and
self-esteem. We might know from previous findings that internet usage
predicts self-esteem. Perhaps higher internet usage is associated with
lower self-esteem.

However, we might theorise that the relationship between internet usage
and self-esteem is mediated by social media usage. That is to say,
internet usage predicts social media usage, which in turn predicts
self-esteem. If this turns out to be the case, then we might say that
social media usage is the mechanism through which internet usage
influences self-esteem. That would mean that it is not internet usage
\emph{per se} that influences self-esteem, but rather the way in which
people use the internet (i.e., through social media) that explains the
relationship.

Importantly, theoerising mediation (like all models) requires a strong
rationale that is based on previous research evidence or a
well-developed theoretical model. We need good reason to believe that
the relationship between X and Y is mediated by M, we cannot simply test
for mediation with no basis and use the result to justify a post facto
theory.

\pandocbounded{\includegraphics[keepaspectratio]{images/mediation_model1.png}}

In the above model, we theorise that the relationship between internet
use and self-esteem is mediated by social media usage.

\section{What is the difference between mediation and
moderation?}\label{what-is-the-difference-between-mediation-and-moderation}

The key difference between mediation and moderation is that mediation is
about the third variable (M) \emph{explaining} the relationship between
X and Y, whereas moderation is about the third variable (M)
\emph{changing} the relationship between X and Y.

\section{Mediation analysis}\label{mediation-analysis-1}

Mediation analysis is based on regression analysis. In the past it was
tested using the Baron and Kenny (1986) approach, but more recently the
Preacher \& Hayes (2004) bootstrapping approach has become more popular.

Baron and Kenny (1986) originally used a 4-step regression model to test
each of these relationships. The Sobel test is then used to test the
significance of mediation.

A summary of the logic of mediation (mirroring the Baron and Kenny
approach) is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The direct relationship between X and Y should be significant
\item
  The relationship between X and M should be significant
\item
  The relationship between M and Y (controlling for X) should be
  significant
\item
  When controlling for M, the strength of the relationship between X and
  Y decreases and is \textbf{not} significant
\end{enumerate}

\subsection{What packages do we need?}\label{what-packages-do-we-need-1}

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{library}\NormalTok{(mediation) }\CommentTok{\#Mediation package}
    
    \FunctionTok{library}\NormalTok{(multilevel) }\CommentTok{\#Sobel Test}
    
    \FunctionTok{library}\NormalTok{(bda) }\CommentTok{\#Another Sobel Test option}
    
    \FunctionTok{library}\NormalTok{(gvlma) }\CommentTok{\#Testing Model Assumptions }
    
    \FunctionTok{library}\NormalTok{(stargazer) }\CommentTok{\#Handy regression tables}
\end{Highlighting}
\end{Shaded}

\section{Mediation analysis (the Baron and Kenny
Approach)}\label{mediation-analysis-the-baron-and-kenny-approach}

\subsection{Conducting mediation analysis (the Baron and Kenny
Approach)}\label{conducting-mediation-analysis-the-baron-and-kenny-approach}

Baron \& Kenny (1986) originally used a 4-step regression model to test
each of these relationships. The sobel test is then used to test the
significance of mediation. The idea with this approach is that you can
test each of the paths in the mediation model to see if they are
significant. If they are, you can then test the significance of the
indirect effect.

A crucial part of this process is the idea that the significance of the
total effect is no longer present when the mediator is included in the
model. This is because the mediator is now explaining the relationship
between the IV and DV.

This highlights one of the issues with this approach: looking for a lack
of significance as indirect evidence of an effect. In addition, the
assumption that the total effect is no longer significant when the
mediator is included does not always hold true.

Another issue with this approach is that it does not test the
significance of the indirect effect. This is where the Sobel test comes
in. The Sobel test checks the significance of the indirect effect.
However, the Sobel test is less reliable with smaller sample sizes and
when the data is not normally distributed.

Nevertheless, it is still a useful approach to understand the logic of
mediation analysis and to be able to interpret the output of mediation
analysis in published papers.

For this example, we will use variables named X, M, and Y. This is just
for the purposes of this example. In your own research, you would
replace these with your own variables. Similarly, the names of the
models are named fit, fita, fitb, and fitc. These are just for the
purposes of this example. In your own research, you can name these
models however you like.

\subsection{Step 1: Total Effect}\label{step-1-total-effect}

The first model we run is the total effect model. This model tests the
relationship between the Precictor (X) and the Outcome (Y). This is
before we include the mediator in the model. We need this model to be
significant to move on to the next steps. If it is not, then we cannot
test for mediation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#1. Total Effect}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, }\AttributeTok{data=}\NormalTok{Meddata)}
\FunctionTok{summary}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Y ~ X, data = Meddata)

Residuals:
    Min      1Q  Median      3Q     Max 
-10.917  -3.738  -0.259   2.910  12.540 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept) 19.88368   14.26371   1.394   0.1665  
X            0.16899    0.08116   2.082   0.0399 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.16 on 98 degrees of freedom
Multiple R-squared:  0.04237,   Adjusted R-squared:  0.0326 
F-statistic: 4.336 on 1 and 98 DF,  p-value: 0.03993
\end{verbatim}

\subsection{Step 2: Path A (X on M)}\label{step-2-path-a-x-on-m}

The second model we run is the relationship between the Predictor (X)
and the Mediator (M). This is to test if the IV predicts the mediator.
This is important because if the IV does not predict the mediator, then
the M variable cannot be a mediator.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#2. Path A (X on M)}
\NormalTok{fita }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(M }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, }\AttributeTok{data=}\NormalTok{Meddata)}
\FunctionTok{summary}\NormalTok{(fita)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = M ~ X, data = Meddata)

Residuals:
    Min      1Q  Median      3Q     Max 
-9.5367 -3.4175 -0.4375  2.9032 16.4520 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  6.04494   13.41692   0.451    0.653    
X            0.66252    0.07634   8.678 8.87e-14 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.854 on 98 degrees of freedom
Multiple R-squared:  0.4346,    Adjusted R-squared:  0.4288 
F-statistic: 75.31 on 1 and 98 DF,  p-value: 8.872e-14
\end{verbatim}

\subsection{Step 3: Path B (M on Y, controlling for
X)}\label{step-3-path-b-m-on-y-controlling-for-x}

The third model we run is the relationship between the Mediator (M) and
the Outcome (Y), controlling for the Predictor (X). This is to test if
the M variable predicts the Outcome, controlling for the Predictor. This
is important because if the M variable does not predict the Outcome,
then it cannot be a mediator.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#3. Path B (M on Y, controlling for X)}
\NormalTok{fitb }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ M }\SpecialCharTok{+}\NormalTok{ X, }\AttributeTok{data=}\NormalTok{Meddata)}
\FunctionTok{summary}\NormalTok{(fitb)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Y ~ M + X, data = Meddata)

Residuals:
    Min      1Q  Median      3Q     Max 
-9.3651 -3.3037 -0.6222  3.1068 10.3991 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 17.32177   13.16216   1.316    0.191    
M            0.42381    0.09899   4.281 4.37e-05 ***
X           -0.11179    0.09949  -1.124    0.264    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.756 on 97 degrees of freedom
Multiple R-squared:  0.1946,    Adjusted R-squared:  0.1779 
F-statistic: 11.72 on 2 and 97 DF,  p-value: 2.771e-05
\end{verbatim}

\subsection{Step 4: Reversed Path C (Y on X, controlling for
M)}\label{step-4-reversed-path-c-y-on-x-controlling-for-m}

The fourth model we run is the relationship between the Outcome (Y) and
the Predictor (X), controlling for the Mediator (M). This is to test if
the relationship between the Predictor and the Outcome is no longer
significant when the Mediator is included in the model. This is
important because if the relationship between the Predictor and the
Outcome is still significant when the Mediator is included, then the
Mediator is not mediating the relationship.

At least, this is the logic of the Baron and Kenny approach. However, as
we mentioned earlier, this assumption does not always hold true. The
significance of the total effect is not always lost when the mediator is
included in the model - it could be that the mediator is not a full
mediator, but a partial mediator, for example.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#4. Reversed Path C (Y on X, controlling for M)}
\NormalTok{fitc }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(X }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Y }\SpecialCharTok{+}\NormalTok{ M, }\AttributeTok{data=}\NormalTok{Meddata)}
\FunctionTok{summary}\NormalTok{(fitc)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = X ~ Y + M, data = Meddata)

Residuals:
    Min      1Q  Median      3Q     Max 
-14.438  -2.573  -0.030   3.010  11.779 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 96.11234    9.27663  10.361  < 2e-16 ***
Y           -0.11493    0.10229  -1.124    0.264    
M            0.69619    0.08356   8.332 5.27e-13 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.823 on 97 degrees of freedom
Multiple R-squared:  0.4418,    Adjusted R-squared:  0.4303 
F-statistic: 38.39 on 2 and 97 DF,  p-value: 5.233e-13
\end{verbatim}

\subsection{Viewing output}\label{viewing-output}

We can use the stargazer package to view the output of the models in a
nice table. This is optional.

\begin{verbatim}
Summary Table
stargazer(fit, fita, fitb, fitc, type = "text", title = "Baron and Kenny Method")
\end{verbatim}

\includegraphics[width=\linewidth,height=2\textheight,keepaspectratio]{img/baronandkenny.png}

\subsection{Interpreting Baron and Kenny
approach}\label{interpreting-baron-and-kenny-approach}

A reminder of the logic of mediation:

\begin{itemize}
\tightlist
\item
  The direct relationship between X and Y should be significant
\item
  The relationship between X and M should be significant
\item
  The relationship between M and Y (controlling for X) should be
  significant
\item
  When controlling for M, the strength of the relationship between X and
  Y decreases and is \textbf{not} significant
\end{itemize}

From the output of the models, we can see that:

\begin{itemize}
\tightlist
\item
  The total effect is significant (model 1)
\item
  The relationship between X and M is significant (model 2)
\item
  The relationship between M and Y is significant (model 3)
\item
  The relationship between X and Y is no longer significant when M is
  included in the model (model 4)
\end{itemize}

This suggests that the relationship between X and Y is mediated by M.

\subsection{Running the Sobel test}\label{running-the-sobel-test}

However, the lack of significance of the total effect when the mediator
is included in the model is not enough to test for mediation. We need to
test the significance of the indirect (mediated) effect. This is where
the Sobel test comes in. The Sobel test checks the significance of the
mediated effect.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(bda)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: boot
\end{verbatim}

\begin{verbatim}
bda - 19.0.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mediation.test}\NormalTok{(Meddata}\SpecialCharTok{$}\NormalTok{M, Meddata}\SpecialCharTok{$}\NormalTok{X, Meddata}\SpecialCharTok{$}\NormalTok{Y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
               Sobel       Aroian      Goodman
z.value 3.8393902040 3.8190525305 3.8600562907
p.value 0.0001233403 0.0001339652 0.0001133609
\end{verbatim}

The output shows that the p value of the Sobel test is significant. This
suggests that the relationship between X and Y is mediated by M.

\section{Mediation analysis (the Mediation
package)}\label{mediation-analysis-the-mediation-package}

\subsection{Preacher \& Hayes (2004) mediation
approach}\label{preacher-hayes-2004-mediation-approach}

The Mediation package in R uses the Preacher \& Hayes (2004)
bootstrapping approach. They argue that in practice, when using the
Baron and Kenny approach, few people actually test the significance of
the indirect effect. Instead, they simply look for a lack of
significance of the total effect when the mediator is included in the
model.

\begin{quote}
\begin{quote}
``Baron and Kenny simply state that perfect mediation has occurred if c'
becomes nonsignificant after controlling for M, so researchers have
focused on that requirement.'' (Preacher \& Hayes, 2004, p.~719)
\end{quote}
\end{quote}

Moreover, they argue that the Sobel test is not reliable with smaller
sample sizes and when the data is not normally distributed. The
bootstrapping approach is more reliable in these cases.

In practice, the newer approach is also simpler. The mediation package
in R allows you to run the mediation analysis in fewer steps and gives
you more information.

\subsection{What is bootstrapping?}\label{what-is-bootstrapping}

Their approach uses bootstrapping to estimate the indirect effect.

\begin{quote}
\begin{quote}
``Bootstrapping is a nonparametric approach to effect-size estimation
and hypothesis testing that makes no assumptions about the shape of the
distributions of the variables or the sampling distribution of the
statistic'' (Preacer \& Hayes, 2004, p.~722)
\end{quote}
\end{quote}

Bootstrapping takes a large number of samples from our data and runs the
analysis on each of these samples. The sampling is done randomly with
replacement, and each sample in the bootstrap is the same size as our
dataset. Using this method, we can create estimates with that fall
within a narrower confidence interval (since we have now run the
analysis on 100's of samples). Bootstrapping overcomes concerns about
the distribution of our original dataset.

Bootstrapping is not confined to mediation analysis. It is a useful tool
in many analysis approaches.

\subsection{Mediation example}\label{mediation-example}

Is the relationship between Internet Use and Self-esteem mediated by
Social Media Usage?

\pandocbounded{\includegraphics[keepaspectratio]{images/mediation_model1.png}}

\subsection{Step 1: Run the models}\label{step-1-run-the-models}

Using this approach, we only need to run two models, rather than four.
They are as follows:

\phantomsection\label{annotated-cell-78}%
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Mediate package}
\FunctionTok{library}\NormalTok{(mediation)}

\NormalTok{fitM }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(M }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X,     }\AttributeTok{data=}\NormalTok{Meddata) }\hspace*{\fill}\NormalTok{\circled{1}}
\NormalTok{fitY }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X }\SpecialCharTok{+}\NormalTok{ M, }\AttributeTok{data=}\NormalTok{Meddata) }\hspace*{\fill}\NormalTok{\circled{2}}
\end{Highlighting}
\end{Shaded}

\begin{description}
\tightlist
\item[\circled{1}]
The first model is the relationship between the Predictor (X) and the
Mediator (M). This is to test if the IV predicts the mediator. This is
important because if the IV does not predict the mediator, then the M
variable cannot be a mediator.
\item[\circled{2}]
The second model is the relationship between the Predictor (X) and the
Outcome (Y), controlling for the Mediator (M). This is to test if the
relationship between the Predictor and the Outcome is no longer
significant when the Mediator is included in the model. This is
important because if the relationship between the Predictor and the
Outcome is still significant when the Mediator is included, then the
Mediator is not mediating the relationship.
\end{description}

You can see that these models are the same as the first and fourth
models in the Baron and Kenny approach.

\subsection{Step 2: Check assumptions}\label{step-2-check-assumptions-1}

Remember that bootstrapping is a non-parametric approach. This means
that it makes no assumptions about the shape of the distributions of the
variables or the sampling distribution of the statistic. However, it is
still important to check other assumptions of the model, such as
linearity. You can use the gvlma function or the plot function to check
these assumptions.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gvlma}\NormalTok{(fitM) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = M ~ X, data = Meddata)

Coefficients:
(Intercept)            X  
     6.0449       0.6625  


ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS
USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:
Level of Significance =  0.05 

Call:
 gvlma(x = fitM) 

                   Value p-value                   Decision
Global Stat        8.833 0.06542    Assumptions acceptable.
Skewness           6.314 0.01198 Assumptions NOT satisfied!
Kurtosis           1.219 0.26949    Assumptions acceptable.
Link Function      1.076 0.29959    Assumptions acceptable.
Heteroscedasticity 0.223 0.63674    Assumptions acceptable.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gvlma}\NormalTok{(fitY)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Y ~ X + M, data = Meddata)

Coefficients:
(Intercept)            X            M  
    17.3218      -0.1118       0.4238  


ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS
USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:
Level of Significance =  0.05 

Call:
 gvlma(x = fitY) 

                     Value p-value                Decision
Global Stat        3.41844  0.4904 Assumptions acceptable.
Skewness           1.85648  0.1730 Assumptions acceptable.
Kurtosis           0.77788  0.3778 Assumptions acceptable.
Link Function      0.71512  0.3977 Assumptions acceptable.
Heteroscedasticity 0.06896  0.7929 Assumptions acceptable.
\end{verbatim}

We can see that there are issues with normality of data, but because we
are bootstrapping, this is not a problem.

\subsection{Step 3.1: Run the mediation analysis on the
models}\label{step-3.1-run-the-mediation-analysis-on-the-models}

The mediate function will run the mediation analysis on the models we
have created. It will give us the following estimates:

\begin{itemize}
\tightlist
\item
  Average Causal Mediation Effects (ACME)
\item
  Average Direct Effects (ADE)
\item
  combined indirect and direct effects (Total Effect)
\item
  the ratio of these estimates (Prop. Mediated).
\end{itemize}

Firstly, we need the Total Effect to be significant. This is the
relationship between X and Y (direct and indirect).

Secondly, we need the ACME to be significant. This is the indirect
effect of M (total effect - direct effect) and thus this value tells us
if our mediation effect is significant.

We can look at the ADE to see if the relationship between X and Y is
direct. If this is not significant, then the relationship between X and
Y is mediated by M. If it is significant (and the ACME is also
significant), then the relationship between X and Y is both direct and
indirect (partial mediation).

The Prop Mediated value tells us the proportion of the total effect that
is mediated by M. It is calculated by dividing the ACME by the Total
Effect. The closer this value is to 1, the more of the total effect is
mediated by M. We can usually read it like a percentage (e.g., 0.87
means 87\% of the total effect is mediated). If your ACME is greater
than 1, then it could be because the ACME and ADE are in opposite
directions (i.e., one is positive and the other is negative). This would
lead to a value more than 1 when calculating the proportion, but for
practical purposes, we could interpret the ACME as if it were 1 (i.e.,
all of the effect is mediated by M).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitMed }\OtherTok{\textless{}{-}} \FunctionTok{mediate}\NormalTok{(fitM, fitY, }\AttributeTok{treat=}\StringTok{"X"}\NormalTok{, }\AttributeTok{mediator=}\StringTok{"M"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(fitMed)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Causal Mediation Analysis 

Quasi-Bayesian Confidence Intervals

                Estimate 95% CI Lower 95% CI Upper p-value    
ACME            0.280779     0.143728     0.424325  <2e-16 ***
ADE            -0.113330    -0.311648     0.086472   0.258    
Total Effect    0.167450     0.020768     0.335915   0.028 *  
Prop. Mediated  1.642775     0.563148     8.444815   0.028 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Sample Size Used: 100 


Simulations: 1000 
\end{verbatim}

\subsection{Step 3.2: Plot the mediation analysis of the
models}\label{step-3.2-plot-the-mediation-analysis-of-the-models}

We can plot the mediation analysis to visualise the results. The plot
below reiterates what was on the output of the summary. We can see that
the confidence intervals of Total Effect and ACME are significant, but
the confidence interval of ADE is not significant.

\textbf{Translation:}

\begin{itemize}
\tightlist
\item
  Total effect is signficant: there is a relationship between X and Y
  (direct and indirect)
\item
  ADE is not significant: the relationship between X and Y is not direct
\item
  ACME is significant: the relationship between X and Y is mediated by M
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(fitMed)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{mediation_files/figure-pdf/unnamed-chunk-13-1.pdf}}

\section{Reporting mediation
analysis}\label{reporting-mediation-analysis}

When reporting mediation analysis, the indirect (ACME) result is most
important. However you should make it clear that the total effect is
significant. You could also report the Prop Mediated values.

For example (us:

\begin{quote}
The mediating effect was using a bootstrapping approach (1000 samples)
with the mediation package (REF) in R. The relationship between Internet
Use and Self-esteem was mediated by Social Media Usage (ACME = 0.28,
95\% CI {[}0.14, 0.42{]}). The total effect was significant (0.167, 95\%
CI {[}0.02, 0.34{]}). The proportion mediated was 1, suggesting that all
of the total effect of relationship between Internet Usage and
Self-esteem was mediated by Social Media Usage.
\end{quote}

\section{References}\label{references}

Demos \& Salas (2019). \emph{A Language, not a Letter: Learning
Statistics in R} (Chapter 14). https://ademos.people.uic.edu/ Accessed
Jan 2020.

Preacher, K. J., \& Hayes, A. F. (2004). SPSS and SAS procedures for
estimating indirect effects in simple mediation models. Behavior
research methods, instruments, \& computers, 36(4), 717-731.

\bookmarksetup{startatroot}

\chapter{Creating plots with ggplot2 in
R}\label{creating-plots-with-ggplot2-in-r}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{At the end of this chapter, you will be able to:}, rightrule=.15mm, left=2mm]

\begin{itemize}
\tightlist
\item
  Describe the ggplot ``grammar of visualisation'': coordinates and
  geoms
\item
  Write a graph function to display multiple variables on a plot
\item
  Amend the titles and legends of a plot
\item
  Save plots in PDF or image formats
\end{itemize}

\end{tcolorbox}

\section{The ``grammar of visualisation'' with
ggplot}\label{the-grammar-of-visualisation-with-ggplot}

With ggplot, graphs are made up of 3 components: - A dataset - A
coordinate system (e.g., the X and Y axes) - Visual marks to represent
data \textbf{(geoms)}

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{img/ggplot1.png}

In the above example, the dataset is the \emph{studentData} that we used
previously. The \emph{grades} variable is mapped to the X axis. The
\emph{hoursOfStudy} variable is mapped to the Y axis. The graph is
created using the following code:

\phantomsection\label{annotated-cell-83}%
\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{studentData, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{grades,}\AttributeTok{y=}\NormalTok{hoursOfStudy)) }\SpecialCharTok{+} \hspace*{\fill}\NormalTok{\circled{1}}
  \FunctionTok{geom\_point}\NormalTok{() }\hspace*{\fill}\NormalTok{\circled{2}}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_files/figure-pdf/unnamed-chunk-2-1.pdf}}

In this code:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We specify the dataset and the variables for the X and Y axes.
\item
  We specify the \textbf{geom} that will represent the data points
  visually (in this case, each datum is a point).
\end{enumerate}

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{img/ggplot2.png}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-important-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-important-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Remember the \texttt{+} operator}, rightrule=.15mm, left=2mm]

\begin{itemize}
\tightlist
\item
  The layers of a ggplot are separated using the \texttt{+} operator.
  That is, we first specify the dataset and the variables for the X and
  Y axes, and then we add a \texttt{+}, then the \texttt{geom} that will
  represent the data points visually.
\end{itemize}

\end{tcolorbox}

\section{Adding more variables to a
plot}\label{adding-more-variables-to-a-plot}

One of the things that makes ggplot so powerful is that you can add more
variables to a plot. You can map these variables to different aesthetic
attributes of the plot, such as colour, shape, or size.

This can be done either in the \texttt{ggplot()} function or in the
\texttt{geom\_point()} function.

\subsection{Example: assigning a variable to colour (2 different
approaches)}\label{example-assigning-a-variable-to-colour-2-different-approaches}

\phantomsection\label{annotated-cell-84}%
\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{studentData, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{grades,}\AttributeTok{y=}\NormalTok{hoursOfStudy, }\AttributeTok{color =}\NormalTok{ route)) }\SpecialCharTok{+} \hspace*{\fill}\NormalTok{\circled{1}}
  \FunctionTok{geom\_point}\NormalTok{() }\hspace*{\fill}\NormalTok{\circled{2}}
\end{Highlighting}
\end{Shaded}

\begin{description}
\tightlist
\item[\circled{1}]
The \texttt{color} aesthetic is mapped to the \texttt{route} variable in
the main \texttt{ggplot()} function.
\item[\circled{2}]
The \texttt{geom\_point()} function is used to represent the data points
visually and it takes its colour from the main \texttt{ggplot()}
function.
\end{description}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_files/figure-pdf/unnamed-chunk-4-1.pdf}}

\phantomsection\label{annotated-cell-85}%
\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{studentData, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{grades,}\AttributeTok{y=}\NormalTok{hoursOfStudy)) }\SpecialCharTok{+} \hspace*{\fill}\NormalTok{\circled{1}}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ route)) }\hspace*{\fill}\NormalTok{\circled{2}}
\end{Highlighting}
\end{Shaded}

\begin{description}
\tightlist
\item[\circled{1}]
The \texttt{color} aesthetic is not mentioned in the main
\texttt{ggplot()} function.
\item[\circled{2}]
The \texttt{color} aesthetic is mapped to the \texttt{route} variable in
the \texttt{geom\_point()} function.
\end{description}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_files/figure-pdf/unnamed-chunk-5-1.pdf}}

\subsection{Example: assigning a variable to
size}\label{example-assigning-a-variable-to-size}

\phantomsection\label{annotated-cell-86}%
\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{studentData, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{grades,}\AttributeTok{y=}\NormalTok{hoursOfStudy, }\AttributeTok{size =}\NormalTok{ satisfactionLevel, }\AttributeTok{colour =}\NormalTok{ route)) }\SpecialCharTok{+} \hspace*{\fill}\NormalTok{\circled{1}}
  \FunctionTok{geom\_point}\NormalTok{() }\hspace*{\fill}\NormalTok{\circled{2}}
\end{Highlighting}
\end{Shaded}

\begin{description}
\tightlist
\item[\circled{1}]
The \texttt{size} aesthetic is mapped to the \texttt{satisfactionLevel}
variable in the main \texttt{ggplot()} function.
\item[\circled{2}]
The \texttt{geom\_point()} function is used to represent the data points
visually and it takes its size from the main \texttt{ggplot()} function.
\end{description}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_files/figure-pdf/unnamed-chunk-6-1.pdf}}

\subsection{Example: assigning a variable to
shape}\label{example-assigning-a-variable-to-shape}

\phantomsection\label{annotated-cell-87}%
\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{studentData, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{grades,}\AttributeTok{y=}\NormalTok{hoursOfStudy, }\AttributeTok{size =}\NormalTok{ satisfactionLevel, }\AttributeTok{colour =}\NormalTok{ route, }\AttributeTok{shape =}\NormalTok{ hasDepdendants)) }\SpecialCharTok{+} \hspace*{\fill}\NormalTok{\circled{1}}
  \FunctionTok{geom\_point}\NormalTok{() }\hspace*{\fill}\NormalTok{\circled{2}}
\end{Highlighting}
\end{Shaded}

\begin{description}
\tightlist
\item[\circled{1}]
The \texttt{shape} aesthetic is mapped to the \texttt{hasDepdendants}
variable in the main \texttt{ggplot()} function.
\item[\circled{2}]
The \texttt{geom\_point()} function is used to represent the data points
visually and it takes its shape from the main \texttt{ggplot()}
function.
\end{description}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_files/figure-pdf/unnamed-chunk-7-1.pdf}}

\section{Different types of geoms: bar
charts}\label{different-types-of-geoms-bar-charts}

The \texttt{geom\_point()} function is just one of many geoms that you
can use in ggplot. Another common geom is \texttt{geom\_col()}, which is
used to create bar charts.

When creating a bar chart there are 2 approaches:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  summarise the data before plotting
\item
  let ggplot do the summarising for you
\end{enumerate}

\subsection{Example: summarising the data before
plotting}\label{example-summarising-the-data-before-plotting}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}

\NormalTok{studentData }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(route) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{meanHours =} \FunctionTok{mean}\NormalTok{(hoursOfStudy)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ route, }\AttributeTok{y =}\NormalTok{ meanHours)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_files/figure-pdf/unnamed-chunk-8-1.pdf}}

In the above code, we first use the \texttt{group\_by()} and
\texttt{summarise()} functions from the \texttt{dplyr} package to
calculate the mean number of hours of study for each route (see previous
lessons for this). We then use the \texttt{ggplot()} function to create
a plot with the \texttt{route} variable on the X axis and the
\texttt{meanHours} variable on the Y axis. Finally, we use the
\texttt{geom\_col()} function to create the bar chart.

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-important-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-important-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Remember to refer to the summarised variable in the \texttt{ggplot()}
function}, rightrule=.15mm, left=2mm]

When you summarise the data, remember to refer to the summarised
variable in the \texttt{ggplot()} function, not the original variable.
In the above example, we use \texttt{meanHours} in the \texttt{ggplot()}
function, not \texttt{hoursOfStudy}.

If you find it easier, you can save the summarised data as a new object
and then use this in the \texttt{ggplot()} function.

\end{tcolorbox}

\subsection{Example: letting ggplot do the summarising for
you}\label{example-letting-ggplot-do-the-summarising-for-you}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{studentData, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ route, }\AttributeTok{y =}\NormalTok{ hoursOfStudy)) }\SpecialCharTok{+} 
  \FunctionTok{stat\_summary}\NormalTok{(}\AttributeTok{fun =}\NormalTok{ mean, }\AttributeTok{geom =} \StringTok{"col"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_files/figure-pdf/unnamed-chunk-9-1.pdf}}

In the above code, we use the \texttt{stat\_summary()} function to
calculate the mean number of hours of study for each route. We specify
that we want to calculate the mean using the \texttt{fun\ =\ mean}
argument, and we specify that we want to create a bar chart using the
\texttt{geom\ =\ "col"} argument.

\section{Changing the titles and legends of a
plot}\label{changing-the-titles-and-legends-of-a-plot}

You can change the titles and legends of a plot using the
\texttt{labs()} function. This function allows you to change the title
of the plot, the titles of the X and Y axes, and the legend title.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{studentData }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(route) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{meanHours =} \FunctionTok{mean}\NormalTok{(hoursOfStudy)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ route, }\AttributeTok{y =}\NormalTok{ meanHours)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Mean hours of study by route"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Route"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Mean hours of study"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Source: Student data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_files/figure-pdf/unnamed-chunk-10-1.pdf}}

In the above code, we use the \texttt{labs()} function to change the
title of the plot to ``Mean hours of study by route'', the title of the
X axis to ``Route'', the title of the Y axis to ``Mean hours of study'',
and the caption to ``Source: Student data''.

\section{Themes in ggplot}\label{themes-in-ggplot}

You can change all of the colours and other appearance elements in
ggplot. However, it can be easier to change the appearance of a plot
using themes. Themes allow you to change the background colour, the
colour of the text, the size of the text, and other aspects of the
appearance of the plot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{studentData }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(route) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{meanHours =} \FunctionTok{mean}\NormalTok{(hoursOfStudy)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ route, }\AttributeTok{y =}\NormalTok{ meanHours)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Mean hours of study by route"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Route"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Mean hours of study"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Source: Student data"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_files/figure-pdf/unnamed-chunk-11-1.pdf}}

In the above code, we use the \texttt{theme\_minimal()} function to
change the appearance of the plot to a minimal theme. There are many
other themes that you can use, such as \texttt{theme\_light()},
\texttt{theme\_dark()}, and \texttt{theme\_bw()}.

\section{Saving plots in ggplot}\label{saving-plots-in-ggplot}

The best way to save plots (and get reliable results) is to save them
using the \texttt{ggsave()} function. This function allows you to save
plots in a variety of formats, such as PDF, PNG, and JPEG. For
publication-quality plots, it is best to save them in PDF format,
because this format is scalable and resolution independent.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{studentData }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(route) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{meanHours =} \FunctionTok{mean}\NormalTok{(hoursOfStudy)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ route, }\AttributeTok{y =}\NormalTok{ meanHours)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Mean hours of study by route"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Route"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Mean hours of study"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Source: Student data"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}

\FunctionTok{ggsave}\NormalTok{(}\StringTok{"studentData\_plot.pdf"}\NormalTok{, }\AttributeTok{width =} \DecValTok{6}\NormalTok{, }\AttributeTok{height =} \DecValTok{4}\NormalTok{, }\AttributeTok{dpi =} \DecValTok{300}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In the above code, we use the \texttt{ggsave()} function to save the
plot as a PDF file called ``studentData\_plot.pdf''. We specify the
width of the plot as 6 inches, the height of the plot as 4 inches, and
the resolution as 300 dots per inch (dpi). dpi is not actually used for
PDF files, but it is useful for other file formats, such as PNG and eps.

In terms of specifying the width and height of the plot, it can take
some fiddling around to get the right dimensions. You can specify the
width and height in pixels, but it is generally better to specify them
in inches, because this is a more standard unit of measurement for
plots. I recommend thinking in terms of the aspect ratio of the plot,
rather than the exact dimensions. For example, if you want a wide plot,
you might specify the width as 6 inches and the height as 4 inches. If
you want a square plot, you might specify the width and height as 4
inches each. If you want a tall plot, you might specify the width as 4
inches and the height as 6 inches.

Once the aspect ratio feels right, you can adjust the width and height
to get the exact right dimensions for your plot.

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, colbacktitle=quarto-callout-important-color!10!white, toptitle=1mm, titlerule=0mm, colback=white, colframe=quarto-callout-important-color-frame, bottomrule=.15mm, opacityback=0, arc=.35mm, toprule=.15mm, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, breakable, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Plots will save into the working directory}, rightrule=.15mm, left=2mm]

When you save a plot using the \texttt{ggsave()} function, the plot will
be saved into the working directory. You can specify a different
directory by specifying the full path to the file, such as
``C:/Users/username/Documents/studentData\_plot.pdf''.

It's easier just to set the working directory to the folder where you
want to save the plots!

\end{tcolorbox}

\section{Summary}\label{summary}

In this chapter, you have learned how to create plots with ggplot in R.
You have learned about the ``grammar of visualisation'' with ggplot,
including the dataset, the coordinate system, and the visual marks that
represent the data. You have learned how to write a graph function to
display multiple variables on a plot, how to amend the titles and
legends of a plot, and how to save plots in PDF or image formats.




\end{document}

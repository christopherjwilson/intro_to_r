[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to R for Clinical Psychology",
    "section": "",
    "text": "Welcome\nThe purpose of this site/book is to introduce you to R and R Studio for use in Clinical Psychology Research. Before you begin, there are some important things to know:",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#tidyverse",
    "href": "index.html#tidyverse",
    "title": "Introduction to R for Clinical Psychology",
    "section": "The tidyverse",
    "text": "The tidyverse\n\n\n\n\n\n\nThis site/book uses the tidyverse set of packages\n\n\n\nThe tidyverse is a collection of R packages designed for to make data manipulation and visualization. easier in R. The tidyverse is a powerful set of tools for data analysis, and it is widely used in the R community. It is assumed that you will have the tidyverse installed and loaded for the examples in this site/book. If you do not have the tidyverse installed, you can install it by running the following code in the R console:\n\ninstall.packages(\"tidyverse\")\nYou only need to install the package on to your machine once. Once you have installed the tidyverse, you can load it by running the following code in the R console:\n\nlibrary(tidyverse)\nThe library() function is used to load packages in R. You will need to load the tidyverse package at the beginning of each R session in which you want to use it.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#textbooks-that-can-be-accessed-online",
    "href": "index.html#textbooks-that-can-be-accessed-online",
    "title": "Introduction to R for Clinical Psychology",
    "section": "Textbooks that can be accessed online",
    "text": "Textbooks that can be accessed online\ne-books can be accessed from the library website: https://www.tees.ac.uk/depts/lis/\n\nResearch Methods and Statistics\nCoolican, 2019. Research Methods and Statistics in Psychology. Taylor & Francis Group\nBarker, C., Pistrang, N., & Elliott, R. (2015). Research methods in clinical psychology: An introduction for students and practitioners (3rd ed.). Chichester, West Sussex: Wiley Blackwell.\nWeiner, I. B., Schinka, J. A., & Velicer, W. F. (2012). Handbook of psychology, research methods in psychology (2. Aufl. ed.). Somerset: Wiley.\n\n\nWorking with R and RStudio to do analysis\nNavarro, D. (2017) Learning statistics with R.\nPhillips N. D. (2018) YaRrr! The Pirate’s Guide to R\nHorton, Pruim and Kaplan (2015) A Student’s Guide to R\nMather, M. (2019) R for Academics\nWickham and Grolemund (2019). R For Data Science\nAllaire and Grolemund (2019). R Markdown: The Definitive Guide\nBasics of RStudio\nData Import\nData Transformation\nData Visualisation with GGPlot",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction to R and R Studio",
    "section": "",
    "text": "1.1 What are R and R Studio?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R and R Studio</span>"
    ]
  },
  {
    "objectID": "intro.html#what-are-r-and-r-studio",
    "href": "intro.html#what-are-r-and-r-studio",
    "title": "1  Introduction to R and R Studio",
    "section": "",
    "text": "At the end of this section, you will be able to:\n\n\n\n\nDownload and install R and R Studio\nUnderstand the basic layout of R Studio\nDescribe some of the differences between SPSS and R\n\n\n\n\n1.1.1 Downloading and installing R and R Studio\nTo get started with R Studio, you need to download and install two pieces of software:\n\nR: The base software that you will use to write and run code.\nR Studio: An integrated development environment (IDE) that makes it easier to write and run code in R.\n\n\n\nClick on these links to download:\n\nR project\nRStudio\n\n\n\n1.1.2 The R Studio layout\nWhen you open R Studio, you will see a screen that looks like this:\n\n\n\nR Studio IDE\n\n\nBriefly, the different panes in R Studio are:\n\nConsole: You can write and run code in this pane. However, it is best practice to write code in a script. You will see output from your code in the console.\nEnvironment/History: This pane shows you the objects that you have created in R, and the history of the commands that you have run.\nFiles/Plots/Packages/Help: These panes allow you to navigate your files, view plots, manage packages, and access help documentation.\n\nYou will learn more about these panes as you work through the course.\n\n\n\nR Studio IDE\n\n\n\n\n1.1.3 Differences between SPSS and R\nR is a statistical programming language, while SPSS is a point-and-click software package. This means that in R, you write code to perform tasks, while in SPSS, you click buttons and select options from menus.\nThis can take some getting used to, but there are many advantages to using R:\n\nReproducibility: You can save your code and rerun it at any time, ensuring that your analysis is reproducible.\nFlexibility: You can write code to perform any task you like, rather than being limited to the options available in a menu.\nCommunity: R has a large and active community of users who share code and help each other to solve problems.\n\nWith R, you won’t manipulate your source data files. Instead, you load the data into R and manipulate it in R. This means that you can always go back to your original data and start again if you need to.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R and R Studio</span>"
    ]
  },
  {
    "objectID": "intro.html#no-more-point-and-click---the-r-workflow.",
    "href": "intro.html#no-more-point-and-click---the-r-workflow.",
    "title": "1  Introduction to R and R Studio",
    "section": "1.2 No more “point and click”! - the R workflow.",
    "text": "1.2 No more “point and click”! - the R workflow.\n\n\n\n\n\n\nAt the end of this section, you will be able to:\n\n\n\n\nOpen a new script in R Studio\nWrite and run code in a script\nSave a script for later use\n\n\n\n\n1.2.1 Using scripts in R Studio\nWhen you work in R, you will write code in a script. This is a text file that contains the code that you want to run. You can write and run code in the console, but it is best practice to write code in a script. This allows you to save your code and run it again later IT also makes it easier to see what you have done.\nTo open a new script in R Studio, click on File &gt; New File &gt; R Script. This will open a new script in the top-left pane of R Studio.\nYou can write code in the script, and then run it by selecting the code that you want to run and clicking the Run button at the top of the script pane. You can also run code by pressing Ctrl + Enter on your keyboard.\nTo save your script, click on File &gt; Save As... and save the file with a .R extension.\n\n\n\n\n\n\n\n\nOrganising your work\n\n\n\nIt is good practice to keep your work organised by putting your scripts, data, and other files in a folder on your computer, for each project that you work on.\nRStudio also allows the creation of projects. You can create a new project in R Studio by clicking on File &gt; New Project.... This will create a new folder on your computer where you can save your scripts, data, and other files. If you save your script in the project folder, you can easily access it by opening the project in R Studio. If you use projects, be aware that R Studio will load the last project you worked on when you open the software.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R and R Studio</span>"
    ]
  },
  {
    "objectID": "intro.html#objects-functions-and-packages-in-r",
    "href": "intro.html#objects-functions-and-packages-in-r",
    "title": "1  Introduction to R and R Studio",
    "section": "1.3 Objects, functions and packages in R",
    "text": "1.3 Objects, functions and packages in R\n\n\n\n\n\n\nAt the end of this section, you will be able to:\n\n\n\n\nCreate objects in R\nUse functions in R to perform tasks\nInstall and load packages in R\n\n\n\n\n1.3.1 What are objects?\nIn R, you can create objects to store data. For example, you can create an object called numbers that contains a set of numbers like this:\nnumbers &lt;- c(1, 2, 3, 4, 5)\nBreaking this code down:\n\nnumbers is the name of the object that you are creating.\n&lt;- is the assignment operator. It assigns the value on the right-hand side of the operator to the object on the left-hand side.\nc(1, 2, 3, 4, 5) is the data that you are assigning to the object. In this case, it is a set of numbers.\n\nWhen you run this code, R will create an object called numbers that contains the numbers 1, 2, 3, 4, and 5. You will be ab` le to see the object in the Environment pane in R Studio.\nYou can then use the object in your code, instead of typing out the data each time (see Section 1.3.2 for example).\n\n\n\n\n1.3.2 What are functions?\nFunctions are code that have been written to perform a specific task. You can use functions in R to perform tasks like reading data into R, summarising data, and creating plots.\nFor example, the mean() function calculates the mean of a set of numbers. You can use the mean() function like this:\nnumbers &lt;- c(1, 2, 3, 4, 5)\n\nmean(numbers)\nFunctions in R have a name, followed by parentheses. You can pass arguments to the function inside the parentheses. In this case, the mean() function takes a set of numbers as an argument, and returns the mean of those numbers.\nTo learn more about a function, you can use the help() function. For example, to learn more about the mean() function, you can run the following code:\nhelp(mean)\nYou can also use the ? operator to get help on a function. For example, to get help on the mean() function, you can run the following code:\n\n?mean\n\n\n\n\n1.3.3 What are packages?\nR has many built-in functions that you can use to perform tasks. However, there are also many packages available that contain additional functions. You can install these packages onto your conputer and then load them into your R session whenever you want to use them.\nTo install a package, you can use the install.packages() function. For example, to install the tidyverse package, you would run the following code:\ninstall.packages(\"tidyverse\")\nTo load a package into your R session, you can use the library() function. For example, to load the tidyverse package, you would run the following code:\nlibrary(tidyverse)\nOnce you have loaded a package, you can use the functions in that package in your code. For example, the tidyverse package contains functions for data manipulation and visualisation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R and R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html",
    "href": "working_with_data.html",
    "title": "2  Working with data in R Studio",
    "section": "",
    "text": "2.1 Importing data into R\nThere are a few different ways to load data into R. You can load data from a file on your computer, from a URL, or from a package. You can load data in different file types, such as CSV, Excel, and SPSS files.\nUsing RStudio, you can load data by clicking on File &gt; Import Dataset. This will open a window where you can select the file that you want to load.\nHowever, you can also load data using code. For example, you can use the read_csv() function from the readr package to load a CSV file into R. You can use the readxl package to load an Excel file, and the haven package to load an SPSS file.\nLet’s break this code down:\nWhen you load data into R, it will be stored as a data frame. A data frame is a type of object in R that is used to store tabular data. It is similar to a spreadsheet in Excel, with rows and columns.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#importing-data-into-r",
    "href": "working_with_data.html#importing-data-into-r",
    "title": "2  Working with data in R Studio",
    "section": "",
    "text": "At the end of this section, you will be able to:\n\n\n\n\nLoad data into R from different file types\nUnderstand the structure of data in R\n\n\n\n\n\n\n\n# Load the readr package\n\nlibrary(readr)\n\n# Load a CSV file into R\n\ndata &lt;- read_csv(\"data.csv\")\n\n\nlibrary(readr) loads the readr package into your R session. This package contains the read_csv() function, which you can use to load a CSV file into R.\nread_csv(\"data.csv\") reads the CSV file called data.csv into R. The data will be stored in an object called data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#how-are-data-stored-in-r",
    "href": "working_with_data.html#how-are-data-stored-in-r",
    "title": "2  Working with data in R Studio",
    "section": "2.2 How are data stored in R?",
    "text": "2.2 How are data stored in R?\nIf you worked through the previous section, you should already have some idea how to load data into R. But how are data stored in R? In R, data are stored in objects. An object is a container that holds data. There are several types of objects in R, but the most common ones are:\n\nVectors (e.g., a sequence of numbers)\nMatrices (e.g., a table of rows and colummns, all of the same data type)\nData frames (e.g., a table of data where each column represents a variable and each row represents an observation)\nLists (e.g., a collection of objects)\n\nIn this section, we will focus on data frames, which are the most common way to store data in R. A data frame is a table of data where each column represents a variable and each row represents an observation. You can think of a data frame as having a structure similar to a spreadsheet.\n\n\n\nData frame with 3 variables/columns",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#how-do-we-use-data-frames-in-r",
    "href": "working_with_data.html#how-do-we-use-data-frames-in-r",
    "title": "2  Working with data in R Studio",
    "section": "2.3 How do we use data frames in R?",
    "text": "2.3 How do we use data frames in R?\nTo view the data in a data frame, you can simply type the name of the data frame in the console and press Enter. For example, if you have a data frame called my_data, you can view the data in the data frame by typing my_data in the console and pressing Enter.\n\n## load the tidyverse package\n\nlibrary(tidyverse)\n\n# Create a data frame\nmy_data &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\"),\n  age = c(25, 30, 35, 40, 45, 50),\n  height = c(160, 175, 180, 165, 170, 190),\n  car = c(\"Electric\", \"Petrol\", \"Electric\", \"Petrol\", \"Petrol\", \"Electric\")\n)\n\n# View or refer to the data in the data frame\n\nmy_data\n\n     name age height      car\n1   Alice  25    160 Electric\n2     Bob  30    175   Petrol\n3 Charlie  35    180 Electric\n4   David  40    165   Petrol\n5     Eve  45    170   Petrol\n6   Frank  50    190 Electric\n\n\nIn the code above, we created a data frame called my_data with four variables: name, age, height, and car. We then used the my_data object to view the data in the data frame.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#view-or-refer-to-a-specific-variable-in-a-data-frame",
    "href": "working_with_data.html#view-or-refer-to-a-specific-variable-in-a-data-frame",
    "title": "2  Working with data in R Studio",
    "section": "2.4 View or refer to a specific variable in a data frame",
    "text": "2.4 View or refer to a specific variable in a data frame\nTo view or refer to a specific variable in a data frame, you can use the $ operator. For example, if you want to view the age variable in the my_data data frame, you can type my_data$age in the console and press Enter.\n\n# View or refer to a specific variable in a data frame\n\nmy_data$age\n\n[1] 25 30 35 40 45 50",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#data-types-in-r",
    "href": "working_with_data.html#data-types-in-r",
    "title": "2  Working with data in R Studio",
    "section": "2.5 Data types in R",
    "text": "2.5 Data types in R\nIn R, each variable in a data frame has a data type. The most common data types in R are:\n\nNumeric: for continuous variables (e.g., age, height)\nFactor: for categorical variables\nLogical: for binary variables (TRUE or FALSE)\n\nYou can use the str() function to view the structure of a data frame, including the data types of each variable.\n\n# View the structure of a data frame\n\nstr(my_data)\n\n'data.frame':   6 obs. of  4 variables:\n $ name  : chr  \"Alice\" \"Bob\" \"Charlie\" \"David\" ...\n $ age   : num  25 30 35 40 45 50\n $ height: num  160 175 180 165 170 190\n $ car   : chr  \"Electric\" \"Petrol\" \"Electric\" \"Petrol\" ...\n\n\nIn the code above, we used the str() function to view the structure of the my_data data frame. The output shows the data types of each variable in the data frame.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#convert-data-types-in-r",
    "href": "working_with_data.html#convert-data-types-in-r",
    "title": "2  Working with data in R Studio",
    "section": "2.6 Convert data types in R",
    "text": "2.6 Convert data types in R\nYou can convert the data type of a variable in R using the as. functions. For example, you can convert a character variable to a factor variable using the as.factor() function.\n\n# Convert a character variable to a factor variable\n\nmy_data$name &lt;- as.factor(my_data$name)\n\nmy_data$car &lt;- as.factor(my_data$car)\n\nIn the code above, we converted the name variable in the my_data data frame from a character variable to a factor variable using the as.factor() function.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#subsetting-data-in-r",
    "href": "working_with_data.html#subsetting-data-in-r",
    "title": "2  Working with data in R Studio",
    "section": "2.7 Subsetting data in R",
    "text": "2.7 Subsetting data in R\n\n\n\n\n\n\nAt the end of this section, you will be able to:\n\n\n\n\nFilter data in R\nCreate subsets of data in R\n\n\n\nSubsetting data in R means selecting a subset of the data based on certain criteria. For example, you might want to select only the rows where a certain variable is greater than a certain value, or only the columns that contain certain variables.\nIf we use the my_data data frame from the previous section, we can subset the data to select only the rows where the age variable is greater than 30.\n\n# Filter the data frame to select only the rows where the age variable is greater than 30\n\n# this method uses the dplyr package, which is a part of the tidyverse. Be sure to load the tidyverse package if you haven't already.\n\nmy_data %&gt;% filter(age &gt; 30)\n\n     name age height      car\n1 Charlie  35    180 Electric\n2   David  40    165   Petrol\n3     Eve  45    170   Petrol\n4   Frank  50    190 Electric\n\n\nLet’s break this code down:\n\nmy_data is the data frame that we want to subset.\n%&gt;% is the pipe operator, which is used to pass the data frame to the next function. This allows us to link multiple steps together in a single line of code.\nfilter(age &gt; 30) is the function that filters the data frame to select only the rows where the age variable is greater than 30.\n\nThe output of this code will be a new data frame that contains only the rows where the age variable is greater than 30. However, this new data frame will not be saved anywhere, so if you want to save it, you need to assign it to a new object. Tp do this, you can use the assignment operator &lt;-.\n\n# Filter the data frame to select only the rows where the age variable is greater than 30 and save the result to a new data frame called new_data\n\nnew_data &lt;- my_data %&gt;% filter(age &gt; 30)\n\nIn this code, on the left side of the assignment operator &lt;-, we have new_data, which is the name of the new data frame that will contain only the filtered subset of the data (i.e., the values where the age variable is greater than 30). The difference between this code and the previous code is that we are now saving the result to a new data frame called new_data, instead of just printing it to the console.\nWe can also combine multiple conditions when subsetting data. For example, we can select only the rows where the age variable is greater than 25 and the height variable is greater than 175.\n\n# Filter the data frame to select only the rows where the age variable is greater than 25 and the height variable is greater than 175\n\nmy_data %&gt;% filter(age &gt; 25 & height &gt; 175)\n\n     name age height      car\n1 Charlie  35    180 Electric\n2   Frank  50    190 Electric\n\n\nThere are many other ways to subset data in R, depending on the criteria you want to use. For example, you can use the select() function to select specific columns, the arrange() function to sort the data, and the mutate() function to create new variables. We will cover some of these functions in later sections.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#grouping-and-summarising-data-in-r",
    "href": "working_with_data.html#grouping-and-summarising-data-in-r",
    "title": "2  Working with data in R Studio",
    "section": "2.8 Grouping and summarising data in R",
    "text": "2.8 Grouping and summarising data in R\n\n\n\n\n\n\nAt the end of this section, you will be able to:\n\n\n\n\nGroup data in R\nSummarise data in R\n\n\n\nGrouping and summarising data in R means grouping the data by one or more variables and then calculating summary statistics for each group. For example, you might want to calculate the mean age for each group of people based on their height.\nIf we use the my_data data frame from the previous section, we can group the data by the car variable and then calculate the mean age for each group.\n\n# Group the data frame by the car variable and calculate the mean age for each group\n\nmy_data %&gt;% group_by(car) %&gt;% \n  summarise(mean_age = mean(age)) %&gt;%\n  ungroup()\n\n# A tibble: 2 × 2\n  car      mean_age\n  &lt;fct&gt;       &lt;dbl&gt;\n1 Electric     36.7\n2 Petrol       38.3\n\n\nLet’s break this code down:\n\nmy_data is the data frame that we want to group and summarise.\n%&gt;% is the pipe operator, which is used to pass the data frame to the next function. This allows us to link multiple steps together in a single line of code.\ngroup_by(car) is the function that groups the data frame by the car variable.\nsummarise(mean_age = mean(age)) is the function that calculates the mean age for each group of cars. The mean_age variable is the name of the new variable that will contain the mean age for each group.\nungroup() is the function that removes the grouping from the data frame. This is optional, but it is good practice to ungroup the data frame after you have finished summarising it.\n\nThe output of this code will be a new data frame that contains the mean age for each group of cars. The car variable is the grouping variable, and the mean_age variable is the summary statistic that we calculated for each group.\nYou can also calculate other summary statistics, such as the median, standard deviation, minimum, and maximum, using the summarise() function. You can also calculate multiple summary statistics at the same time by specifying multiple variables inside the summarise() function. For example, you can calculate the mean and standard deviation of the age variable for each group of cars.\n\n# Group the data frame by the car variable and calculate the mean and standard deviation of the age variable for each group\n\nmy_data %&gt;% group_by(car) %&gt;% \n  summarise(mean_age = mean(age), sd_age = sd(age)) %&gt;%\n  ungroup()\n\n# A tibble: 2 × 3\n  car      mean_age sd_age\n  &lt;fct&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n1 Electric     36.7  12.6 \n2 Petrol       38.3   7.64\n\n\nIn this code, we calculated the mean and standard deviation of the age variable for each group of cars. The mean_age and sd_age variables are the names of the new variables that will contain the mean and standard deviation for each group.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html",
    "href": "descriptive_stats.html",
    "title": "3  Exploratory and descriptive analysis",
    "section": "",
    "text": "3.1 Mean, median, and mode\nFor this section we will use the album_sales dataset, which we have already loaded in some of the videos. Let’s start by loading the dataset and displaying the first few rows:\nlibrary(tidyverse)\n\n# Load the album_sales dataset. The location of the dataset will be different based on where you saved it on your computer.\n\nalbum_sales &lt;- read.csv(\"Datasets/album_sales.csv\")\n\n# Display the first few rows of the dataset\n\nhead(album_sales)\n\n   Adverts Sales Airplay Attract   Genre\n1   10.256   330      43      10 Country\n2  985.685   120      28       7     Pop\n3 1445.563   360      35       7  HipHop\n4 1188.193   270      33       7  HipHop\n5  574.513   220      44       5   Metal\n6  568.954   170      19       5 Country\nThe dataset contains 5 variables: Adverts, Sales, Airplay, Attract and Genre. We will focus on the Sales variable for this section.\n# Calculate the mean of the Sales variable\n\nmean_sales &lt;- mean(album_sales$Sales)\n\nmean_sales\n\n[1] 193.2\nLet’s break down the code above:\nNext, let’s calculate the median of the Sales variable:\n# Calculate the median of the Sales variable\n\nmedian_sales &lt;- median(album_sales$Sales)\n\nmedian_sales\n\n[1] 200\nThe code above calculates the median of the Sales variable in the album_sales dataset. The median sales value is stored in the median_sales variable.\nFinally, let’s calculate the mode of the Sales variable. Unfortunately, R does not have a built-in function to calculate the mode. However, we do this in the following way:\n# Calculate the mode of the Sales variable\n\nalbum_sales$Sales %&gt;%\n table() \n\n.\n 10  30  40  50  60  70  80  90 100 110 120 130 140 150 160 170 180 190 200 210 \n  1   1   3   1   5   6   3   4   8   5  10   3  11  12   5   4   8   8   7  13 \n220 230 240 250 260 270 280 290 300 310 320 330 340 360 \n  6  17   7  10   3   3   5   8   6   2   4   2   3   6\nWe can see from the output that the mode of the Sales variable is 210, since that value appears most frequently in the dataset (13 times).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory and descriptive analysis</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#mean-median-and-mode",
    "href": "descriptive_stats.html#mean-median-and-mode",
    "title": "3  Exploratory and descriptive analysis",
    "section": "",
    "text": "At the end of this section, you will be able to:\n\n\n\n\nCalculate the mean, median, and mode of a dataset\n\n\n\n\n\n\n\n\n\nWe used the mean() function to calculate the mean of the Sales variable in the album_sales dataset. To do this, we specified the dataset album_sales and the variable Sales using the $ operator.\nThe mean sales value is stored in the mean_sales variable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory and descriptive analysis</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#standard-deviation-and-variance",
    "href": "descriptive_stats.html#standard-deviation-and-variance",
    "title": "3  Exploratory and descriptive analysis",
    "section": "3.2 Standard deviation and variance",
    "text": "3.2 Standard deviation and variance\n\n\n\n\n\n\nAt the end of this section, you will be able to:\n\n\n\n\nCalculate the standard deviation\nCalculate the variance\nCalculate the range of a dataset\nCalculate the interquartile range (IQR)\n\n\n\nNext, let’s calculate the standard deviation and variance of the Sales variable in the album_sales dataset:\n\n# Calculate the standard deviation of the Sales variable\n\nsd_sales &lt;- sd(album_sales$Sales)\n\nsd_sales\n\n[1] 80.69896\n\n\nThe code above calculates the standard deviation of the Sales variable in the album_sales dataset. The standard deviation value is stored in the sd_sales variable.\nNext, let’s calculate the variance of the Sales variable:\n\n# Calculate the variance of the Sales variable\n\nvar_sales &lt;- var(album_sales$Sales)\n\nvar_sales\n\n[1] 6512.322\n\n\nThe code above calculates the variance of the Sales variable in the album_sales dataset. The variance value is stored in the var_sales variable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory and descriptive analysis</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#range-and-interquartile-range",
    "href": "descriptive_stats.html#range-and-interquartile-range",
    "title": "3  Exploratory and descriptive analysis",
    "section": "3.3 Range and interquartile range",
    "text": "3.3 Range and interquartile range\nThe range of a dataset is the difference between the maximum and minimum values. Let’s calculate the range of the Sales variable in the album_sales dataset:\n\n# Calculate the range of the Sales variable\n\nrange_sales &lt;- range(album_sales$Sales)\n\nrange_sales\n\n[1]  10 360\n\n\nThe code above calculates the range of the Sales variable in the album_sales dataset. The range of the sales values is stored in the range_sales variable.\nThe interquartile range (IQR) is the difference between the 75th percentile (Q3) and the 25th percentile (Q1) of a dataset. Let’s calculate the IQR of the Sales variable in the album_sales dataset:\n\n# Calculate the interquartile range of the Sales variable\n\nIQR_sales &lt;- IQR(album_sales$Sales)\n\nIQR_sales\n\n[1] 112.5\n\n\nThe code above calculates the interquartile range (IQR) of the Sales variable in the album_sales dataset. The IQR value is stored in the IQR_sales variable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory and descriptive analysis</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#distribution-plots",
    "href": "descriptive_stats.html#distribution-plots",
    "title": "3  Exploratory and descriptive analysis",
    "section": "3.4 Distribution plots",
    "text": "3.4 Distribution plots\n\n\n\n\n\n\nAt the end of this section, you will be able to:\n\n\n\n\nCreate a histogram to visualize the distribution of a dataset\nCreate a box plot to visualize the distribution of a dataset\n\n\n\nNext, let’s create a histogram to visualize the distribution of the Sales variable in the album_sales dataset:\n\n# Create a histogram of the Sales variable\n\nhist(album_sales$Sales, main = \"Histogram of Sales\", xlab = \"Sales\", ylab = \"Frequency\", col = \"lightblue\")\n\n\n\n\n\n\n\n\nThe code above creates a histogram of the Sales variable in the album_sales dataset. The histogram displays the frequency of sales values in the dataset. The only required argument for the hist() function is the variable you want to plot. The main, xlab, ylab, and col arguments are optional and allow you to customize the appearance of the histogram.\nFinally, let’s create a box plot to visualize the distribution of the Sales variable in the album_sales dataset:\n\n# Create a box plot of the Sales variable\n\nboxplot(album_sales$Sales, main = \"Boxplot of Sales\", xlab = \"Sales\", col = \"lightblue\")\n\n\n\n\n\n\n\n\nThe code above creates a box plot of the Sales variable in the album_sales dataset. The box plot displays the distribution of sales values, including the median, quartiles, and outliers. The only required argument for the boxplot() function is the variable you want to plot. The main, xlab, and col arguments are optional and allow you to customize the appearance of the box plot.\n\nWe will learn more about plotting data with ggplot2 in another section. However, for now, we have used the base R functions hist() and boxplot() to create simple distribution plots.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory and descriptive analysis</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#assessing-the-normality-of-data",
    "href": "descriptive_stats.html#assessing-the-normality-of-data",
    "title": "3  Exploratory and descriptive analysis",
    "section": "3.5 Assessing the normality of data",
    "text": "3.5 Assessing the normality of data\n\n\n\n\n\n\nAt the end of this section, you will be able to:\n\n\n\n\nAssess the skewness and kurtosis of a dataset\nTest the normality of a dataset using the Shapiro-Wilk test\n\n\n\nMany statistical tests assume that the data is normally distributed. When your data sample size is small, violation of the normaility assumption could be an issue. Let’s assess the normality of the Sales variable in the album_sales dataset by calculating the skewness and kurtosis. In order to do this, we will use the psych package, which provides functions for calculating skewness and kurtosis. If you haven’t installed the psych package yet, you can do so by running the following code:\n\n# Install the psych package if you haven't already\n\ninstall.packages(\"psych\")\nNow, let’s calculate the skewness and kurtosis of the Sales variable in the album_sales dataset:\n\n# Load the psych package\n\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\n# Calculate the skewness of the Sales variable\n\nskew_sales &lt;- skew(album_sales$Sales)\n\nskew_sales\n\n[1] 0.0432729\n\n# Calculate the kurtosis of the Sales variable\n\nkurt_sales &lt;- kurtosi(album_sales$Sales)\n\nkurt_sales\n\n[1] -0.7157339\n\n\nWhen interpreting the skewness and kurtosis values, remember that values of 0 indicate a normal distribution.\nWe can also test the normality of the Sales variable using the Shapiro-Wilk test. The null hypothesis of the Shapiro-Wilk test is that the data is normally distributed. Let’s perform the Shapiro-Wilk test on the Sales variable in the album_sales dataset:\n\n# Perform the Shapiro-Wilk test on the Sales variable\n\nshapiro.test(album_sales$Sales)\n\n\n    Shapiro-Wilk normality test\n\ndata:  album_sales$Sales\nW = 0.98479, p-value = 0.02965\n\n\nThe output of the Shapiro-Wilk test includes the test statistic and the p-value. If the p-value is less than 0.05, we reject the null hypothesis and conclude that the data is not normally distributed. If the p-value is greater than 0.05, we fail to reject the null hypothesis and conclude that the data is normally distributed.\n\n\n\n\n\n\n\n\nAssessing normality\n\n\n\nThe shapiro-wilk test is sensitive to sample size. For small sample sizes, the test may be too conservative and reject the null hypothesis too often. For large sample sizes, the test may be too lenient and fail to reject the null hypothesis too often. Therefore, you should not rely solely on the Shapiro-Wilk test to assess the normality of your data. Visual inspection of the data using histograms and Q-Q plots is also recommended. Also remember that the central limit theorem states that the sampling distribution of the mean will be approximately normally distributed for large sample sizes, regardless of the distribution of the original data.\nWe could also use non-parametric bootstrapping methods to deal with non-normal data. We will cover this in a later section.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory and descriptive analysis</span>"
    ]
  },
  {
    "objectID": "basic_tests.html",
    "href": "basic_tests.html",
    "title": "5  Basic statistical tests",
    "section": "",
    "text": "5.1 Independent t-test\nThe independent t-test is used to compare the means of two independent groups. In R, you can use the t.test() function to perform an independent t-test. Here is an example:\n# This example uses the mtcars dataset, which is a built-in dataset in R that contains data on various car models.\n\n# Load the mtcars dataset\n\ndata(mtcars)\n\n# Independent t-test example: Is there a difference in fuel efficiency between automatic and manual cars?\n# the variable am is a binary variable indicating the type of transmission (0 = automatic, 1 = manual)\n\n# Perform the independent t-test\n\nt_test_result &lt;- t.test(mpg ~ am, data = mtcars)\n\n# Print the result\n\nt_test_result\n\n\n    Welch Two Sample t-test\n\ndata:  mpg by am\nt = -3.7671, df = 18.332, p-value = 0.001374\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -11.280194  -3.209684\nsample estimates:\nmean in group 0 mean in group 1 \n       17.14737        24.39231\nIn this example, we are comparing the fuel efficiency (mpg) of automatic and manual cars in the mtcars dataset. The mpg variable is the dependent variable, and the am variable is the independent variable. The t.test() function is used to perform the independent t-test, and the result is stored in the t_test_result variable.\nIf we look at the output, we can see the following:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "power_effectSize.html",
    "href": "power_effectSize.html",
    "title": "6  Sampling, power and effect size",
    "section": "",
    "text": "6.1 Different measures of effect size\nThere are different ways to calculate effect size depending on the type of data and the statistical test used. Here are some common effect size measures:\nIn the following sections, we will calculate the effect size for different types of data using some of these measures.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling, power and effect size</span>"
    ]
  },
  {
    "objectID": "power_effectSize.html#different-measures-of-effect-size",
    "href": "power_effectSize.html#different-measures-of-effect-size",
    "title": "6  Sampling, power and effect size",
    "section": "",
    "text": "Cohen’s d: This is a measure of the difference between two means in standard deviation units. It is commonly used in t-tests and ANOVA tests.\nEta-squared (\\(\\eta^2\\)): This is a measure of the proportion of variance in the dependent variable that is explained by the independent variable. It is commonly used in ANOVA tests.\nPhi coefficient (\\(\\phi\\)): This is a measure of the association between two binary variables. It is commonly used in chi-square tests.\nCorrelation coefficient (\\(r\\)): This is a measure of the strength and direction of the relationship between two continuous variables. It is commonly used in correlation tests.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling, power and effect size</span>"
    ]
  },
  {
    "objectID": "power_effectSize.html#calculating-cohens-d",
    "href": "power_effectSize.html#calculating-cohens-d",
    "title": "6  Sampling, power and effect size",
    "section": "6.2 Calculating Cohen’s d",
    "text": "6.2 Calculating Cohen’s d\nCohen’s d is a measure of the difference between two means in standard deviation units. It is calculated as the difference between the means divided by the pooled standard deviation. The formula for Cohen’s d is:\n\\[ d = \\frac{{\\bar{X}_1 - \\bar{X}_2}}{{s_p}} \\]\nwhere:\n\n\\(\\bar{X}_1\\) and \\(\\bar{X}_2\\) are the means of the two groups.\n\\(s_p\\) is the pooled standard deviation, calculated as:\n\n\\[s_p = \\sqrt{\\frac{{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}}{{n_1 + n_2 - 2}}} \\]\nwhere:\n\n\\(n_1\\) and \\(n_2\\) are the sample sizes of the two groups.\n\\(s_1\\) and \\(s_2\\) are the standard deviations of the two groups.\n\nLet’s calculate Cohen’s d for a hypothetical dataset with two groups. The dataset contains the following information:\n\nGroup 1: Mean = 10, Standard deviation = 2, Sample size = 30\nGroup 2: Mean = 12, Standard deviation = 3, Sample size = 30\n\nTo calculate Cohen’s d, we first need to calculate the pooled standard deviation (\\(s_p\\)) using the formula above. Then, we can calculate Cohen’s d using the formula for Cohen’s d.\nLet’s calculate Cohen’s d for this dataset using R:\n\n# Calculate Cohen's d\n\n# Group 1\n\nmean1 &lt;- 10\n\nsd1 &lt;- 2\n\nn1 &lt;- 30\n\n# Group 2\n\nmean2 &lt;- 12\n\nsd2 &lt;- 3\n\nn2 &lt;- 30\n\n# Calculate pooled standard deviation\n\nsp &lt;- sqrt(((n1 - 1) * sd1^2 + (n2 - 1) * sd2^2) / (n1 + n2 - 2))\n\n# Calculate Cohen's d\n\nd &lt;- (mean1 - mean2) / sp\n\n\nd\n\n[1] -0.7844645\n\n\nThe calculated value of Cohen’s d is -0.7844645. This negative value indicates that the mean of Group 1 is smaller than the mean of Group 2 by approximately 0.78 standard deviations.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling, power and effect size</span>"
    ]
  },
  {
    "objectID": "power_effectSize.html#calculating-eta-squared-and-other-effect-size-measures",
    "href": "power_effectSize.html#calculating-eta-squared-and-other-effect-size-measures",
    "title": "6  Sampling, power and effect size",
    "section": "6.3 Calculating Eta-squared and other effect size measures",
    "text": "6.3 Calculating Eta-squared and other effect size measures\nIt is possible to calculate other effect size measures such as Eta-squared, Phi coefficient. However, these measures are most commonly calculated using the output of statistical tests such as ANOVA and chi-square tests etc. To obtain these measures for the purpose of sample size calculation, you would usually look at previous studies or meta analyses to determine the expected effect size. For clinical research, you may also use the minimal clinically important difference (MCID) as a guide to determine the effect size.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling, power and effect size</span>"
    ]
  },
  {
    "objectID": "power_effectSize.html#power-analysis",
    "href": "power_effectSize.html#power-analysis",
    "title": "6  Sampling, power and effect size",
    "section": "6.4 Power analysis",
    "text": "6.4 Power analysis\nPower analysis is a method used to determine the sample size required to detect an effect of a given size with a certain level of confidence. It is important to conduct a power analysis before conducting a study to ensure that the sample size is adequate to detect the effect of interest.\nTo conduct a power analysis, you need to specify the following parameters:\n\nThe effect size: The size of the effect you want to detect. This is usually determined based on previous studies, meta-analyses or MCID.\nThe significance level (\\(\\alpha\\)): The probability of rejecting the null hypothesis when it is true (Type 1 error rate). This is commonly set at 0.05.\nThe power (\\(1 - \\beta\\)): The probability of correctly rejecting the null hypothesis when it is false (1 - Type 2 error rate). This is commonly set at 0.80 or 0.90.\nThe number of groups or conditions: The number of groups or conditions in the study.\n\nThe sample size required to achieve a desired power level can be calculated using power analysis functions in R. There are many packages in r for power analysis. The package pwr is one such package that provides functions to calculate the sample size required for different types of statistical tests.\nThe functions for some basic research designs are:\n\npwr.t.test(): For t-tests\npwr.anova.test(): For ANOVA tests\npwr.chisq.test(): For chi-square tests\npwr.f2.test(): For regression models\n\nLet’s calculate the sample size required to achieve a power of 0.80 for a t-test with the example data we used earlier. We will use the pwr.t.test() function from the pwr package to calculate the sample size required to achieve a power of 0.80 for a t-test with the following parameters:\n\nEffect size (Cohen’s d) = -0.7844645\nSignificance level (\\(\\alpha\\)) = 0.05\n\n\n# Load the pwr package\n\n\nlibrary(pwr)\n\n# Calculate the sample size required for a t-test\n# using the d value calculated earlier\n\npwr.t.test(d = d, sig.level = 0.05, power = 0.80)\n\n\n     Two-sample t test power calculation \n\n              n = 26.50429\n              d = 0.7844645\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThe output of the pwr.t.test() function provides the sample size required to achieve a power of 0.80 for a t-test with the specified effect size and significance level. The output includes the following information:\n\nn = The sample size required for each group to achieve a power of 0.80.\nd = The effect size (Cohen’s d) used in the power analysis.\nsig.level = The significance level used in the power analysis.\npower = The power level achieved with the specified sample size.\n\nThe sample size required to achieve a power of 0.80 for a t-test with the specified effect size and significance level is 26.5 for each group. Since the sample size must be a whole number, we would need to round up to the nearest whole number. Therefore, the sample size required for each group is 27.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling, power and effect size</span>"
    ]
  },
  {
    "objectID": "power_effectSize.html#more-complex-power-analysis",
    "href": "power_effectSize.html#more-complex-power-analysis",
    "title": "6  Sampling, power and effect size",
    "section": "6.5 More complex power analysis",
    "text": "6.5 More complex power analysis\nFor more complex designs, different approaches to power analysis might be necessary, such as using simulation. This is possible to do in R, but is beyond the scope of this chapter.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling, power and effect size</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#independent-t-test",
    "href": "basic_tests.html#independent-t-test",
    "title": "5  Basic statistical tests",
    "section": "",
    "text": "The t-test result is presented.\nThe alternative hypothesis is that the means are not equal.\nThe 95% confidence interval for the difference in means is presented.\nThe 2 sample means are presented.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#paired-t-test",
    "href": "basic_tests.html#paired-t-test",
    "title": "5  Basic statistical tests",
    "section": "5.2 Paired t-test",
    "text": "5.2 Paired t-test\n\n\nThe paired t-test is used to compare the means of two related groups. In R, you can use the t.test() function with the paired = TRUE argument to perform a paired t-test. Here is an example:\n\n# This example uses the sleep dataset, which is a built-in dataset in R that contains data on the effect of two soporific drugs on sleep duration.\n\n# Load the sleep dataset\n\ndata(sleep)\n\n# Paired t-test example: Is there a difference in sleep duration between the two drugs?\n\n# Perform the paired t-test\n\npaired_t_test_result &lt;- t.test(sleep$extra ~ sleep$group, paired = TRUE)\n\n# Print the result\n\npaired_t_test_result\n\n\n    Paired t-test\n\ndata:  sleep$extra by sleep$group\nt = -4.0621, df = 9, p-value = 0.002833\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.4598858 -0.7001142\nsample estimates:\nmean difference \n          -1.58 \n\n\nIn this example, we are comparing the sleep duration (extra) between patients who were given different drugs in the sleep dataset (note: 10 people were each measured twice). The extra variable is the dependent variable, and the group variable is the independent variable. The t.test() function is used to perform the paired t-test, and the result is stored in the paired_t_test_result variable.\nIf we look at the output, we can see the following:\n\nThe t-test result is presented.\nThe alternative hypothesis is that the means are not equal.\nThe 95% confidence interval for the difference in means is presented.\nThe mean difference is presented.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#wilcoxon-signed-rank-test",
    "href": "basic_tests.html#wilcoxon-signed-rank-test",
    "title": "5  Basic statistical tests",
    "section": "5.3 Wilcoxon signed-rank test",
    "text": "5.3 Wilcoxon signed-rank test\nThe Wilcoxon signed-rank test is a non-parametric test used to compare two related groups. In R, you can use the wilcox.test() function to perform a Wilcoxon signed-rank test. Here is an example:\n\n# This example uses the sleep dataset, which is a built-in dataset in R that contains data on the effect of two soporific drugs on sleep duration.\n\n# Load the sleep dataset\n\ndata(sleep)\n\n# Wilcoxon signed-rank test example: Is there a difference in sleep duration between the two drugs?\n\n# Perform the Wilcoxon signed-rank test\n\nwilcoxon_test_result &lt;- wilcox.test(sleep$extra ~ sleep$group, paired = TRUE)\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with ties\n\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with zeroes\n\n# Print the result\n\nwilcoxon_test_result\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  sleep$extra by sleep$group\nV = 0, p-value = 0.009091\nalternative hypothesis: true location shift is not equal to 0\n\n\nIn this example, we are comparing the sleep duration (extra) between patients who were given different drugs in the sleep dataset. The extra variable is the dependent variable, and the group variable is the independent variable. The wilcox.test() function is used to perform the Wilcoxon signed-rank test, and the result is stored in the wilcoxon_test_result variable.\nIf we look at the output, we can see the following:\n\nThe Wilcoxon signed-rank test result is presented ( V is the sum of the ranks of the differences between the pairs of observations).\nThe alternative hypothesis (true location shift is not equal to 0) is presented. This means that the medians of the two groups are not equal.\n\nNote: if there are ties in the data, the exact p-value is not calculated. Instead, an approximate p-value is presented.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#mann-whitney-u-test",
    "href": "basic_tests.html#mann-whitney-u-test",
    "title": "5  Basic statistical tests",
    "section": "5.4 Mann-Whitney U test",
    "text": "5.4 Mann-Whitney U test\nThe Mann-Whitney U test is a non-parametric test used to compare the means of two independent groups. In R, you can use the wilcox.test() function to perform a Mann-Whitney U test. Note: we will not include the paired = TRUE argument that we did in the previous example. Here is an example:\n\n# This example uses the mtcars dataset, which is a built-in dataset in R that contains data on various car models.\n\n# Load the mtcars dataset\n\ndata(mtcars)\n\n# Mann-Whitney U test example: Is there a difference in fuel efficiency between automatic and manual cars?\n\n# Perform the Mann-Whitney U test\n\nmann_whitney_test_result &lt;- wilcox.test(mpg ~ am, data = mtcars)\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with ties\n\n# Print the result\n\nmann_whitney_test_result\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  mpg by am\nW = 42, p-value = 0.001871\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe output is interpreted in the same way as the Wilcoxon signed-rank test output.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#chi-squared-test",
    "href": "basic_tests.html#chi-squared-test",
    "title": "5  Basic statistical tests",
    "section": "5.5 Chi-squared test",
    "text": "5.5 Chi-squared test\n\n\nThe chi-squared test is used to test the association between two categorical variables. In R, you can use the chisq.test() function to perform a chi-squared test. Here is an example:\n\n# This example uses the mtcars dataset, which is a built-in dataset in R that contains data on various car models.\n\n# Load the mtcars dataset\n\ndata(mtcars)\n\n# Chi-squared test example: Is there an association between the number of cylinders and the type of transmission?\n\n# Create a contingency table\n\ncontingency_table &lt;- table(mtcars$cyl, mtcars$am)\n\n# Perform the chi-squared test\n\nchi_squared_test_result &lt;- chisq.test(contingency_table)\n\nWarning in chisq.test(contingency_table): Chi-squared approximation may be\nincorrect\n\n# Print the result\n\nchi_squared_test_result\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 8.7407, df = 2, p-value = 0.01265\n\n\nIn this example, we are testing the association between the number of cylinders (cyl) and the type of transmission (am) in the mtcars dataset. The chisq.test() function is used to perform the chi-squared test, and the result is stored in the chi_squared_test_result variable.\nIf we look at the output, we can see the following:\n\nThe chi-squared test result is presented.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#correlation",
    "href": "basic_tests.html#correlation",
    "title": "5  Basic statistical tests",
    "section": "5.6 Correlation",
    "text": "5.6 Correlation\n\n\n\nCorrelation is used to test the relationship between two continuous variables. In R, you can use the cor.test() function to calculate the correlation coefficient with significance test. Here is an example:\n\n# This example uses the mtcars dataset, which is a built-in dataset in R that contains data on various car models.\n\n# Load the mtcars dataset\n\ndata(mtcars)\n\n# Correlation example: Is there a relationship between fuel efficiency and horsepower?\n\n# Calculate the correlation coefficient\n\ncorrelation_result &lt;- cor.test(mtcars$mpg, mtcars$hp)\n\n# Print the result\n\ncorrelation_result\n\n\n    Pearson's product-moment correlation\n\ndata:  mtcars$mpg and mtcars$hp\nt = -6.7424, df = 30, p-value = 1.788e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.8852686 -0.5860994\nsample estimates:\n       cor \n-0.7761684 \n\n\nIn this example, we are testing the relationship between fuel efficiency (mpg) and horsepower (hp) in the mtcars dataset. The cor.test() function is used to calculate the correlation coefficient, and the result is stored in the correlation_result variable.\nIf we look at the output, we can see the following:\n\nThe correlation coefficient is presented in the last line of the output.\nThe significance level of the correlation is tested using a t-test, which is also presented in the output.\nThe confidence interval for the correlation coefficient is presented.\nThe alternative hypothesis is that the correlation is not equal to 0.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#one-way-anova",
    "href": "basic_tests.html#one-way-anova",
    "title": "5  Basic statistical tests",
    "section": "5.7 One-way ANOVA",
    "text": "5.7 One-way ANOVA\nOne-way ANOVA (Analysis of Variance) is used to test the differences between the means of three or more groups. In R, you can use the aov() function to perform an ANOVA. Here is an example:\n\n\n\n# This example uses the mtcars dataset, which is a built-in dataset in R that contains data on various car models.\n\n# Load the mtcars dataset\n\ndata(mtcars)\n\n# One-way ANOVA example: Is there a difference in fuel efficiency between cars with different numbers of cylinders?\n\n# cyl should be a factor\n\nmtcars$cyl &lt;- as.factor(mtcars$cyl)\n\n# Perform the ANOVA\n\nanova_result &lt;- aov(mpg ~ cyl, data = mtcars)\n\n# Print the result\n\nsummary(anova_result)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncyl          2  824.8   412.4    39.7 4.98e-09 ***\nResiduals   29  301.3    10.4                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this example, we are testing the differences in fuel efficiency (mpg) between cars with different numbers of cylinders (cyl) in the mtcars dataset. The aov() function is used to perform the ANOVA, and the result is stored in the anova_result variable.\nIf we look at the output, we can see the following:\n\nThe ANOVA result is within the summary() function. The summary includes the F-statistic, the p-value, and the significance level of the ANOVA test.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#factorial-anova",
    "href": "basic_tests.html#factorial-anova",
    "title": "5  Basic statistical tests",
    "section": "5.8 Factorial ANOVA",
    "text": "5.8 Factorial ANOVA\nFactorial ANOVA is used to test the effects of two or more independent variables on a dependent variable. In R, you can use the aov() function with interaction terms to perform a factorial ANOVA. Here is an example:\n\n# This example uses the mtcars dataset, which is a built-in dataset in R that contains data on various car models.\n\n# Load the mtcars dataset\n\ndata(mtcars)\n\n# Factorial ANOVA example: Is there an interaction effect between the number of cylinders and the type of transmission on fuel efficiency?\n\n# cyl and am should be factors\n\nmtcars$cyl &lt;- as.factor(mtcars$cyl)\nmtcars$am &lt;- as.factor(mtcars$am)\n\n# Perform the factorial ANOVA\n\nfactorial_anova_result &lt;- aov(mpg ~ cyl * am, data = mtcars)\n\n# Print the result\n\nsummary(factorial_anova_result)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncyl          2  824.8   412.4  44.852 3.73e-09 ***\nam           1   36.8    36.8   3.999   0.0561 .  \ncyl:am       2   25.4    12.7   1.383   0.2686    \nResiduals   26  239.1     9.2                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this example, we are testing the interaction effect between the number of cylinders (cyl) and the type of transmission (am) on fuel efficiency (mpg) in the mtcars dataset. The aov() function is used to perform the factorial ANOVA, and the result is stored in the factorial_anova_result variable.\nThe interaction term is specified using the * operator in the formula.\nIf we look at the output, we can see the following:\n\nThe ANOVA result is within the summary() function. The summary includes the F-statistic, the p-value, and the significance level of each term tested in the ANOVA.\nEach independent variable is tested separately (main effects), and the interaction effect is also tested.\nThe interaction effect is denoted by the cyl:am term in the output.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#conclusion",
    "href": "basic_tests.html#conclusion",
    "title": "5  Basic statistical tests",
    "section": "5.9 Conclusion",
    "text": "5.9 Conclusion\nIn this chapter, we have covered several basic statistical tests that you can perform in R. These are included for reference and to help you get started with performing statistical tests in R. However, we will be focusing on modelling our data, using different types of regression models, rather than re-learning these tests.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "6  Correlation in R",
    "section": "",
    "text": "6.1 What is Correlation?\nCorrelation is a measure of the strength and direction of a relationship between two variables. It is most commonly used when we want to see if there is a relationship between two continuous variables. However, it is possible to run correlations between a continuous and a categorical variable (this is known as point-biserial correlation) or between two categorical variables (this is known as phi coefficient).\nSticking to the more conventional form of correlation; when we calculate correlation, we get an r value of between -1 and 1. This tells us two things:\nWe also often calculate the significance of the correlation, which tests against a null hypothesis that the correlation is 0. This is not part of the correlation per se, but it is often part of correlational research questions.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation in R</span>"
    ]
  },
  {
    "objectID": "correlation.html#what-is-correlation",
    "href": "correlation.html#what-is-correlation",
    "title": "6  Correlation in R",
    "section": "",
    "text": "The closer the value is to 1, the stronger the relationship. The closer the value is to 0, the weaker the relationship.\nThe sign of the value tells us the direction of the relationship. Positive values indicate a positive relationship (i.e. as one variable increases, so does the other). Negative values indicate a negative relationship (i.e. as one variable increases, the other decreases).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation in R</span>"
    ]
  },
  {
    "objectID": "correlation.html#how-is-correlation-calculated",
    "href": "correlation.html#how-is-correlation-calculated",
    "title": "6  Correlation in R",
    "section": "6.3 How is correlation calculated?",
    "text": "6.3 How is correlation calculated?\nCorrelation can be thought of as covariance divided by individual variance. Covariance is a measure of how much two variables change together. Variance is a measure of how much a variable changes on its own. When we divide covariance by variance, we get a value that is standardised and can be compared across different data sets.\nIf the changes are consistent with both variables (i.e. the covariance is higher and the individual variance is lower), then the final correlation value will be higher. However, if the changes are inconsistent (i.e. the covariance is lower and the individual variance is higher), then the final correlation value will be lower.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation in R</span>"
    ]
  },
  {
    "objectID": "correlation.html#running-correlation-in-r",
    "href": "correlation.html#running-correlation-in-r",
    "title": "5  Correlation",
    "section": "5.3 Running correlation in R",
    "text": "5.3 Running correlation in R\n\nStep 1: Check assumptions\n\nData,distribution,linearity\n\nStep 2: Run correlation\nStep 3: Check R value\nStep 4: Check significance\n\n\n5.3.1 Check assumptions: data\n\nParametric tests require interval or ratio data\nIf the data are ordinal then a non-parametric correlation is used\n\n\nWhat type of data are treatment duration and aggression level?\n\n\n\n5.3.2 Check assumptions: distribution\n\nParametric tests require normally distributed data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.3.3 Check assumptions: distribution #2\n\nParametric tests require normally distributed data\n\n\nshapiro.test(regression_data$treatment_duration)\n\n\n    Shapiro-Wilk normality test\n\ndata:  regression_data$treatment_duration\nW = 0.94971, p-value = 0.0007939\n\n\n\nshapiro.test(regression_data$aggression_level)\n\n\n    Shapiro-Wilk normality test\n\ndata:  regression_data$aggression_level\nW = 0.9928, p-value = 0.8756\n\n\n\nThe normality assumption is less of an issue when sample size is &gt; 30\n\n\n\n5.3.4 Checking assumptions: linearity\n\n\n\n\n\n\n\n\n\n\nregression_data %&gt;% ggplot(aes(x=treatment_duration,y=aggression_level)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nHere we are looking to see if the relationship is linear\n\n\n\n5.3.5 Run correlation\n\nR can run correlations using the cor.test() command\n\n\ncor.test(regression_data$treatment_duration,regression_data$aggression_level)\n\n\n    Pearson's product-moment correlation\n\ndata:  regression_data$treatment_duration and regression_data$aggression_level\nt = -9.5503, df = 98, p-value = 1.146e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7838251 -0.5765006\nsample estimates:\n       cor \n-0.6942996 \n\n\n\n\n5.3.6 Check r Value (correlation value)\n\nThe r value tells us the strength and direction of the relationship\nIn the output it is labelled as “cor” (short for correlation)\n\n\ncor.test(regression_data$treatment_duration,regression_data$aggression_level)\n\n\n    Pearson's product-moment correlation\n\ndata:  regression_data$treatment_duration and regression_data$aggression_level\nt = -9.5503, df = 98, p-value = 1.146e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7838251 -0.5765006\nsample estimates:\n       cor \n-0.6942996 \n\n\n\n\n5.3.7 Check the significance of the correlation\n\nWe can see that the significance by looking at the p value\n\nThe significance is 1.146^-15\nThis means: 0.0000000000000001146\n\nTherefore p value &lt; 0.05\n\n\ncor.test(regression_data$treatment_duration,regression_data$aggression_level)\n\n\n    Pearson's product-moment correlation\n\ndata:  regression_data$treatment_duration and regression_data$aggression_level\nt = -9.5503, df = 98, p-value = 1.146e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7838251 -0.5765006\nsample estimates:\n       cor \n-0.6942996",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#what-is-regression",
    "href": "correlation.html#what-is-regression",
    "title": "5  Correlation and simple regression",
    "section": "6.1 What is regression?",
    "text": "6.1 What is regression?\n\nTesting to see if we can make predictions based on data that are correlated\n\n\nWe found a strong correlation between treatment duration and agression levels. Can we use this data to predict aggression levels of other clients, based on their treatment duration?\n\n\nWhen we carry out regression, we get a information about:\n\nHow much variance in the outcome is explained by the predictor\nHow confident we can be about these results generalising (i.e. significance)\nHow much error we can expect from anu predictions that we make (i.e. standard error of the estimate)\nThe figures we need to calculate a predicted outcome value (i.e. coefficient values)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#how-is-regression-calculated",
    "href": "correlation.html#how-is-regression-calculated",
    "title": "5  Correlation and simple regression",
    "section": "6.2 How is regression calculated?",
    "text": "6.2 How is regression calculated?\n\n\n\n\n\n\n\n\n\n\nWhen we run a regression analysis, a calculation is done to select the “line of best fit”\nThis is a “prediction line” that minimises the overall amount of error\n\nError = difference between the data points and the line",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#the-regression-equation",
    "href": "correlation.html#the-regression-equation",
    "title": "5  Correlation and simple regression",
    "section": "6.3 The regression equation",
    "text": "6.3 The regression equation\n\n\n\n\n\n\n\n\n\n\nOnce the line of best fit is calculated, predictions are based on this line\nTo make predictions we need the intercept and slope of the line\n\nIntercept or constant= where the line crosses the y axis\nSlope or beta = the angle of the line\n\nPredictions are made using the calculation for a line: Y = bX + c\nYou can think of the equation like this:\n\npredicted outcome value = beta coefficient * value of predictor + constant",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#running-regression-in-r",
    "href": "correlation.html#running-regression-in-r",
    "title": "5  Correlation and simple regression",
    "section": "6.4 Running regression in R",
    "text": "6.4 Running regression in R\n\nStep 1: Run regression\nStep 2: Check assumptions\n\nData\nDistribution\nLinearity\nHomogeneity of variance\nUncorrelated predictors\nIndpendence of residuals\nNo influental cases / outliers\n\nStep 3: Check R^2 value\nStep 4: Check model significance\nStep 5: Check coefficient values",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#run-regression",
    "href": "correlation.html#run-regression",
    "title": "5  Correlation and simple regression",
    "section": "6.5 Run regression",
    "text": "6.5 Run regression\n\nWe use the lm() command to run regression while saving the results\nWe then use the summary() function to check the results\n\n\nmodel1 &lt;- lm(formula= aggression_level ~ treatment_duration ,data=regression_data)\nsummary(model1)\n\n\nCall:\nlm(formula = aggression_level ~ treatment_duration, data = regression_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4251 -1.1493 -0.0593  0.8814  3.4542 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         12.3300     0.7509   16.42  &lt; 2e-16 ***\ntreatment_duration  -0.6933     0.0726   -9.55 1.15e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.551 on 98 degrees of freedom\nMultiple R-squared:  0.4821,    Adjusted R-squared:  0.4768 \nF-statistic: 91.21 on 1 and 98 DF,  p-value: 1.146e-15",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#what-are-residuals",
    "href": "correlation.html#what-are-residuals",
    "title": "5  Correlation and simple regression",
    "section": "6.6 What are residuals?",
    "text": "6.6 What are residuals?\n\nIn regression, the assumptions apply to the residuals, not the data themselves\nResidual just means the difference between the data point and the regression line",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#check-assumptions-distribution-1",
    "href": "correlation.html#check-assumptions-distribution-1",
    "title": "5  Correlation",
    "section": "6.7 Check assumptions: distribution",
    "text": "6.7 Check assumptions: distribution\n\nUsing the plot() command on our regression model will give us some useful diagnostic plots\nThe second plot that it outputs shows the normality\n\n\nplot(model1, which=2)\n\n\n\n\n\n\n\n\n\nWe could also use a histogram to check the distribution\nNotice how we can use the $ sign to get the residuals from the model\n\n\nhist(model1$residuals)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#check-assumptions-linearity",
    "href": "correlation.html#check-assumptions-linearity",
    "title": "5  Correlation and simple regression",
    "section": "6.8 Check assumptions: linearity",
    "text": "6.8 Check assumptions: linearity\n\nUsing the plot() command on our regression model will give us some useful diagnostic plots\nThe first plot that it outputs shows the residuals vs the fitted values\nHere, we want to see them spread out, with the line being horizontal and straight\n\n\nplot(model1, which=1)\n\n\n\n\n\n\n\n\n\nThere is a slight amount of curvilinearity here but nothing to be worried about",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#check-assumptions-homogeneity-of-variance-1",
    "href": "correlation.html#check-assumptions-homogeneity-of-variance-1",
    "title": "5  Correlation and simple regression",
    "section": "6.9 Check assumptions: Homogeneity of Variance #1",
    "text": "6.9 Check assumptions: Homogeneity of Variance #1\n\nWe can use the sample plot to check Homogeneity of Variance\nWe want the variance to be constant across the data set. We do not want the variance to change at different points in the data\n\n\nplot(model1, which=1)\n\n\n\n\n\n\n\n\n\nA violation of Homogeneity of Variance would usually look like a funnel, with the data narrowing",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#check-assumptions-influential-cases",
    "href": "correlation.html#check-assumptions-influential-cases",
    "title": "5  Correlation and simple regression",
    "section": "6.10 Check assumptions: Influential cases",
    "text": "6.10 Check assumptions: Influential cases\n\nWe need to check that there are no extreme outliers - they could throw off our predictions\nWe are looking for participants that have high rediduals + high leverage\n\nSome guidance suggests anything higher than 1 is an influential case\nOthers suggest 4/n is the cut off point (4 divided by number of participants)\n\n\n\nplot(model1, which=4)\n\n\n\n\n\n\n\n\n\nWe are looking for participants that have high rediduals + high leverage\n\nNo cases over 1\nMany are over 0.04 (4/n = 0.04)\n\n\n\nplot(model1, which=5)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#check-the-r-squared-value",
    "href": "correlation.html#check-the-r-squared-value",
    "title": "5  Correlation and simple regression",
    "section": "6.11 Check the r squared value",
    "text": "6.11 Check the r squared value\n\nr^2 = the amount of variance in the outcome that is explained by the predictor(s)\nThe closer this value is to 1, the more useful our regression model is for predicting the outcome\n\n\nmodelSummary &lt;- summary(model1)\nmodelSummary\n\n\nCall:\nlm(formula = aggression_level ~ treatment_duration, data = regression_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4251 -1.1493 -0.0593  0.8814  3.4542 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         12.3300     0.7509   16.42  &lt; 2e-16 ***\ntreatment_duration  -0.6933     0.0726   -9.55 1.15e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.551 on 98 degrees of freedom\nMultiple R-squared:  0.4821,    Adjusted R-squared:  0.4768 \nF-statistic: 91.21 on 1 and 98 DF,  p-value: 1.146e-15\n\n\n\nThe r^2 of 0.482052 means that 48% of the variance in aggression level is explained by treatment duration",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#check-model-significance",
    "href": "correlation.html#check-model-significance",
    "title": "5  Correlation and simple regression",
    "section": "6.12 Check model significance",
    "text": "6.12 Check model significance\n\nThe model significance is displayed at the very end of the output\n\np-value: 1.146e-15\nAs p &lt; 0.05, the model is significant\n\n\n\nmodelSummary\n\n\nCall:\nlm(formula = aggression_level ~ treatment_duration, data = regression_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4251 -1.1493 -0.0593  0.8814  3.4542 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         12.3300     0.7509   16.42  &lt; 2e-16 ***\ntreatment_duration  -0.6933     0.0726   -9.55 1.15e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.551 on 98 degrees of freedom\nMultiple R-squared:  0.4821,    Adjusted R-squared:  0.4768 \nF-statistic: 91.21 on 1 and 98 DF,  p-value: 1.146e-15",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#check-coefficient-values",
    "href": "correlation.html#check-coefficient-values",
    "title": "5  Correlation and simple regression",
    "section": "6.13 Check coefficient values",
    "text": "6.13 Check coefficient values\n\nThe coefficient values are displayed in the coefficients table\nIf we have more than one predictor, they are all listed here\n\n\nmodelSummary$coefficients\n\n                     Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)        12.3300211 0.75087601 16.420848 6.840516e-30\ntreatment_duration -0.6933201 0.07259671 -9.550297 1.145898e-15\n\n\n\nThe beta coefficient for treatment duration is in the Estimate column\nFor every unit increase in treatment duration, aggression level decreases by 0.69",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#the-regression-equation-1",
    "href": "correlation.html#the-regression-equation-1",
    "title": "5  Correlation and simple regression",
    "section": "6.14 The regression equation",
    "text": "6.14 The regression equation\n\nThe regression equation is:\n\nOutcome = predictor value * beta coefficient + constant\n\nFor this model, that is:\n\nAggression level = treatment duration * -0.69 + 12.33\n\nmodelSummary$coefficients\n\n                     Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)        12.3300211 0.75087601 16.420848 6.840516e-30\ntreatment_duration -0.6933201 0.07259671 -9.550297 1.145898e-15",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#accounting-for-error-in-predictions",
    "href": "correlation.html#accounting-for-error-in-predictions",
    "title": "5  Correlation and simple regression",
    "section": "6.15 Accounting for error in predictions",
    "text": "6.15 Accounting for error in predictions\n\nWe also know that the accuracy of predictions will be within a certain margin of error\nThis is known as standard error of the estimate or residual standard error\n\n\nmodelSummary\n\n\nCall:\nlm(formula = aggression_level ~ treatment_duration, data = regression_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4251 -1.1493 -0.0593  0.8814  3.4542 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         12.3300     0.7509   16.42  &lt; 2e-16 ***\ntreatment_duration  -0.6933     0.0726   -9.55 1.15e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.551 on 98 degrees of freedom\nMultiple R-squared:  0.4821,    Adjusted R-squared:  0.4768 \nF-statistic: 91.21 on 1 and 98 DF,  p-value: 1.146e-15",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "intro.html#revision-quiz",
    "href": "intro.html#revision-quiz",
    "title": "1  Introduction to R and R Studio",
    "section": "Revision Quiz",
    "text": "Revision Quiz\n\nWhat is the purpose of R scripts in the context of data analysis? r mcq(c( \"To create interactive visualizations\", \"To organize files and folders on your computer\", answer =  \"To type and save analysis code for later use\", \"To display the output of R code immediately\"))\nWhat does the Environment in R Studio primarily display? r mcq(c( \"The R script files\", \"Publication-ready outputs\", answer = \"Objects (data, analysis, plots) currently in memory\",  \"The R package installation menu\"))\nWhich of the following is a reason to learn and use R? r mcq(c( \"It is a closed-source software\", \"It is used exclusively by programmers\", answer = \"It has a large number of packages and a wide community of users\",  \"It doesn't require any coding skills\"))\nHow can you install an R package in R Studio? r mcq(c( \"By clicking 'Load' in the Environment section\", \"By running the package's executable file\", answer = \"By using the 'install.packages()' function\", \"By copying the package files to the working directory\"))\nWhat does the ‘Run’ button do in R Studio? r mcq(c( \"It compiles the R script into a standalone program\", \"It generates publication-ready outputs\", answer = \"It runs the code in the R script\",  \"It opens the R package manager\"))\nHow can you load a previously installed R package? r mcq(c( \"By clicking the 'Load' button in the Console\", \"By typing 'install.packages()' in the Console\", answer =  \"By using the 'library(package_name)' command\", \"By copying the package files to the working directory\"))\nTrue or False: You can work on multiple datasets at the same time using R / R Studio? r torf(TRUE)\nWhat advantage does using R scripts for analysis provide? r mcq(c( \"They enable real-time collaboration with other users\", \"They allow you to create complex interactive visualizations\",answer =  \"They encourage transparency and reproducibility of analysis\", \"They automatically generate publication-ready reports\"))\nWhat is the main role of the R Console in R Studio? r mcq(c( \"To create interactive visualizations\", \"To dispay objects in memory\", \"To organize and manage files in the workspace\", answer = \"To provide an interactive environment to run R code\"))\n“What is the difference between R and R Studio? r mcq(c( answer = \"R is a statistical programming language, while R Studio is a software developed for using R to do analysis\", \"R is a data visualization tool, while R Studio is a statistical analysis software\", \"R is used for web development, while R Studio is used for machine learning\",  \"R is the name of the software, while R Studio is a community forum\"))”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R and R Studio</span>"
    ]
  },
  {
    "objectID": "correlation.html#which-correlation-to-use",
    "href": "correlation.html#which-correlation-to-use",
    "title": "6  Correlation in R",
    "section": "6.4 Which correlation to use?",
    "text": "6.4 Which correlation to use?\nWhen we run correlation in R, we use the cor.test() command. This command will give us the correlation value, the p value and the confidence intervals.\nWe can specify a Pearson correlation (the default) or a Spearman correlation (for non-parametric data).\n\n6.4.1 Running correlation in R\n\nR can run correlations using the cor.test() command\n\n\ncor.test(regression_data$treatment_duration,regression_data$aggression_level) \n\n\n    Pearson's product-moment correlation\n\ndata:  regression_data$treatment_duration and regression_data$aggression_level\nt = -9.5503, df = 98, p-value = 1.146e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7838251 -0.5765006\nsample estimates:\n       cor \n-0.6942996 \n\n\nIn the above example, we are testing the correlation between treatment duration and aggression level. Each variable is separated by a comma.\n\n\n6.4.2 Interpreting the output\n\nThe r value tells us the strength and direction of the relationship\nIn the output it is labelled as “cor” (short for correlation)\n\nCorrelation values can range from -1 to 1. The closer the value is to 1, the stronger the relationship. The closer the value is to 0, the weaker the relationship. Positive values indicate a positive relationship (i.e. as one variable increases, so does the other). Negative values indicate a negative relationship (i.e. as one variable increases, the other decreases).\n\ncor.test(regression_data$treatment_duration,regression_data$aggression_level)\n\n\n    Pearson's product-moment correlation\n\ndata:  regression_data$treatment_duration and regression_data$aggression_level\nt = -9.5503, df = 98, p-value = 1.146e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7838251 -0.5765006\nsample estimates:\n       cor \n-0.6942996 \n\n\n\n\n6.4.3 Check the significance of the correlation\n\nWe can see that the significance by looking at the p value\n\nThe significance is 1.146^-15\nThis means: 0.0000000000000001146\n\nTherefore p value &lt; 0.05\n\n\ncor.test(regression_data$treatment_duration,regression_data$aggression_level)\n\n\n    Pearson's product-moment correlation\n\ndata:  regression_data$treatment_duration and regression_data$aggression_level\nt = -9.5503, df = 98, p-value = 1.146e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7838251 -0.5765006\nsample estimates:\n       cor \n-0.6942996 \n\n\n\n\n\n\n\n\nExponent values\n\n\n\nWhen we see a value like 1.146e-15, this is a shorthand way of writing a very small number. The e-15 means that we move the decimal point 15 places to the left. So 1.146e-15 is the same as 0.000000000000001146",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation in R</span>"
    ]
  },
  {
    "objectID": "correlation.html#exponent-values",
    "href": "correlation.html#exponent-values",
    "title": "6  Correlation in R",
    "section": "6.5 Exponent values",
    "text": "6.5 Exponent values\nWhen we see a value like 1.146e-15, this is a shorthand way of writing a very small number. The e-15 means that we move the decimal point 15 places to the left. So 1.146e-15 is the same as 0.000000000000001146",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation in R</span>"
    ]
  },
  {
    "objectID": "correlation.html#check-assumptions-distribution",
    "href": "correlation.html#check-assumptions-distribution",
    "title": "5  Correlation and simple regression",
    "section": "6.7 Check assumptions: distribution",
    "text": "6.7 Check assumptions: distribution\n\nUsing the plot() command on our regression model will give us some useful diagnostic plots\nThe second plot that it outputs shows the normality\n\n\nplot(model1, which=2)\n\n\n\n\n\n\n\n\n\nWe could also use a histogram to check the distribution\nNotice how we can use the $ sign to get the residuals from the model\n\n\nhist(model1$residuals)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#visualising-correlation",
    "href": "correlation.html#visualising-correlation",
    "title": "6  Correlation in R",
    "section": "6.2 Visualising correlation",
    "text": "6.2 Visualising correlation\nWe can visualise correlation using a scatterplot. This is a graph where each data point is plotted on a graph, with one variable on the x axis and the other on the y axis. If the data points form something resembling a straight line, with all of the data points in a consistent pattern, then we have a strong correlation. If the data points are scattered, then we have a weaker correlation. However, if the data points are inconsistent or more diffuse, the correlation is weaker. The direction of the line tells us the direction of the correlation.\n\n\n\n\n\n\n\n\n\nVisualising the data in this way can give us a good idea of the strength and direction of the correlation. However, it is not a substitute for running the correlation itself. At the same time, correlation coefficients can be misleading on their own. It is always a good idea to visualise the data as well.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation in R</span>"
    ]
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "7  Simple Regression in R",
    "section": "",
    "text": "8 What is regression analysis?\nRegression analysis is a statistical technique used to model the relationship between an outcome (dependent) variable and one or more predictor (criterion/independent) variables. The goal of regression analysis is to understand how the outcome variable changes as the predictor variable changes.\nWhen we say simple regression, we mean that there is only one predictor variable. This is to distinguish from multiple regression, where there are two or more predictor variables.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simple Regression in R</span>"
    ]
  },
  {
    "objectID": "regression.html#checking-model-assumptions",
    "href": "regression.html#checking-model-assumptions",
    "title": "7  Simple Regression in R",
    "section": "9.1 Checking model assumptions",
    "text": "9.1 Checking model assumptions\nBefore interpreting the results of a regression analysis, it is essential to check the assumptions of the model. The key assumptions of linear regression are:\n\nLinearity: The relationship between the predictor and outcome variables is linear.\nIndependence: The residuals (errors) are independent of each other.\nHomoscedasticity: The residuals have constant variance.\nNormality: The residuals are normally distributed.\n\nWe can check these assumptions using diagnostic plots. The plot() function in R can be used to create diagnostic plots for the regression model.\n\n# Create diagnostic plots\n\n1plot(regression_model)\n\n\n1\n\nWe create diagnostic plots for the regression model using the plot() function. The diagnostic plots include a scatterplot of the residuals against the fitted values, a Q-Q plot of the residuals, a scale-location plot, and a plot of the residuals against the predictor variable.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simple Regression in R</span>"
    ]
  },
  {
    "objectID": "regression.html#check-the-assumptions",
    "href": "regression.html#check-the-assumptions",
    "title": "7  Simple Regression in R",
    "section": "9.2 Check the assumptions",
    "text": "9.2 Check the assumptions\nWe can check the assumptions of linear regression using the plot() function.\n\n# check the assumptions\n\n1plot(model)\n\n\n1\n\nThe plot() function is used to create a plot of the model. It actually shows several plots in sequence.\n\n\n\n\n\nWhen you run the plot function, there will be a message in the console that says “Hit  to see next plot:”. You need to press enter to move through the plots.\n\n\n9.2.1 The assumption of independence\nThis assumption is the idea that each observation is independent of the others. In other words, the value of one observation (e.g. a participant’s score) should not be related to the value of another participant’s score. This is not interpreted from the plots, but is an assumption that should be considered when collecting data. For example, you would not use this approach to analyse data from a repeated measures design, because the observations are not independent.\n\n\n9.2.2 The assumption of linearity\nThe first plot is a plot of the residuals against the fitted values. This plot is used to check the assumption of linearity. The assumption of linearity states that the relationship between the predictor variable and the outcome variable is linear. In other words, the relationship between the predictor variable and the outcome variable can be described by a straight line. If the relationship is not linear, then the model is not a good fit for the data.\n\n\n\nLinear and non-linear data\n\n\n\n\n\n\n\n\nLinear and non-linear residuals\n\n\n\nGood: Random scatter around the line. The line is straight.\nBad: Non-random scatter around the line. The line is not straight.\nWhat do we do if this assumption is violated?: Linearity is a very important assumption. If the assumption is violated, then the linear model is likely not a good fit for the data. In this case we should probably try a different model. Data are never perfect, but we shouldn’t ignore a clear violation of linearity.\n\n\n\n\n9.2.3 The assumption of normality\nThe second plot is a normal Q-Q plot of the residuals. This plot is used to check the assumption of normality. The assumption of normality states that the residuals are normally distributed. If the residuals are not normally distributed, then the model is not a good fit for the data.\n\n\n\nqqplot of residuals\n\n\n\n\n\n\n\n\nNormal and non-normal residuals\n\n\n\nGood: The points follow the line.\nBad: The points do not follow the line.\nWhat do we do if this assumption is violated?: Normality affects the accuracy of beta values, significance tests and confidence intervals. However, it is most important with small sample sizes. As sample size increases, the assumption of normality becomes less important.\n\n\n\n\n9.2.4 The assumption of homoscedasticity\nThe third plot is a scale-location plot of the residuals against the fitted values. This plot is used to check the assumption of homoscedasticity. The assumption of homoscedasticity states that the residuals have equal variance. If the residuals do not have equal variance, then the model is not a good fit for the data.\n\n\n\nhomoscedasticity\n\n\n\n\n\n\n\n\nHomoscedastic and heteroscedastic residuals\n\n\n\nGood: The points are randomly scattered around the line. and the line is horizontal.\nBad: The points are not randomly scattered around the line. and the line is not horizontal.\nWhat do we do if this assumption is violated?: This will affect the accuracy of the beta values, significance tests and confidence intervals. Essentially, it means that conclusions we draw from the model are less accurate. What we do depends on the situation. Transformation of the DV (e.g. log transformation) might help. If not, there are weighted regression models that can be used.\n\n\n\n\n9.2.5 Checking for outliers or influential cases\nThe fourth plot is a plot of Cook’s distance. This plot is used to check for outliers. An outlier is a data point that is very different from the rest of the data. If there are outliers, then they could be affecting the regression model. The threshold for Cook’s distance is 1. If a data point has a Cook’s distance greater than 1, then it is considered an outlier.\nThe fifth plot is a plot of the residuals against the leverage. The sixth plot is a plot of the Cook’s distance against the leverage. They are pretty much the same plots as the plot 4.\n\n\n\nInfluential Cases and Leverage\n\n\nOn the plot above, Cook’s Distance is indicated by a red line. If a data point is outside the red line, then it is considered an outlier.\nLeverage is the idea that a particular outlier might have a lot of influence on the regression model. Look for data points that are outside the red line on the top right or bottom right of the plot. These are data points that have a lot of leverage and might be influencing the regression model.\n\n\n\n\n\n\nOutliers and influential cases\n\n\n\nGood: No data points outside the red lines.\nBad: Data points outside the red lines. Outliers with high leverage.\nWhat do we do if this assumption is violated?: Outliers will affect the calculation of variances (e.g. sum of squares or standard deviation) that are used in many calculations related to the regression model. If there are influentual cases, we should consider removing them from the analysis. When doing so, it is important to explain why you removed them, and be transparent about how this affected the model results.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simple Regression in R</span>"
    ]
  },
  {
    "objectID": "regression.html#interpret-the-regression-model-results",
    "href": "regression.html#interpret-the-regression-model-results",
    "title": "7  Simple Regression in R",
    "section": "9.3 Interpret the regression model results",
    "text": "9.3 Interpret the regression model results\nTo view the results of the regression model, we use the summary() function.\n\n# View the results\n\n1summary(regression_model)\n\n\n1\n\nWe view the results of the regression model using the summary() function. The output includes the coefficients, standard errors, t-values, p-values, and R-squared value of the model.\n\n\n\n\n\nCall:\nlm(formula = trust_score ~ treatment_duration, data = regression_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.585 -24.991   0.304  24.285  46.804 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          64.754     14.176   4.568 1.44e-05 ***\ntreatment_duration   -1.130      1.371  -0.824    0.412    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 29.29 on 98 degrees of freedom\nMultiple R-squared:  0.006886,  Adjusted R-squared:  -0.003248 \nF-statistic: 0.6795 on 1 and 98 DF,  p-value: 0.4118\n\n\n\n9.3.1 Call: The regression formula\nThe first line of the output is the regression formula. This is the formula that was used to create the model.\n\n\n9.3.2 Residuals: The residuals\nThe second line of the output is the residuals. The residuals are the difference between the actual values of the outcome variable and the predicted values of the outcome variable. This section is giving us some summary statistics about the residuals. However, we usually check the assumptions using the plots.\n\n\n9.3.3 Coefficients: The beta values\nThe third section of the output is the coefficients. You will see a line of values for the intercept and another line for each of the predictor variables in the model. Estimate is the beta value. Std. Error is the standard error of the beta value. Pr(&gt;|t|) is the p value for the beta value.\n\nIntercept: We are not usually interested in this line by itself. It is the value of the outcome variable when all of the predictor variables are equal to zero. In this case, it is the value of avoidance when depression is equal to zero. However, it might be the case that depression cannot be equal to zero. In this case, the intercept would not be meaningful. If the predictor were a categorical variable, then the intercept would be the value of the outcome variable when the predictor variable is equal to the reference category (i.e. The mean of the outcome for that group).\nDepression: This is the beta value for depression. It is the amount that avoidance changes when depression increases by one unit. What unit means, depends on how the variables were measured, so it is likely to mean one point in the scale used to measure depression, for example.\n\nThe final section in the output shows:\n\nResidual standard error. This is the standard deviation of the residuals. It is the average amount that the actual values of the outcome variable differ from the predicted values of the outcome variable.\nMultiple R-squared. This is the R-squared value. It is the amount of variance in the outcome variable that is explained by the model. We usually talk about this as a percentage value.\nAdjusted R-squared. This is the adjusted R-squared value. It is the amount of variance in the outcome variable that is explained by the model, adjusted for the number of predictor variables in the model. This is to account for the fact that having more predictors in the model will always increase the R-squared value, even if the predictors are not related to the outcome variable. It is relevant when we have more than one predictor variable in the model.\nF Statistic. The F value comes from the ANOVA that is used to test the significance of the model. It tests the null hypothesis that all of the beta values are equal to zero.\np-value. This is the p value for the F statistic (the significance of the overall regression model, with all of the predictors).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simple Regression in R</span>"
    ]
  },
  {
    "objectID": "regression.html#regression-with-a-categorical-predictor-variable",
    "href": "regression.html#regression-with-a-categorical-predictor-variable",
    "title": "7  Simple Regression in R",
    "section": "9.4 Regression with a categorical predictor variable",
    "text": "9.4 Regression with a categorical predictor variable\nSo far, we have conducted a regression analysis with a continuous predictor variable. Now, let’s consider a regression analysis with a categorical predictor variable.\n\n\nOur next hypothesis is that the level of depression is different for each treatment group. We can test this hypothesis using a regression model with a categorical predictor variable. Let’s conduct a regression analysis with the treatment variable as the predictor variable. If there are 2 levels of the predictor variable, then the model will compare the two levels. If there are more than 2 levels, then the model will compare each level to the reference level. By default, R uses the first level of the predictor variable as the reference level. Howeverm you can specify a different reference level using the relevel() function.\n\n# specify the reference level - this is more useful when we have more than 2 levels\n\n0regression_data$treatment_group &lt;- as.factor(regression_data$treatment_group)\n1regression_data$treatment &lt;- relevel(regression_data$treatment_group, ref = \"therapy1\")\n\n# Fit a regression model with a categorical predictor variable\n\n2regression_model2 &lt;- lm(aggression_level ~ treatment_group, data = regression_data)\n\n# Summary of the model\n\n3summary(regression_model2)\n\n\n0\n\nWe convert the treatment variable to a factor variable using the as.factor() function. This is necessary for R to recognize the variable as a categorical variable.\n\n1\n\nWe specify the reference level for the treatment variable using the relevel() function. In this case, we set the reference level to “therapy1”. This is likely the default level, but we are specifying it here for clarity. If we had more than 2 levels, we could specify a different reference level using the ref argument. For example, `ref = “control”\n\n2\n\nWe fit a regression model with the treatment_group variable as the predictor variable. The outcome variable is aggression_level. The data are specified using the data argument.\n\n3\n\nWe view the results of the regression model using the summary() function. The output includes the coefficients, standard errors, t-values, p-values, and R-squared value of the model.\n\n\n\n\n\nCall:\nlm(formula = aggression_level ~ treatment_group, data = regression_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.6800 -1.4882 -0.0185  1.3460  4.4131 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               4.6800     0.2843  16.464  &lt; 2e-16 ***\ntreatment_grouptherapy2   1.3201     0.4103   3.217  0.00175 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.05 on 98 degrees of freedom\nMultiple R-squared:  0.09554,   Adjusted R-squared:  0.08631 \nF-statistic: 10.35 on 1 and 98 DF,  p-value: 0.001753\n\n\nThe output tells us that the model is significant, and that the beta values for the treatment_group therapy2 is different from the reference group (therapy1) which is the intercept of this model. The beta values are the difference in the outcome variable between the reference group and the other groups. The beta value for the reference group is the mean of the outcome variable (aggression_level) for that group. The beta values for therapy2 is the difference in the mean outcome variable for that group and the reference group.\n\n\n\n\n\n\nInterpreting the results of a regression model with a categorical predictor variable\n\n\n\nWhen we have a categorical predictor variable, the interpretation of the beta values changes. The beta values are the difference in the outcome variable between the reference category and the other categories. The reference category is the category that is not included in the model. The beta value for the reference category is the mean of the outcome variable for that category. The beta values for the other categories are the difference in the outcome variable between that category and the reference category.\nIf you want to see this done another way, if your predictor has 2 levels, you can conduct a t-test and compare the means of the two groups. The t-test will give you the same results as the regression model. The t-test is a simpler way to compare the means of two groups, but the regression model is more flexible and can handle more complex models (by adding more predictor variables).\n\n\n\n\n\n\n\n\nTesting assumptions with a categorical predictor variable\n\n\n\nWhen we have a categorical predictor variable, we need to be careful about how we interpret the assumptions of linear regression. The assumptions of linearity, normality and homoscedasticity are still relevant. However, because we have a categorical predictor, the plots will look different. We need to check the assumptions for each level of the categorical predictor variable. Some of the plots will change to reflect this.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simple Regression in R</span>"
    ]
  },
  {
    "objectID": "regression.html#comparing-multiple-levels-of-predictor-variables-estimated-marginal-means",
    "href": "regression.html#comparing-multiple-levels-of-predictor-variables-estimated-marginal-means",
    "title": "7  Simple Regression in R",
    "section": "9.5 Comparing multiple levels of predictor variables (estimated marginal means)",
    "text": "9.5 Comparing multiple levels of predictor variables (estimated marginal means)\nIf we have more than 2 levels of the predictor variable, we can compare each level to the reference level. We can also compare all levels to each other using the emmeans package. The emmeans package provides estimated marginal means for the levels of a predictor variable. We can use the emmeans() function to calculate the estimated marginal means for the levels of the predictor variable.\nFor this example, let’s use the mtcar dataset, which contains information about cars. We will conduct a regression analysis with the cyl variable as the predictor variable and the mpg variable as the outcome variable. The cyl variable has 3 levels (4, 6, and 8 cylinders). We will compare the mean mpg for each level of the cyl variable.\n\n# Load the mtcars dataset\n\ndata(mtcars)\n\n# Convert the cyl variable to a factor variable\n\nmtcars$cyl &lt;- as.factor(mtcars$cyl)\n\n# Fit a regression model with the cyl variable as the predictor variable\n\nregression_model3 &lt;- lm(mpg ~ cyl, data = mtcars)\n\n# Summary of the model\n\nsummary(regression_model3)\n\n# Compare the levels of the predictor variable with pairwise comparisons\n\nlibrary(emmeans)\n\n\n3\n\nWe use the emmeans() function from the emmeans package to calculate the estimated marginal means for the levels of the cyl variable. The pairwise ~ cyl argument specifies that we want to compare the levels of the cyl variable with pairwise comparisons.\n\n\n\n\nWelcome to emmeans.\nCaution: You lose important information if you filter this package's results.\nSee '? untidy'\n\n3emmeans(regression_model3, pairwise ~ cyl)\n\n\nCall:\nlm(formula = mpg ~ cyl, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2636 -1.8357  0.0286  1.3893  7.2364 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  26.6636     0.9718  27.437  &lt; 2e-16 ***\ncyl6         -6.9208     1.5583  -4.441 0.000119 ***\ncyl8        -11.5636     1.2986  -8.905 8.57e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.223 on 29 degrees of freedom\nMultiple R-squared:  0.7325,    Adjusted R-squared:  0.714 \nF-statistic:  39.7 on 2 and 29 DF,  p-value: 4.979e-09\n\n$emmeans\n cyl emmean    SE df lower.CL upper.CL\n 4     26.7 0.972 29     24.7     28.7\n 6     19.7 1.220 29     17.3     22.2\n 8     15.1 0.861 29     13.3     16.9\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast    estimate   SE df t.ratio p.value\n cyl4 - cyl6     6.92 1.56 29   4.441  0.0003\n cyl4 - cyl8    11.56 1.30 29   8.905  &lt;.0001\n cyl6 - cyl8     4.64 1.49 29   3.112  0.0112\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nThe output tells us that the mean mpg for the 4-cylinder cars is significantly different from the mean mpg for the 6-cylinder cars and the 8-cylinder cars. The mean mpg for the 6-cylinder cars is also significantly different from the mean mpg for the 8-cylinder cars. The estimated marginal means provide a way to compare the levels of the predictor variable and determine if there are significant differences between them.\n\n\n\n\n\n\nWhat are estimated marginal means?\n\n\n\nEstimated marginal means are the predicted means of the outcome variable for each level of the predictor variable. They are calculated by averaging the predicted values of the outcome variable for each level of the predictor variable, while holding all other variables constant. Estimated marginal means provide a way to compare the levels of the predictor variable and determine if there are significant differences between them.\n\n9.5.1 What is the difference between estimated marginal means and actual means - how should I report them?\nEstimated marginal means are the predicted means of the outcome variable for each level of the predictor variable. They are calculated by averaging the predicted values of the outcome variable for each level of the predictor variable, while holding all other variables constant. Actual means are the observed means of the outcome variable for each level of the predictor variable. The difference between estimated marginal means and actual means is that estimated marginal means are based on the regression model, while actual means are based on the observed data. This allows the estimated marginal means to take into account other variables in the model, for example.\nWhen reporting the results of a regression analysis, it is common to report the estimated marginal means rather than the actual means. This is because the estimated marginal means take into account the effects of other variables in the model, while the actual means do not. Reporting the estimated marginal means can give a more accurate representation of the relationship between the predictor variable and the outcome variable.\n\n\n9.5.2 What are Tukey corrected p-values?\nTukey corrected p-values are used to adjust for multiple comparisons in a pairwise comparison analysis. When conducting multiple comparisons between the levels of a predictor variable, there is an increased risk of making a Type I error (false positive) due to the number of comparisons being made. Tukey corrected p-values adjust for this increased risk. This means that the p-values are adjusted to account for the number of comparisons being made, reducing the likelihood of making a false positive conclusion.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simple Regression in R</span>"
    ]
  }
]
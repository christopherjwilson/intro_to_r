[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to R for Clinical Psychology",
    "section": "",
    "text": "Welcome\nThe purpose of this site/book is to introduce you to R and R Studio for use in Clinical Psychology Research. Before you begin, there are some important things to know:",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#tidyverse",
    "href": "index.html#tidyverse",
    "title": "Introduction to R for Clinical Psychology",
    "section": "The tidyverse",
    "text": "The tidyverse\n\n\n\n\n\n\nThis site/book uses the tidyverse set of packages\n\n\n\nThe tidyverse is a collection of R packages designed for to make data manipulation and visualization. easier in R. The tidyverse is a powerful set of tools for data analysis, and it is widely used in the R community. It is assumed that you will have the tidyverse installed and loaded for the examples in this site/book. If you do not have the tidyverse installed, you can install it by running the following code in the R console:\n\ninstall.packages(\"tidyverse\")\nYou only need to install the package on to your machine once. Once you have installed the tidyverse, you can load it by running the following code in the R console:\n\nlibrary(tidyverse)\nThe library() function is used to load packages in R. You will need to load the tidyverse package at the beginning of each R session in which you want to use it.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#textbooks-that-can-be-accessed-online",
    "href": "index.html#textbooks-that-can-be-accessed-online",
    "title": "Introduction to R for Clinical Psychology",
    "section": "Textbooks that can be accessed online",
    "text": "Textbooks that can be accessed online\ne-books can be accessed from the library website: https://www.tees.ac.uk/depts/lis/\n\nResearch Methods and Statistics\nCoolican, 2019. Research Methods and Statistics in Psychology. Taylor & Francis Group\nBarker, C., Pistrang, N., & Elliott, R. (2015). Research methods in clinical psychology: An introduction for students and practitioners (3rd ed.). Chichester, West Sussex: Wiley Blackwell.\nWeiner, I. B., Schinka, J. A., & Velicer, W. F. (2012). Handbook of psychology, research methods in psychology (2. Aufl. ed.). Somerset: Wiley.\n\n\nWorking with R and RStudio to do analysis\nNavarro, D. (2017) Learning statistics with R.\nPhillips N. D. (2018) YaRrr! The Pirate’s Guide to R\nHorton, Pruim and Kaplan (2015) A Student’s Guide to R\nMather, M. (2019) R for Academics\nWickham and Grolemund (2019). R For Data Science\nAllaire and Grolemund (2019). R Markdown: The Definitive Guide\nBasics of RStudio\nData Import\nData Transformation\nData Visualisation with GGPlot",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction to R and R Studio",
    "section": "",
    "text": "1.1 What are R and R Studio?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R and R Studio</span>"
    ]
  },
  {
    "objectID": "intro.html#what-are-r-and-r-studio",
    "href": "intro.html#what-are-r-and-r-studio",
    "title": "1  Introduction to R and R Studio",
    "section": "",
    "text": "At the end of this section, you will be able to:\n\n\n\n\nDownload and install R and R Studio\nUnderstand the basic layout of R Studio\nDescribe some of the differences between SPSS and R\n\n\n\n\n1.1.1 Downloading and installing R and R Studio\nTo get started with R Studio, you need to download and install two pieces of software:\n\nR: The base software that you will use to write and run code.\nR Studio: An integrated development environment (IDE) that makes it easier to write and run code in R.\n\n\n\nClick on these links to download:\n\nR project\nRStudio\n\n\n\n1.1.2 The R Studio layout\nWhen you open R Studio, you will see a screen that looks like this:\n\n\n\nR Studio IDE\n\n\nBriefly, the different panes in R Studio are:\n\nConsole: You can write and run code in this pane. However, it is best practice to write code in a script. You will see output from your code in the console.\nEnvironment/History: This pane shows you the objects that you have created in R, and the history of the commands that you have run.\nFiles/Plots/Packages/Help: These panes allow you to navigate your files, view plots, manage packages, and access help documentation.\n\nYou will learn more about these panes as you work through the course.\n\n\n\nR Studio IDE\n\n\n\n\n1.1.3 Differences between SPSS and R\nR is a statistical programming language, while SPSS is a point-and-click software package. This means that in R, you write code to perform tasks, while in SPSS, you click buttons and select options from menus.\nThis can take some getting used to, but there are many advantages to using R:\n\nReproducibility: You can save your code and rerun it at any time, ensuring that your analysis is reproducible.\nFlexibility: You can write code to perform any task you like, rather than being limited to the options available in a menu.\nCommunity: R has a large and active community of users who share code and help each other to solve problems.\n\nWith R, you won’t manipulate your source data files. Instead, you load the data into R and manipulate it in R. This means that you can always go back to your original data and start again if you need to.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R and R Studio</span>"
    ]
  },
  {
    "objectID": "intro.html#no-more-point-and-click---the-r-workflow.",
    "href": "intro.html#no-more-point-and-click---the-r-workflow.",
    "title": "1  Introduction to R and R Studio",
    "section": "1.2 No more “point and click”! - the R workflow.",
    "text": "1.2 No more “point and click”! - the R workflow.\n\n\n\n\n\n\nAt the end of this section, you will be able to:\n\n\n\n\nOpen a new script in R Studio\nWrite and run code in a script\nSave a script for later use\n\n\n\n\n1.2.1 Using scripts in R Studio\nWhen you work in R, you will write code in a script. This is a text file that contains the code that you want to run. You can write and run code in the console, but it is best practice to write code in a script. This allows you to save your code and run it again later IT also makes it easier to see what you have done.\nTo open a new script in R Studio, click on File &gt; New File &gt; R Script. This will open a new script in the top-left pane of R Studio.\nYou can write code in the script, and then run it by selecting the code that you want to run and clicking the Run button at the top of the script pane. You can also run code by pressing Ctrl + Enter on your keyboard.\nTo save your script, click on File &gt; Save As... and save the file with a .R extension.\n\n\n\n\n\n\n\n\nOrganising your work\n\n\n\nIt is good practice to keep your work organised by putting your scripts, data, and other files in a folder on your computer, for each project that you work on.\nRStudio also allows the creation of projects. You can create a new project in R Studio by clicking on File &gt; New Project.... This will create a new folder on your computer where you can save your scripts, data, and other files. If you save your script in the project folder, you can easily access it by opening the project in R Studio. If you use projects, be aware that R Studio will load the last project you worked on when you open the software.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R and R Studio</span>"
    ]
  },
  {
    "objectID": "intro.html#objects-functions-and-packages-in-r",
    "href": "intro.html#objects-functions-and-packages-in-r",
    "title": "1  Introduction to R and R Studio",
    "section": "1.3 Objects, functions and packages in R",
    "text": "1.3 Objects, functions and packages in R\n\n\n\n\n\n\nAt the end of this section, you will be able to:\n\n\n\n\nCreate objects in R\nUse functions in R to perform tasks\nInstall and load packages in R\n\n\n\n\n1.3.1 What are objects?\nIn R, you can create objects to store data. For example, you can create an object called numbers that contains a set of numbers like this:\nnumbers &lt;- c(1, 2, 3, 4, 5)\nBreaking this code down:\n\nnumbers is the name of the object that you are creating.\n&lt;- is the assignment operator. It assigns the value on the right-hand side of the operator to the object on the left-hand side.\nc(1, 2, 3, 4, 5) is the data that you are assigning to the object. In this case, it is a set of numbers.\n\nWhen you run this code, R will create an object called numbers that contains the numbers 1, 2, 3, 4, and 5. You will be ab` le to see the object in the Environment pane in R Studio.\nYou can then use the object in your code, instead of typing out the data each time (see Section 1.3.2 for example).\n\n\n\n\n1.3.2 What are functions?\nFunctions are code that have been written to perform a specific task. You can use functions in R to perform tasks like reading data into R, summarising data, and creating plots.\nFor example, the mean() function calculates the mean of a set of numbers. You can use the mean() function like this:\nnumbers &lt;- c(1, 2, 3, 4, 5)\n\nmean(numbers)\nFunctions in R have a name, followed by parentheses. You can pass arguments to the function inside the parentheses. In this case, the mean() function takes a set of numbers as an argument, and returns the mean of those numbers.\nTo learn more about a function, you can use the help() function. For example, to learn more about the mean() function, you can run the following code:\nhelp(mean)\nYou can also use the ? operator to get help on a function. For example, to get help on the mean() function, you can run the following code:\n\n?mean\n\n\n\n\n1.3.3 What are packages?\nR has many built-in functions that you can use to perform tasks. However, there are also many packages available that contain additional functions. You can install these packages onto your conputer and then load them into your R session whenever you want to use them.\nTo install a package, you can use the install.packages() function. For example, to install the tidyverse package, you would run the following code:\ninstall.packages(\"tidyverse\")\nTo load a package into your R session, you can use the library() function. For example, to load the tidyverse package, you would run the following code:\nlibrary(tidyverse)\nOnce you have loaded a package, you can use the functions in that package in your code. For example, the tidyverse package contains functions for data manipulation and visualisation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R and R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html",
    "href": "working_with_data.html",
    "title": "2  Working with data in R Studio",
    "section": "",
    "text": "2.1 Importing data into R\nThere are a few different ways to load data into R. You can load data from a file on your computer, from a URL, or from a package. You can load data in different file types, such as CSV, Excel, and SPSS files.\nUsing RStudio, you can load data by clicking on File &gt; Import Dataset. This will open a window where you can select the file that you want to load.\nHowever, you can also load data using code. For example, you can use the read_csv() function from the readr package to load a CSV file into R. You can use the readxl package to load an Excel file, and the haven package to load an SPSS file.\nLet’s break this code down:\nWhen you load data into R, it will be stored as a data frame. A data frame is a type of object in R that is used to store tabular data. It is similar to a spreadsheet in Excel, with rows and columns.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#importing-data-into-r",
    "href": "working_with_data.html#importing-data-into-r",
    "title": "2  Working with data in R Studio",
    "section": "",
    "text": "At the end of this section, you will be able to:\n\n\n\n\nLoad data into R from different file types\nUnderstand the structure of data in R\n\n\n\n\n\n\n\n# Load the readr package\n\nlibrary(readr)\n\n# Load a CSV file into R\n\ndata &lt;- read_csv(\"data.csv\")\n\n\nlibrary(readr) loads the readr package into your R session. This package contains the read_csv() function, which you can use to load a CSV file into R.\nread_csv(\"data.csv\") reads the CSV file called data.csv into R. The data will be stored in an object called data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#how-are-data-stored-in-r",
    "href": "working_with_data.html#how-are-data-stored-in-r",
    "title": "2  Working with data in R Studio",
    "section": "2.2 How are data stored in R?",
    "text": "2.2 How are data stored in R?\nIf you worked through the previous section, you should already have some idea how to load data into R. But how are data stored in R? In R, data are stored in objects. An object is a container that holds data. There are several types of objects in R, but the most common ones are:\n\nVectors (e.g., a sequence of numbers)\nMatrices (e.g., a table of rows and colummns, all of the same data type)\nData frames (e.g., a table of data where each column represents a variable and each row represents an observation)\nLists (e.g., a collection of objects)\n\nIn this section, we will focus on data frames, which are the most common way to store data in R. A data frame is a table of data where each column represents a variable and each row represents an observation. You can think of a data frame as having a structure similar to a spreadsheet.\n\n\n\nData frame with 3 variables/columns",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#how-do-we-use-data-frames-in-r",
    "href": "working_with_data.html#how-do-we-use-data-frames-in-r",
    "title": "2  Working with data in R Studio",
    "section": "2.3 How do we use data frames in R?",
    "text": "2.3 How do we use data frames in R?\nTo view the data in a data frame, you can simply type the name of the data frame in the console and press Enter. For example, if you have a data frame called my_data, you can view the data in the data frame by typing my_data in the console and pressing Enter.\n\n## load the tidyverse package\n\nlibrary(tidyverse)\n\n# Create a data frame\nmy_data &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\"),\n  age = c(25, 30, 35, 40, 45, 50),\n  height = c(160, 175, 180, 165, 170, 190),\n  car = c(\"Electric\", \"Petrol\", \"Electric\", \"Petrol\", \"Petrol\", \"Electric\")\n)\n\n# View or refer to the data in the data frame\n\nmy_data\n\n     name age height      car\n1   Alice  25    160 Electric\n2     Bob  30    175   Petrol\n3 Charlie  35    180 Electric\n4   David  40    165   Petrol\n5     Eve  45    170   Petrol\n6   Frank  50    190 Electric\n\n\nIn the code above, we created a data frame called my_data with four variables: name, age, height, and car. We then used the my_data object to view the data in the data frame.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#view-or-refer-to-a-specific-variable-in-a-data-frame",
    "href": "working_with_data.html#view-or-refer-to-a-specific-variable-in-a-data-frame",
    "title": "2  Working with data in R Studio",
    "section": "2.4 View or refer to a specific variable in a data frame",
    "text": "2.4 View or refer to a specific variable in a data frame\nTo view or refer to a specific variable in a data frame, you can use the $ operator. For example, if you want to view the age variable in the my_data data frame, you can type my_data$age in the console and press Enter.\n\n# View or refer to a specific variable in a data frame\n\nmy_data$age\n\n[1] 25 30 35 40 45 50",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#data-types-in-r",
    "href": "working_with_data.html#data-types-in-r",
    "title": "2  Working with data in R Studio",
    "section": "2.5 Data types in R",
    "text": "2.5 Data types in R\nIn R, each variable in a data frame has a data type. The most common data types in R are:\n\nNumeric: for continuous variables (e.g., age, height)\nFactor: for categorical variables\nLogical: for binary variables (TRUE or FALSE)\n\nYou can use the str() function to view the structure of a data frame, including the data types of each variable.\n\n# View the structure of a data frame\n\nstr(my_data)\n\n'data.frame':   6 obs. of  4 variables:\n $ name  : chr  \"Alice\" \"Bob\" \"Charlie\" \"David\" ...\n $ age   : num  25 30 35 40 45 50\n $ height: num  160 175 180 165 170 190\n $ car   : chr  \"Electric\" \"Petrol\" \"Electric\" \"Petrol\" ...\n\n\nIn the code above, we used the str() function to view the structure of the my_data data frame. The output shows the data types of each variable in the data frame.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#convert-data-types-in-r",
    "href": "working_with_data.html#convert-data-types-in-r",
    "title": "2  Working with data in R Studio",
    "section": "2.6 Convert data types in R",
    "text": "2.6 Convert data types in R\nYou can convert the data type of a variable in R using the as. functions. For example, you can convert a character variable to a factor variable using the as.factor() function.\n\n# Convert a character variable to a factor variable\n\nmy_data$name &lt;- as.factor(my_data$name)\n\nmy_data$car &lt;- as.factor(my_data$car)\n\nIn the code above, we converted the name variable in the my_data data frame from a character variable to a factor variable using the as.factor() function.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#subsetting-data-in-r",
    "href": "working_with_data.html#subsetting-data-in-r",
    "title": "2  Working with data in R Studio",
    "section": "2.7 Subsetting data in R",
    "text": "2.7 Subsetting data in R\n\n\n\n\n\n\nAt the end of this section, you will be able to:\n\n\n\n\nFilter data in R\nCreate subsets of data in R\n\n\n\nSubsetting data in R means selecting a subset of the data based on certain criteria. For example, you might want to select only the rows where a certain variable is greater than a certain value, or only the columns that contain certain variables.\nIf we use the my_data data frame from the previous section, we can subset the data to select only the rows where the age variable is greater than 30.\n\n# Filter the data frame to select only the rows where the age variable is greater than 30\n\n# this method uses the dplyr package, which is a part of the tidyverse. Be sure to load the tidyverse package if you haven't already.\n\nmy_data %&gt;% filter(age &gt; 30)\n\n     name age height      car\n1 Charlie  35    180 Electric\n2   David  40    165   Petrol\n3     Eve  45    170   Petrol\n4   Frank  50    190 Electric\n\n\nLet’s break this code down:\n\nmy_data is the data frame that we want to subset.\n%&gt;% is the pipe operator, which is used to pass the data frame to the next function. This allows us to link multiple steps together in a single line of code.\nfilter(age &gt; 30) is the function that filters the data frame to select only the rows where the age variable is greater than 30.\n\nThe output of this code will be a new data frame that contains only the rows where the age variable is greater than 30. However, this new data frame will not be saved anywhere, so if you want to save it, you need to assign it to a new object. Tp do this, you can use the assignment operator &lt;-.\n\n# Filter the data frame to select only the rows where the age variable is greater than 30 and save the result to a new data frame called new_data\n\nnew_data &lt;- my_data %&gt;% filter(age &gt; 30)\n\nIn this code, on the left side of the assignment operator &lt;-, we have new_data, which is the name of the new data frame that will contain only the filtered subset of the data (i.e., the values where the age variable is greater than 30). The difference between this code and the previous code is that we are now saving the result to a new data frame called new_data, instead of just printing it to the console.\nWe can also combine multiple conditions when subsetting data. For example, we can select only the rows where the age variable is greater than 25 and the height variable is greater than 175.\n\n# Filter the data frame to select only the rows where the age variable is greater than 25 and the height variable is greater than 175\n\nmy_data %&gt;% filter(age &gt; 25 & height &gt; 175)\n\n     name age height      car\n1 Charlie  35    180 Electric\n2   Frank  50    190 Electric\n\n\nThere are many other ways to subset data in R, depending on the criteria you want to use. For example, you can use the select() function to select specific columns, the arrange() function to sort the data, and the mutate() function to create new variables. We will cover some of these functions in later sections.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#grouping-and-summarising-data-in-r",
    "href": "working_with_data.html#grouping-and-summarising-data-in-r",
    "title": "2  Working with data in R Studio",
    "section": "2.8 Grouping and summarising data in R",
    "text": "2.8 Grouping and summarising data in R\n\n\n\n\n\n\nAt the end of this section, you will be able to:\n\n\n\n\nGroup data in R\nSummarise data in R\n\n\n\nGrouping and summarising data in R means grouping the data by one or more variables and then calculating summary statistics for each group. For example, you might want to calculate the mean age for each group of people based on their height.\nIf we use the my_data data frame from the previous section, we can group the data by the car variable and then calculate the mean age for each group.\n\n# Group the data frame by the car variable and calculate the mean age for each group\n\nmy_data %&gt;% group_by(car) %&gt;% \n  summarise(mean_age = mean(age)) %&gt;%\n  ungroup()\n\n# A tibble: 2 × 2\n  car      mean_age\n  &lt;fct&gt;       &lt;dbl&gt;\n1 Electric     36.7\n2 Petrol       38.3\n\n\nLet’s break this code down:\n\nmy_data is the data frame that we want to group and summarise.\n%&gt;% is the pipe operator, which is used to pass the data frame to the next function. This allows us to link multiple steps together in a single line of code.\ngroup_by(car) is the function that groups the data frame by the car variable.\nsummarise(mean_age = mean(age)) is the function that calculates the mean age for each group of cars. The mean_age variable is the name of the new variable that will contain the mean age for each group.\nungroup() is the function that removes the grouping from the data frame. This is optional, but it is good practice to ungroup the data frame after you have finished summarising it.\n\nThe output of this code will be a new data frame that contains the mean age for each group of cars. The car variable is the grouping variable, and the mean_age variable is the summary statistic that we calculated for each group.\nYou can also calculate other summary statistics, such as the median, standard deviation, minimum, and maximum, using the summarise() function. You can also calculate multiple summary statistics at the same time by specifying multiple variables inside the summarise() function. For example, you can calculate the mean and standard deviation of the age variable for each group of cars.\n\n# Group the data frame by the car variable and calculate the mean and standard deviation of the age variable for each group\n\nmy_data %&gt;% group_by(car) %&gt;% \n  summarise(mean_age = mean(age), sd_age = sd(age)) %&gt;%\n  ungroup()\n\n# A tibble: 2 × 3\n  car      mean_age sd_age\n  &lt;fct&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n1 Electric     36.7  12.6 \n2 Petrol       38.3   7.64\n\n\nIn this code, we calculated the mean and standard deviation of the age variable for each group of cars. The mean_age and sd_age variables are the names of the new variables that will contain the mean and standard deviation for each group.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html",
    "href": "descriptive_stats.html",
    "title": "3  Exploratory and descriptive analysis",
    "section": "",
    "text": "3.1 Mean, median, and mode\nFor this section we will use the album_sales dataset, which we have already loaded in some of the videos. Let’s start by loading the dataset and displaying the first few rows:\nlibrary(tidyverse)\n\n# Load the album_sales dataset. The location of the dataset will be different based on where you saved it on your computer.\n\nalbum_sales &lt;- read.csv(\"Datasets/album_sales.csv\")\n\n# Display the first few rows of the dataset\n\nhead(album_sales)\n\n   Adverts Sales Airplay Attract   Genre\n1   10.256   330      43      10 Country\n2  985.685   120      28       7     Pop\n3 1445.563   360      35       7  HipHop\n4 1188.193   270      33       7  HipHop\n5  574.513   220      44       5   Metal\n6  568.954   170      19       5 Country\nThe dataset contains 5 variables: Adverts, Sales, Airplay, Attract and Genre. We will focus on the Sales variable for this section.\n# Calculate the mean of the Sales variable\n\nmean_sales &lt;- mean(album_sales$Sales)\n\nmean_sales\n\n[1] 193.2\nLet’s break down the code above:\nNext, let’s calculate the median of the Sales variable:\n# Calculate the median of the Sales variable\n\nmedian_sales &lt;- median(album_sales$Sales)\n\nmedian_sales\n\n[1] 200\nThe code above calculates the median of the Sales variable in the album_sales dataset. The median sales value is stored in the median_sales variable.\nFinally, let’s calculate the mode of the Sales variable. Unfortunately, R does not have a built-in function to calculate the mode. However, we do this in the following way:\n# Calculate the mode of the Sales variable\n\nalbum_sales$Sales %&gt;%\n table() \n\n.\n 10  30  40  50  60  70  80  90 100 110 120 130 140 150 160 170 180 190 200 210 \n  1   1   3   1   5   6   3   4   8   5  10   3  11  12   5   4   8   8   7  13 \n220 230 240 250 260 270 280 290 300 310 320 330 340 360 \n  6  17   7  10   3   3   5   8   6   2   4   2   3   6\nWe can see from the output that the mode of the Sales variable is 210, since that value appears most frequently in the dataset (13 times).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory and descriptive analysis</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#mean-median-and-mode",
    "href": "descriptive_stats.html#mean-median-and-mode",
    "title": "3  Exploratory and descriptive analysis",
    "section": "",
    "text": "At the end of this section, you will be able to:\n\n\n\n\nCalculate the mean, median, and mode of a dataset\n\n\n\n\n\n\n\n\n\nWe used the mean() function to calculate the mean of the Sales variable in the album_sales dataset. To do this, we specified the dataset album_sales and the variable Sales using the $ operator.\nThe mean sales value is stored in the mean_sales variable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory and descriptive analysis</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#standard-deviation-and-variance",
    "href": "descriptive_stats.html#standard-deviation-and-variance",
    "title": "3  Exploratory and descriptive analysis",
    "section": "3.2 Standard deviation and variance",
    "text": "3.2 Standard deviation and variance\n\n\n\n\n\n\nAt the end of this section, you will be able to:\n\n\n\n\nCalculate the standard deviation\nCalculate the variance\nCalculate the range of a dataset\nCalculate the interquartile range (IQR)\n\n\n\nNext, let’s calculate the standard deviation and variance of the Sales variable in the album_sales dataset:\n\n# Calculate the standard deviation of the Sales variable\n\nsd_sales &lt;- sd(album_sales$Sales)\n\nsd_sales\n\n[1] 80.69896\n\n\nThe code above calculates the standard deviation of the Sales variable in the album_sales dataset. The standard deviation value is stored in the sd_sales variable.\nNext, let’s calculate the variance of the Sales variable:\n\n# Calculate the variance of the Sales variable\n\nvar_sales &lt;- var(album_sales$Sales)\n\nvar_sales\n\n[1] 6512.322\n\n\nThe code above calculates the variance of the Sales variable in the album_sales dataset. The variance value is stored in the var_sales variable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory and descriptive analysis</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#range-and-interquartile-range",
    "href": "descriptive_stats.html#range-and-interquartile-range",
    "title": "3  Exploratory and descriptive analysis",
    "section": "3.3 Range and interquartile range",
    "text": "3.3 Range and interquartile range\nThe range of a dataset is the difference between the maximum and minimum values. Let’s calculate the range of the Sales variable in the album_sales dataset:\n\n# Calculate the range of the Sales variable\n\nrange_sales &lt;- range(album_sales$Sales)\n\nrange_sales\n\n[1]  10 360\n\n\nThe code above calculates the range of the Sales variable in the album_sales dataset. The range of the sales values is stored in the range_sales variable.\nThe interquartile range (IQR) is the difference between the 75th percentile (Q3) and the 25th percentile (Q1) of a dataset. Let’s calculate the IQR of the Sales variable in the album_sales dataset:\n\n# Calculate the interquartile range of the Sales variable\n\nIQR_sales &lt;- IQR(album_sales$Sales)\n\nIQR_sales\n\n[1] 112.5\n\n\nThe code above calculates the interquartile range (IQR) of the Sales variable in the album_sales dataset. The IQR value is stored in the IQR_sales variable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory and descriptive analysis</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#distribution-plots",
    "href": "descriptive_stats.html#distribution-plots",
    "title": "3  Exploratory and descriptive analysis",
    "section": "3.4 Distribution plots",
    "text": "3.4 Distribution plots\n\n\n\n\n\n\nAt the end of this section, you will be able to:\n\n\n\n\nCreate a histogram to visualize the distribution of a dataset\nCreate a box plot to visualize the distribution of a dataset\n\n\n\nNext, let’s create a histogram to visualize the distribution of the Sales variable in the album_sales dataset:\n\n# Create a histogram of the Sales variable\n\nhist(album_sales$Sales, main = \"Histogram of Sales\", xlab = \"Sales\", ylab = \"Frequency\", col = \"lightblue\")\n\n\n\n\n\n\n\n\nThe code above creates a histogram of the Sales variable in the album_sales dataset. The histogram displays the frequency of sales values in the dataset. The only required argument for the hist() function is the variable you want to plot. The main, xlab, ylab, and col arguments are optional and allow you to customize the appearance of the histogram.\nFinally, let’s create a box plot to visualize the distribution of the Sales variable in the album_sales dataset:\n\n# Create a box plot of the Sales variable\n\nboxplot(album_sales$Sales, main = \"Boxplot of Sales\", xlab = \"Sales\", col = \"lightblue\")\n\n\n\n\n\n\n\n\nThe code above creates a box plot of the Sales variable in the album_sales dataset. The box plot displays the distribution of sales values, including the median, quartiles, and outliers. The only required argument for the boxplot() function is the variable you want to plot. The main, xlab, and col arguments are optional and allow you to customize the appearance of the box plot.\n\nWe will learn more about plotting data with ggplot2 in another section. However, for now, we have used the base R functions hist() and boxplot() to create simple distribution plots.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory and descriptive analysis</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#assessing-the-normality-of-data",
    "href": "descriptive_stats.html#assessing-the-normality-of-data",
    "title": "3  Exploratory and descriptive analysis",
    "section": "3.5 Assessing the normality of data",
    "text": "3.5 Assessing the normality of data\n\n\n\n\n\n\nAt the end of this section, you will be able to:\n\n\n\n\nAssess the skewness and kurtosis of a dataset\nTest the normality of a dataset using the Shapiro-Wilk test\n\n\n\nMany statistical tests assume that the data is normally distributed. When your data sample size is small, violation of the normaility assumption could be an issue. Let’s assess the normality of the Sales variable in the album_sales dataset by calculating the skewness and kurtosis. In order to do this, we will use the psych package, which provides functions for calculating skewness and kurtosis. If you haven’t installed the psych package yet, you can do so by running the following code:\n\n# Install the psych package if you haven't already\n\ninstall.packages(\"psych\")\nNow, let’s calculate the skewness and kurtosis of the Sales variable in the album_sales dataset:\n\n# Load the psych package\n\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\n# Calculate the skewness of the Sales variable\n\nskew_sales &lt;- skew(album_sales$Sales)\n\nskew_sales\n\n[1] 0.0432729\n\n# Calculate the kurtosis of the Sales variable\n\nkurt_sales &lt;- kurtosi(album_sales$Sales)\n\nkurt_sales\n\n[1] -0.7157339\n\n\nWhen interpreting the skewness and kurtosis values, remember that values of 0 indicate a normal distribution.\nWe can also test the normality of the Sales variable using the Shapiro-Wilk test. The null hypothesis of the Shapiro-Wilk test is that the data is normally distributed. Let’s perform the Shapiro-Wilk test on the Sales variable in the album_sales dataset:\n\n# Perform the Shapiro-Wilk test on the Sales variable\n\nshapiro.test(album_sales$Sales)\n\n\n    Shapiro-Wilk normality test\n\ndata:  album_sales$Sales\nW = 0.98479, p-value = 0.02965\n\n\nThe output of the Shapiro-Wilk test includes the test statistic and the p-value. If the p-value is less than 0.05, we reject the null hypothesis and conclude that the data is not normally distributed. If the p-value is greater than 0.05, we fail to reject the null hypothesis and conclude that the data is normally distributed.\n\n\n\n\n\n\n\n\nAssessing normality\n\n\n\nThe shapiro-wilk test is sensitive to sample size. For small sample sizes, the test may be too conservative and reject the null hypothesis too often. For large sample sizes, the test may be too lenient and fail to reject the null hypothesis too often. Therefore, you should not rely solely on the Shapiro-Wilk test to assess the normality of your data. Visual inspection of the data using histograms and Q-Q plots is also recommended. Also remember that the central limit theorem states that the sampling distribution of the mean will be approximately normally distributed for large sample sizes, regardless of the distribution of the original data.\nWe could also use non-parametric bootstrapping methods to deal with non-normal data. We will cover this in a later section.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory and descriptive analysis</span>"
    ]
  },
  {
    "objectID": "basic_tests.html",
    "href": "basic_tests.html",
    "title": "4  Basic statistical tests",
    "section": "",
    "text": "4.1 Independent t-test\nThe independent t-test is used to compare the means of two independent groups. In R, you can use the t.test() function to perform an independent t-test. Here is an example:\n# This example uses the mtcars dataset, which is a built-in dataset in R that contains data on various car models.\n\n# Load the mtcars dataset\n\ndata(mtcars)\n\n# Independent t-test example: Is there a difference in fuel efficiency between automatic and manual cars?\n# the variable am is a binary variable indicating the type of transmission (0 = automatic, 1 = manual)\n\n# Perform the independent t-test\n\nt_test_result &lt;- t.test(mpg ~ am, data = mtcars)\n\n# Print the result\n\nt_test_result\n\n\n    Welch Two Sample t-test\n\ndata:  mpg by am\nt = -3.7671, df = 18.332, p-value = 0.001374\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -11.280194  -3.209684\nsample estimates:\nmean in group 0 mean in group 1 \n       17.14737        24.39231\nIn this example, we are comparing the fuel efficiency (mpg) of automatic and manual cars in the mtcars dataset. The mpg variable is the dependent variable, and the am variable is the independent variable. The t.test() function is used to perform the independent t-test, and the result is stored in the t_test_result variable.\nIf we look at the output, we can see the following:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "power_effectSize.html",
    "href": "power_effectSize.html",
    "title": "6  Sampling, power and effect size",
    "section": "",
    "text": "6.1 Different measures of effect size\nThere are different ways to calculate effect size depending on the type of data and the statistical test used. Here are some common effect size measures:\nIn the following sections, we will calculate the effect size for different types of data using some of these measures.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling, power and effect size</span>"
    ]
  },
  {
    "objectID": "power_effectSize.html#different-measures-of-effect-size",
    "href": "power_effectSize.html#different-measures-of-effect-size",
    "title": "6  Sampling, power and effect size",
    "section": "",
    "text": "Cohen’s d: This is a measure of the difference between two means in standard deviation units. It is commonly used in t-tests and ANOVA tests.\nEta-squared (\\(\\eta^2\\)): This is a measure of the proportion of variance in the dependent variable that is explained by the independent variable. It is commonly used in ANOVA tests.\nPhi coefficient (\\(\\phi\\)): This is a measure of the association between two binary variables. It is commonly used in chi-square tests.\nCorrelation coefficient (\\(r\\)): This is a measure of the strength and direction of the relationship between two continuous variables. It is commonly used in correlation tests.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling, power and effect size</span>"
    ]
  },
  {
    "objectID": "power_effectSize.html#calculating-cohens-d",
    "href": "power_effectSize.html#calculating-cohens-d",
    "title": "6  Sampling, power and effect size",
    "section": "6.2 Calculating Cohen’s d",
    "text": "6.2 Calculating Cohen’s d\nCohen’s d is a measure of the difference between two means in standard deviation units. It is calculated as the difference between the means divided by the pooled standard deviation. The formula for Cohen’s d is:\n\\[ d = \\frac{{\\bar{X}_1 - \\bar{X}_2}}{{s_p}} \\]\nwhere:\n\n\\(\\bar{X}_1\\) and \\(\\bar{X}_2\\) are the means of the two groups.\n\\(s_p\\) is the pooled standard deviation, calculated as:\n\n\\[s_p = \\sqrt{\\frac{{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}}{{n_1 + n_2 - 2}}} \\]\nwhere:\n\n\\(n_1\\) and \\(n_2\\) are the sample sizes of the two groups.\n\\(s_1\\) and \\(s_2\\) are the standard deviations of the two groups.\n\nLet’s calculate Cohen’s d for a hypothetical dataset with two groups. The dataset contains the following information:\n\nGroup 1: Mean = 10, Standard deviation = 2, Sample size = 30\nGroup 2: Mean = 12, Standard deviation = 3, Sample size = 30\n\nTo calculate Cohen’s d, we first need to calculate the pooled standard deviation (\\(s_p\\)) using the formula above. Then, we can calculate Cohen’s d using the formula for Cohen’s d.\nLet’s calculate Cohen’s d for this dataset using R:\n\n# Calculate Cohen's d\n\n# Group 1\n\nmean1 &lt;- 10\n\nsd1 &lt;- 2\n\nn1 &lt;- 30\n\n# Group 2\n\nmean2 &lt;- 12\n\nsd2 &lt;- 3\n\nn2 &lt;- 30\n\n# Calculate pooled standard deviation\n\nsp &lt;- sqrt(((n1 - 1) * sd1^2 + (n2 - 1) * sd2^2) / (n1 + n2 - 2))\n\n# Calculate Cohen's d\n\nd &lt;- (mean1 - mean2) / sp\n\n\nd\n\n[1] -0.7844645\n\n\nThe calculated value of Cohen’s d is -0.7844645. This negative value indicates that the mean of Group 1 is smaller than the mean of Group 2 by approximately 0.78 standard deviations.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling, power and effect size</span>"
    ]
  },
  {
    "objectID": "power_effectSize.html#calculating-eta-squared-and-other-effect-size-measures",
    "href": "power_effectSize.html#calculating-eta-squared-and-other-effect-size-measures",
    "title": "6  Sampling, power and effect size",
    "section": "6.3 Calculating Eta-squared and other effect size measures",
    "text": "6.3 Calculating Eta-squared and other effect size measures\nIt is possible to calculate other effect size measures such as Eta-squared, Phi coefficient. However, these measures are most commonly calculated using the output of statistical tests such as ANOVA and chi-square tests etc. To obtain these measures for the purpose of sample size calculation, you would usually look at previous studies or meta analyses to determine the expected effect size. For clinical research, you may also use the minimal clinically important difference (MCID) as a guide to determine the effect size.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling, power and effect size</span>"
    ]
  },
  {
    "objectID": "power_effectSize.html#power-analysis",
    "href": "power_effectSize.html#power-analysis",
    "title": "6  Sampling, power and effect size",
    "section": "6.4 Power analysis",
    "text": "6.4 Power analysis\nPower analysis is a method used to determine the sample size required to detect an effect of a given size with a certain level of confidence. It is important to conduct a power analysis before conducting a study to ensure that the sample size is adequate to detect the effect of interest.\nTo conduct a power analysis, you need to specify the following parameters:\n\nThe effect size: The size of the effect you want to detect. This is usually determined based on previous studies, meta-analyses or MCID.\nThe significance level (\\(\\alpha\\)): The probability of rejecting the null hypothesis when it is true (Type 1 error rate). This is commonly set at 0.05.\nThe power (\\(1 - \\beta\\)): The probability of correctly rejecting the null hypothesis when it is false (1 - Type 2 error rate). This is commonly set at 0.80 or 0.90.\nThe number of groups or conditions: The number of groups or conditions in the study.\n\nThe sample size required to achieve a desired power level can be calculated using power analysis functions in R. There are many packages in r for power analysis. The package pwr is one such package that provides functions to calculate the sample size required for different types of statistical tests.\nThe functions for some basic research designs are:\n\npwr.t.test(): For t-tests\npwr.anova.test(): For ANOVA tests\npwr.chisq.test(): For chi-square tests\npwr.f2.test(): For regression models\n\nLet’s calculate the sample size required to achieve a power of 0.80 for a t-test with the example data we used earlier. We will use the pwr.t.test() function from the pwr package to calculate the sample size required to achieve a power of 0.80 for a t-test with the following parameters:\n\nEffect size (Cohen’s d) = -0.7844645\nSignificance level (\\(\\alpha\\)) = 0.05\n\n\n# Load the pwr package\n\n\nlibrary(pwr)\n\n# Calculate the sample size required for a t-test\n# using the d value calculated earlier\n\npwr.t.test(d = d, sig.level = 0.05, power = 0.80)\n\n\n     Two-sample t test power calculation \n\n              n = 26.50429\n              d = 0.7844645\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThe output of the pwr.t.test() function provides the sample size required to achieve a power of 0.80 for a t-test with the specified effect size and significance level. The output includes the following information:\n\nn = The sample size required for each group to achieve a power of 0.80.\nd = The effect size (Cohen’s d) used in the power analysis.\nsig.level = The significance level used in the power analysis.\npower = The power level achieved with the specified sample size.\n\nThe sample size required to achieve a power of 0.80 for a t-test with the specified effect size and significance level is 26.5 for each group. Since the sample size must be a whole number, we would need to round up to the nearest whole number. Therefore, the sample size required for each group is 27.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling, power and effect size</span>"
    ]
  },
  {
    "objectID": "power_effectSize.html#more-complex-power-analysis",
    "href": "power_effectSize.html#more-complex-power-analysis",
    "title": "6  Sampling, power and effect size",
    "section": "6.5 More complex power analysis",
    "text": "6.5 More complex power analysis\nFor more complex designs, different approaches to power analysis might be necessary, such as using simulation. This is possible to do in R, but is beyond the scope of this chapter.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sampling, power and effect size</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#independent-t-test",
    "href": "basic_tests.html#independent-t-test",
    "title": "4  Basic statistical tests",
    "section": "",
    "text": "The t-test result is presented.\nThe alternative hypothesis is that the means are not equal.\nThe 95% confidence interval for the difference in means is presented.\nThe 2 sample means are presented.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#paired-t-test",
    "href": "basic_tests.html#paired-t-test",
    "title": "4  Basic statistical tests",
    "section": "4.2 Paired t-test",
    "text": "4.2 Paired t-test\n\n\nThe paired t-test is used to compare the means of two related groups. In R, you can use the t.test() function with the paired = TRUE argument to perform a paired t-test. Here is an example:\n\n# This example uses the sleep dataset, which is a built-in dataset in R that contains data on the effect of two soporific drugs on sleep duration.\n\n# Load the sleep dataset\n\ndata(sleep)\n\n# Paired t-test example: Is there a difference in sleep duration between the two drugs?\n\n# Perform the paired t-test\n\npaired_t_test_result &lt;- t.test(sleep$extra ~ sleep$group, paired = TRUE)\n\n# Print the result\n\npaired_t_test_result\n\n\n    Paired t-test\n\ndata:  sleep$extra by sleep$group\nt = -4.0621, df = 9, p-value = 0.002833\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.4598858 -0.7001142\nsample estimates:\nmean difference \n          -1.58 \n\n\nIn this example, we are comparing the sleep duration (extra) between patients who were given different drugs in the sleep dataset (note: 10 people were each measured twice). The extra variable is the dependent variable, and the group variable is the independent variable. The t.test() function is used to perform the paired t-test, and the result is stored in the paired_t_test_result variable.\nIf we look at the output, we can see the following:\n\nThe t-test result is presented.\nThe alternative hypothesis is that the means are not equal.\nThe 95% confidence interval for the difference in means is presented.\nThe mean difference is presented.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#wilcoxon-signed-rank-test",
    "href": "basic_tests.html#wilcoxon-signed-rank-test",
    "title": "4  Basic statistical tests",
    "section": "4.3 Wilcoxon signed-rank test",
    "text": "4.3 Wilcoxon signed-rank test\nThe Wilcoxon signed-rank test is a non-parametric test used to compare two related groups. In R, you can use the wilcox.test() function to perform a Wilcoxon signed-rank test. Here is an example:\n\n# This example uses the sleep dataset, which is a built-in dataset in R that contains data on the effect of two soporific drugs on sleep duration.\n\n# Load the sleep dataset\n\ndata(sleep)\n\n# Wilcoxon signed-rank test example: Is there a difference in sleep duration between the two drugs?\n\n# Perform the Wilcoxon signed-rank test\n\nwilcoxon_test_result &lt;- wilcox.test(sleep$extra ~ sleep$group, paired = TRUE)\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with ties\n\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with zeroes\n\n# Print the result\n\nwilcoxon_test_result\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  sleep$extra by sleep$group\nV = 0, p-value = 0.009091\nalternative hypothesis: true location shift is not equal to 0\n\n\nIn this example, we are comparing the sleep duration (extra) between patients who were given different drugs in the sleep dataset. The extra variable is the dependent variable, and the group variable is the independent variable. The wilcox.test() function is used to perform the Wilcoxon signed-rank test, and the result is stored in the wilcoxon_test_result variable.\nIf we look at the output, we can see the following:\n\nThe Wilcoxon signed-rank test result is presented ( V is the sum of the ranks of the differences between the pairs of observations).\nThe alternative hypothesis (true location shift is not equal to 0) is presented. This means that the medians of the two groups are not equal.\n\nNote: if there are ties in the data, the exact p-value is not calculated. Instead, an approximate p-value is presented.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#mann-whitney-u-test",
    "href": "basic_tests.html#mann-whitney-u-test",
    "title": "4  Basic statistical tests",
    "section": "4.4 Mann-Whitney U test",
    "text": "4.4 Mann-Whitney U test\nThe Mann-Whitney U test is a non-parametric test used to compare the means of two independent groups. In R, you can use the wilcox.test() function to perform a Mann-Whitney U test. Note: we will not include the paired = TRUE argument that we did in the previous example. Here is an example:\n\n# This example uses the mtcars dataset, which is a built-in dataset in R that contains data on various car models.\n\n# Load the mtcars dataset\n\ndata(mtcars)\n\n# Mann-Whitney U test example: Is there a difference in fuel efficiency between automatic and manual cars?\n\n# Perform the Mann-Whitney U test\n\nmann_whitney_test_result &lt;- wilcox.test(mpg ~ am, data = mtcars)\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with ties\n\n# Print the result\n\nmann_whitney_test_result\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  mpg by am\nW = 42, p-value = 0.001871\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe output is interpreted in the same way as the Wilcoxon signed-rank test output.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#chi-squared-test",
    "href": "basic_tests.html#chi-squared-test",
    "title": "4  Basic statistical tests",
    "section": "4.5 Chi-squared test",
    "text": "4.5 Chi-squared test\n\n\nThe chi-squared test is used to test the association between two categorical variables. In R, you can use the chisq.test() function to perform a chi-squared test. Here is an example:\n\n# This example uses the mtcars dataset, which is a built-in dataset in R that contains data on various car models.\n\n# Load the mtcars dataset\n\ndata(mtcars)\n\n# Chi-squared test example: Is there an association between the number of cylinders and the type of transmission?\n\n# Create a contingency table\n\ncontingency_table &lt;- table(mtcars$cyl, mtcars$am)\n\n# Perform the chi-squared test\n\nchi_squared_test_result &lt;- chisq.test(contingency_table)\n\nWarning in chisq.test(contingency_table): Chi-squared approximation may be\nincorrect\n\n# Print the result\n\nchi_squared_test_result\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 8.7407, df = 2, p-value = 0.01265\n\n\nIn this example, we are testing the association between the number of cylinders (cyl) and the type of transmission (am) in the mtcars dataset. The chisq.test() function is used to perform the chi-squared test, and the result is stored in the chi_squared_test_result variable.\nIf we look at the output, we can see the following:\n\nThe chi-squared test result is presented.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#correlation",
    "href": "basic_tests.html#correlation",
    "title": "4  Basic statistical tests",
    "section": "4.6 Correlation",
    "text": "4.6 Correlation\nCorrelation is used to test the relationship between two continuous variables. In R, you can use the cor() function to calculate the correlation coefficient. Here is an example:\n\n# This example uses the mtcars dataset, which is a built-in dataset in R that contains data on various car models.\n\n# Load the mtcars dataset\n\ndata(mtcars)\n\n# Correlation example: Is there a relationship between fuel efficiency and horsepower?\n\n# Calculate the correlation coefficient\n\ncorrelation_result &lt;- cor.test(mtcars$mpg, mtcars$hp)\n\n# Print the result\n\ncorrelation_result\n\n\n    Pearson's product-moment correlation\n\ndata:  mtcars$mpg and mtcars$hp\nt = -6.7424, df = 30, p-value = 1.788e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.8852686 -0.5860994\nsample estimates:\n       cor \n-0.7761684 \n\n\nIn this example, we are testing the relationship between fuel efficiency (mpg) and horsepower (hp) in the mtcars dataset. The cor.test() function is used to calculate the correlation coefficient, and the result is stored in the correlation_result variable.\nIf we look at the output, we can see the following:\n\nThe correlation coefficient is presented in the last line of the output.\nThe significance level of the correlation is tested using a t-test, which is also presented in the output.\nThe confidence interval for the correlation coefficient is presented.\nThe alternative hypothesis is that the correlation is not equal to 0.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#one-way-anova",
    "href": "basic_tests.html#one-way-anova",
    "title": "4  Basic statistical tests",
    "section": "4.7 One-way ANOVA",
    "text": "4.7 One-way ANOVA\nOne-way ANOVA (Analysis of Variance) is used to test the differences between the means of three or more groups. In R, you can use the aov() function to perform an ANOVA. Here is an example:\n\n# This example uses the mtcars dataset, which is a built-in dataset in R that contains data on various car models.\n\n# Load the mtcars dataset\n\ndata(mtcars)\n\n# One-way ANOVA example: Is there a difference in fuel efficiency between cars with different numbers of cylinders?\n\n# Perform the ANOVA\n\nanova_result &lt;- aov(mpg ~ cyl, data = mtcars)\n\n# Print the result\n\nsummary(anova_result)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncyl          1  817.7   817.7   79.56 6.11e-10 ***\nResiduals   30  308.3    10.3                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this example, we are testing the differences in fuel efficiency (mpg) between cars with different numbers of cylinders (cyl) in the mtcars dataset. The aov() function is used to perform the ANOVA, and the result is stored in the anova_result variable.\nIf we look at the output, we can see the following:\n\nThe ANOVA result is within the summary() function. The summary includes the F-statistic, the p-value, and the significance level of the ANOVA test.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#factorial-anova",
    "href": "basic_tests.html#factorial-anova",
    "title": "4  Basic statistical tests",
    "section": "4.8 Factorial ANOVA",
    "text": "4.8 Factorial ANOVA\nFactorial ANOVA is used to test the effects of two or more independent variables on a dependent variable. In R, you can use the aov() function with interaction terms to perform a factorial ANOVA. Here is an example:\n\n# This example uses the mtcars dataset, which is a built-in dataset in R that contains data on various car models.\n\n# Load the mtcars dataset\n\ndata(mtcars)\n\n# Factorial ANOVA example: Is there an interaction effect between the number of cylinders and the type of transmission on fuel efficiency?\n\n# Perform the factorial ANOVA\n\nfactorial_anova_result &lt;- aov(mpg ~ cyl * am, data = mtcars)\n\n# Print the result\n\nsummary(factorial_anova_result)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncyl          1  817.7   817.7  94.642 1.76e-10 ***\nam           1   37.0    37.0   4.279   0.0479 *  \ncyl:am       1   29.4    29.4   3.407   0.0755 .  \nResiduals   28  241.9     8.6                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this example, we are testing the interaction effect between the number of cylinders (cyl) and the type of transmission (am) on fuel efficiency (mpg) in the mtcars dataset. The aov() function is used to perform the factorial ANOVA, and the result is stored in the factorial_anova_result variable.\nThe interaction term is specified using the * operator in the formula.\nIf we look at the output, we can see the following:\n\nThe ANOVA result is within the summary() function. The summary includes the F-statistic, the p-value, and the significance level of each term tested in the ANOVA.\nEach independent variable is tested separately (main effects), and the interaction effect is also tested.\nThe interaction effect is denoted by the cyl:am term in the output.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#conclusion",
    "href": "basic_tests.html#conclusion",
    "title": "4  Basic statistical tests",
    "section": "4.9 Conclusion",
    "text": "4.9 Conclusion\nIn this chapter, we have covered several basic statistical tests that you can perform in R. These are included for reference and to help you get started with performing statistical tests in R. However, we will be focusing on modelling our data, using different types of regression models, rather than re-learning these tests.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "5  Correlation and simple regression",
    "section": "",
    "text": "5.1 What is Correlation?\nCorrelation is a measure of the strength and direction of a relationship between two variables. It is most commonly used when we want to see if there is a relationship between two continuous variables. When we calculate correlation, we get a value between -1 and 1. The closer the value is to 1, the stronger the relationship. The closer the value is to 0, the weaker the relationship. We also often calculate the significance of the correlation, which tests against a null hypothesis that the correlation is 0.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#what-is-correlation",
    "href": "correlation.html#what-is-correlation",
    "title": "5  Correlation",
    "section": "",
    "text": "The relationship between 2 variables\nQuestion: Is treatment duration related to aggression levels?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#how-is-correlation-calculated",
    "href": "correlation.html#how-is-correlation-calculated",
    "title": "5  Correlation and simple regression",
    "section": "5.2 How is correlation calculated?",
    "text": "5.2 How is correlation calculated?\nCorrelation can be thought of as covariance divided by individual variance. Covariance is a measure of how much two variables change together. Variance is a measure of how much a variable changes on its own. When we divide covariance by variance, we get a value that is standardised and can be compared across different data sets.\nIf the changes are consistent with both variables (i.e. the covariance is higher and the individual variance is lower), then the final correlation value will be higher.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#running-correlation-in-r",
    "href": "correlation.html#running-correlation-in-r",
    "title": "5  Correlation",
    "section": "5.3 Running correlation in R",
    "text": "5.3 Running correlation in R\n\nStep 1: Check assumptions\n\nData,distribution,linearity\n\nStep 2: Run correlation\nStep 3: Check R value\nStep 4: Check significance\n\n\n5.3.1 Check assumptions: data\n\nParametric tests require interval or ratio data\nIf the data are ordinal then a non-parametric correlation is used\n\n\nWhat type of data are treatment duration and aggression level?\n\n\n\n5.3.2 Check assumptions: distribution\n\nParametric tests require normally distributed data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.3.3 Check assumptions: distribution #2\n\nParametric tests require normally distributed data\n\n\nshapiro.test(regression_data$treatment_duration)\n\n\n    Shapiro-Wilk normality test\n\ndata:  regression_data$treatment_duration\nW = 0.94971, p-value = 0.0007939\n\n\n\nshapiro.test(regression_data$aggression_level)\n\n\n    Shapiro-Wilk normality test\n\ndata:  regression_data$aggression_level\nW = 0.9928, p-value = 0.8756\n\n\n\nThe normality assumption is less of an issue when sample size is &gt; 30\n\n\n\n5.3.4 Checking assumptions: linearity\n\n\n\n\n\n\n\n\n\n\nregression_data %&gt;% ggplot(aes(x=treatment_duration,y=aggression_level)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nHere we are looking to see if the relationship is linear\n\n\n\n5.3.5 Run correlation\n\nR can run correlations using the cor.test() command\n\n\ncor.test(regression_data$treatment_duration,regression_data$aggression_level)\n\n\n    Pearson's product-moment correlation\n\ndata:  regression_data$treatment_duration and regression_data$aggression_level\nt = -9.5503, df = 98, p-value = 1.146e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7838251 -0.5765006\nsample estimates:\n       cor \n-0.6942996 \n\n\n\n\n5.3.6 Check r Value (correlation value)\n\nThe r value tells us the strength and direction of the relationship\nIn the output it is labelled as “cor” (short for correlation)\n\n\ncor.test(regression_data$treatment_duration,regression_data$aggression_level)\n\n\n    Pearson's product-moment correlation\n\ndata:  regression_data$treatment_duration and regression_data$aggression_level\nt = -9.5503, df = 98, p-value = 1.146e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7838251 -0.5765006\nsample estimates:\n       cor \n-0.6942996 \n\n\n\n\n5.3.7 Check the significance of the correlation\n\nWe can see that the significance by looking at the p value\n\nThe significance is 1.146^-15\nThis means: 0.0000000000000001146\n\nTherefore p value &lt; 0.05\n\n\ncor.test(regression_data$treatment_duration,regression_data$aggression_level)\n\n\n    Pearson's product-moment correlation\n\ndata:  regression_data$treatment_duration and regression_data$aggression_level\nt = -9.5503, df = 98, p-value = 1.146e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7838251 -0.5765006\nsample estimates:\n       cor \n-0.6942996",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#what-is-regression",
    "href": "correlation.html#what-is-regression",
    "title": "5  Correlation and simple regression",
    "section": "6.1 What is regression?",
    "text": "6.1 What is regression?\n\nTesting to see if we can make predictions based on data that are correlated\n\n\nWe found a strong correlation between treatment duration and agression levels. Can we use this data to predict aggression levels of other clients, based on their treatment duration?\n\n\nWhen we carry out regression, we get a information about:\n\nHow much variance in the outcome is explained by the predictor\nHow confident we can be about these results generalising (i.e. significance)\nHow much error we can expect from anu predictions that we make (i.e. standard error of the estimate)\nThe figures we need to calculate a predicted outcome value (i.e. coefficient values)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#how-is-regression-calculated",
    "href": "correlation.html#how-is-regression-calculated",
    "title": "5  Correlation and simple regression",
    "section": "6.2 How is regression calculated?",
    "text": "6.2 How is regression calculated?\n\n\n\n\n\n\n\n\n\n\nWhen we run a regression analysis, a calculation is done to select the “line of best fit”\nThis is a “prediction line” that minimises the overall amount of error\n\nError = difference between the data points and the line",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#the-regression-equation",
    "href": "correlation.html#the-regression-equation",
    "title": "5  Correlation and simple regression",
    "section": "6.3 The regression equation",
    "text": "6.3 The regression equation\n\n\n\n\n\n\n\n\n\n\nOnce the line of best fit is calculated, predictions are based on this line\nTo make predictions we need the intercept and slope of the line\n\nIntercept or constant= where the line crosses the y axis\nSlope or beta = the angle of the line\n\nPredictions are made using the calculation for a line: Y = bX + c\nYou can think of the equation like this:\n\npredicted outcome value = beta coefficient * value of predictor + constant",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#running-regression-in-r",
    "href": "correlation.html#running-regression-in-r",
    "title": "5  Correlation and simple regression",
    "section": "6.4 Running regression in R",
    "text": "6.4 Running regression in R\n\nStep 1: Run regression\nStep 2: Check assumptions\n\nData\nDistribution\nLinearity\nHomogeneity of variance\nUncorrelated predictors\nIndpendence of residuals\nNo influental cases / outliers\n\nStep 3: Check R^2 value\nStep 4: Check model significance\nStep 5: Check coefficient values",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#run-regression",
    "href": "correlation.html#run-regression",
    "title": "5  Correlation and simple regression",
    "section": "6.5 Run regression",
    "text": "6.5 Run regression\n\nWe use the lm() command to run regression while saving the results\nWe then use the summary() function to check the results\n\n\nmodel1 &lt;- lm(formula= aggression_level ~ treatment_duration ,data=regression_data)\nsummary(model1)\n\n\nCall:\nlm(formula = aggression_level ~ treatment_duration, data = regression_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4251 -1.1493 -0.0593  0.8814  3.4542 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         12.3300     0.7509   16.42  &lt; 2e-16 ***\ntreatment_duration  -0.6933     0.0726   -9.55 1.15e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.551 on 98 degrees of freedom\nMultiple R-squared:  0.4821,    Adjusted R-squared:  0.4768 \nF-statistic: 91.21 on 1 and 98 DF,  p-value: 1.146e-15",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#what-are-residuals",
    "href": "correlation.html#what-are-residuals",
    "title": "5  Correlation and simple regression",
    "section": "6.6 What are residuals?",
    "text": "6.6 What are residuals?\n\nIn regression, the assumptions apply to the residuals, not the data themselves\nResidual just means the difference between the data point and the regression line",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#check-assumptions-distribution-1",
    "href": "correlation.html#check-assumptions-distribution-1",
    "title": "5  Correlation",
    "section": "6.7 Check assumptions: distribution",
    "text": "6.7 Check assumptions: distribution\n\nUsing the plot() command on our regression model will give us some useful diagnostic plots\nThe second plot that it outputs shows the normality\n\n\nplot(model1, which=2)\n\n\n\n\n\n\n\n\n\nWe could also use a histogram to check the distribution\nNotice how we can use the $ sign to get the residuals from the model\n\n\nhist(model1$residuals)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#check-assumptions-linearity",
    "href": "correlation.html#check-assumptions-linearity",
    "title": "5  Correlation and simple regression",
    "section": "6.8 Check assumptions: linearity",
    "text": "6.8 Check assumptions: linearity\n\nUsing the plot() command on our regression model will give us some useful diagnostic plots\nThe first plot that it outputs shows the residuals vs the fitted values\nHere, we want to see them spread out, with the line being horizontal and straight\n\n\nplot(model1, which=1)\n\n\n\n\n\n\n\n\n\nThere is a slight amount of curvilinearity here but nothing to be worried about",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#check-assumptions-homogeneity-of-variance-1",
    "href": "correlation.html#check-assumptions-homogeneity-of-variance-1",
    "title": "5  Correlation and simple regression",
    "section": "6.9 Check assumptions: Homogeneity of Variance #1",
    "text": "6.9 Check assumptions: Homogeneity of Variance #1\n\nWe can use the sample plot to check Homogeneity of Variance\nWe want the variance to be constant across the data set. We do not want the variance to change at different points in the data\n\n\nplot(model1, which=1)\n\n\n\n\n\n\n\n\n\nA violation of Homogeneity of Variance would usually look like a funnel, with the data narrowing",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#check-assumptions-influential-cases",
    "href": "correlation.html#check-assumptions-influential-cases",
    "title": "5  Correlation and simple regression",
    "section": "6.10 Check assumptions: Influential cases",
    "text": "6.10 Check assumptions: Influential cases\n\nWe need to check that there are no extreme outliers - they could throw off our predictions\nWe are looking for participants that have high rediduals + high leverage\n\nSome guidance suggests anything higher than 1 is an influential case\nOthers suggest 4/n is the cut off point (4 divided by number of participants)\n\n\n\nplot(model1, which=4)\n\n\n\n\n\n\n\n\n\nWe are looking for participants that have high rediduals + high leverage\n\nNo cases over 1\nMany are over 0.04 (4/n = 0.04)\n\n\n\nplot(model1, which=5)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#check-the-r-squared-value",
    "href": "correlation.html#check-the-r-squared-value",
    "title": "5  Correlation and simple regression",
    "section": "6.11 Check the r squared value",
    "text": "6.11 Check the r squared value\n\nr^2 = the amount of variance in the outcome that is explained by the predictor(s)\nThe closer this value is to 1, the more useful our regression model is for predicting the outcome\n\n\nmodelSummary &lt;- summary(model1)\nmodelSummary\n\n\nCall:\nlm(formula = aggression_level ~ treatment_duration, data = regression_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4251 -1.1493 -0.0593  0.8814  3.4542 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         12.3300     0.7509   16.42  &lt; 2e-16 ***\ntreatment_duration  -0.6933     0.0726   -9.55 1.15e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.551 on 98 degrees of freedom\nMultiple R-squared:  0.4821,    Adjusted R-squared:  0.4768 \nF-statistic: 91.21 on 1 and 98 DF,  p-value: 1.146e-15\n\n\n\nThe r^2 of 0.482052 means that 48% of the variance in aggression level is explained by treatment duration",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#check-model-significance",
    "href": "correlation.html#check-model-significance",
    "title": "5  Correlation and simple regression",
    "section": "6.12 Check model significance",
    "text": "6.12 Check model significance\n\nThe model significance is displayed at the very end of the output\n\np-value: 1.146e-15\nAs p &lt; 0.05, the model is significant\n\n\n\nmodelSummary\n\n\nCall:\nlm(formula = aggression_level ~ treatment_duration, data = regression_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4251 -1.1493 -0.0593  0.8814  3.4542 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         12.3300     0.7509   16.42  &lt; 2e-16 ***\ntreatment_duration  -0.6933     0.0726   -9.55 1.15e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.551 on 98 degrees of freedom\nMultiple R-squared:  0.4821,    Adjusted R-squared:  0.4768 \nF-statistic: 91.21 on 1 and 98 DF,  p-value: 1.146e-15",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#check-coefficient-values",
    "href": "correlation.html#check-coefficient-values",
    "title": "5  Correlation and simple regression",
    "section": "6.13 Check coefficient values",
    "text": "6.13 Check coefficient values\n\nThe coefficient values are displayed in the coefficients table\nIf we have more than one predictor, they are all listed here\n\n\nmodelSummary$coefficients\n\n                     Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)        12.3300211 0.75087601 16.420848 6.840516e-30\ntreatment_duration -0.6933201 0.07259671 -9.550297 1.145898e-15\n\n\n\nThe beta coefficient for treatment duration is in the Estimate column\nFor every unit increase in treatment duration, aggression level decreases by 0.69",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#the-regression-equation-1",
    "href": "correlation.html#the-regression-equation-1",
    "title": "5  Correlation and simple regression",
    "section": "6.14 The regression equation",
    "text": "6.14 The regression equation\n\nThe regression equation is:\n\nOutcome = predictor value * beta coefficient + constant\n\nFor this model, that is:\n\nAggression level = treatment duration * -0.69 + 12.33\n\nmodelSummary$coefficients\n\n                     Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)        12.3300211 0.75087601 16.420848 6.840516e-30\ntreatment_duration -0.6933201 0.07259671 -9.550297 1.145898e-15",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#accounting-for-error-in-predictions",
    "href": "correlation.html#accounting-for-error-in-predictions",
    "title": "5  Correlation and simple regression",
    "section": "6.15 Accounting for error in predictions",
    "text": "6.15 Accounting for error in predictions\n\nWe also know that the accuracy of predictions will be within a certain margin of error\nThis is known as standard error of the estimate or residual standard error\n\n\nmodelSummary\n\n\nCall:\nlm(formula = aggression_level ~ treatment_duration, data = regression_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4251 -1.1493 -0.0593  0.8814  3.4542 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         12.3300     0.7509   16.42  &lt; 2e-16 ***\ntreatment_duration  -0.6933     0.0726   -9.55 1.15e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.551 on 98 degrees of freedom\nMultiple R-squared:  0.4821,    Adjusted R-squared:  0.4768 \nF-statistic: 91.21 on 1 and 98 DF,  p-value: 1.146e-15",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "intro.html#revision-quiz",
    "href": "intro.html#revision-quiz",
    "title": "1  Introduction to R and R Studio",
    "section": "Revision Quiz",
    "text": "Revision Quiz\n\nWhat is the purpose of R scripts in the context of data analysis? r mcq(c( \"To create interactive visualizations\", \"To organize files and folders on your computer\", answer =  \"To type and save analysis code for later use\", \"To display the output of R code immediately\"))\nWhat does the Environment in R Studio primarily display? r mcq(c( \"The R script files\", \"Publication-ready outputs\", answer = \"Objects (data, analysis, plots) currently in memory\",  \"The R package installation menu\"))\nWhich of the following is a reason to learn and use R? r mcq(c( \"It is a closed-source software\", \"It is used exclusively by programmers\", answer = \"It has a large number of packages and a wide community of users\",  \"It doesn't require any coding skills\"))\nHow can you install an R package in R Studio? r mcq(c( \"By clicking 'Load' in the Environment section\", \"By running the package's executable file\", answer = \"By using the 'install.packages()' function\", \"By copying the package files to the working directory\"))\nWhat does the ‘Run’ button do in R Studio? r mcq(c( \"It compiles the R script into a standalone program\", \"It generates publication-ready outputs\", answer = \"It runs the code in the R script\",  \"It opens the R package manager\"))\nHow can you load a previously installed R package? r mcq(c( \"By clicking the 'Load' button in the Console\", \"By typing 'install.packages()' in the Console\", answer =  \"By using the 'library(package_name)' command\", \"By copying the package files to the working directory\"))\nTrue or False: You can work on multiple datasets at the same time using R / R Studio? r torf(TRUE)\nWhat advantage does using R scripts for analysis provide? r mcq(c( \"They enable real-time collaboration with other users\", \"They allow you to create complex interactive visualizations\",answer =  \"They encourage transparency and reproducibility of analysis\", \"They automatically generate publication-ready reports\"))\nWhat is the main role of the R Console in R Studio? r mcq(c( \"To create interactive visualizations\", \"To dispay objects in memory\", \"To organize and manage files in the workspace\", answer = \"To provide an interactive environment to run R code\"))\n“What is the difference between R and R Studio? r mcq(c( answer = \"R is a statistical programming language, while R Studio is a software developed for using R to do analysis\", \"R is a data visualization tool, while R Studio is a statistical analysis software\", \"R is used for web development, while R Studio is used for machine learning\",  \"R is the name of the software, while R Studio is a community forum\"))”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R and R Studio</span>"
    ]
  },
  {
    "objectID": "correlation.html#which-correlation-to-use",
    "href": "correlation.html#which-correlation-to-use",
    "title": "5  Correlation and simple regression",
    "section": "5.3 Which correlation to use?",
    "text": "5.3 Which correlation to use?\nWhen we run correlation in R, we use the cor.test() command. This command will give us the correlation value, the p value and the confidence intervals.\nWe can specify a Pearson correlation (the default) or a Spearman correlation (for non-parametric data).\n\n5.3.1 Running correlation in R\n\nR can run correlations using the cor.test() command\n\n\ncor.test(regression_data$treatment_duration,regression_data$aggression_level) \n\n\n    Pearson's product-moment correlation\n\ndata:  regression_data$treatment_duration and regression_data$aggression_level\nt = -9.5503, df = 98, p-value = 1.146e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7838251 -0.5765006\nsample estimates:\n       cor \n-0.6942996 \n\n\nIn the above example, we are testing the correlation between treatment duration and aggression level. Each variable is separated by a comma.\n\n\n5.3.2 Interpreting the output\n\nThe r value tells us the strength and direction of the relationship\nIn the output it is labelled as “cor” (short for correlation)\n\nCorrelation values can range from -1 to 1. The closer the value is to 1, the stronger the relationship. The closer the value is to 0, the weaker the relationship. Positive values indicate a positive relationship (i.e. as one variable increases, so does the other). Negative values indicate a negative relationship (i.e. as one variable increases, the other decreases).\n\ncor.test(regression_data$treatment_duration,regression_data$aggression_level)\n\n\n    Pearson's product-moment correlation\n\ndata:  regression_data$treatment_duration and regression_data$aggression_level\nt = -9.5503, df = 98, p-value = 1.146e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7838251 -0.5765006\nsample estimates:\n       cor \n-0.6942996 \n\n\n\n\n5.3.3 Check the significance of the correlation\n\nWe can see that the significance by looking at the p value\n\nThe significance is 1.146^-15\nThis means: 0.0000000000000001146\n\nTherefore p value &lt; 0.05\n\n\ncor.test(regression_data$treatment_duration,regression_data$aggression_level)\n\n\n    Pearson's product-moment correlation\n\ndata:  regression_data$treatment_duration and regression_data$aggression_level\nt = -9.5503, df = 98, p-value = 1.146e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7838251 -0.5765006\nsample estimates:\n       cor \n-0.6942996",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#exponent-values",
    "href": "correlation.html#exponent-values",
    "title": "5  Correlation and simple regression",
    "section": "5.4 Exponent values",
    "text": "5.4 Exponent values\nWhen we see a value like 1.146e-15, this is a shorthand way of writing a very small number. The e-15 means that we move the decimal point 15 places to the left. So 1.146e-15 is the same as 0.000000000000001146",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  },
  {
    "objectID": "correlation.html#check-assumptions-distribution",
    "href": "correlation.html#check-assumptions-distribution",
    "title": "5  Correlation and simple regression",
    "section": "6.7 Check assumptions: distribution",
    "text": "6.7 Check assumptions: distribution\n\nUsing the plot() command on our regression model will give us some useful diagnostic plots\nThe second plot that it outputs shows the normality\n\n\nplot(model1, which=2)\n\n\n\n\n\n\n\n\n\nWe could also use a histogram to check the distribution\nNotice how we can use the $ sign to get the residuals from the model\n\n\nhist(model1$residuals)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and simple regression</span>"
    ]
  }
]
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to R for Clinical Psychology",
    "section": "",
    "text": "Welcome\nThe purpose of this site/book is to introduce you to R and R Studio for use in Clinical Psychology Research. Before you begin, there are some important things to know:",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#tidyverse",
    "href": "index.html#tidyverse",
    "title": "Introduction to R for Clinical Psychology",
    "section": "The tidyverse",
    "text": "The tidyverse\n\n\n\n\n\n\nImportantThis site/book uses the tidyverse set of packages\n\n\n\nThe tidyverse is a collection of R packages designed for to make data manipulation and visualization. easier in R. The tidyverse is a powerful set of tools for data analysis, and it is widely used in the R community. It is assumed that you will have the tidyverse installed and loaded for the examples in this site/book. If you do not have the tidyverse installed, you can install it by running the following code in the R console:\n\ninstall.packages(\"tidyverse\")\nYou only need to install the package on to your machine once. Once you have installed the tidyverse, you can load it by running the following code in the R console:\n\nlibrary(tidyverse)\nThe library() function is used to load packages in R. You will need to load the tidyverse package at the beginning of each R session in which you want to use it.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#textbooks-that-can-be-accessed-online",
    "href": "index.html#textbooks-that-can-be-accessed-online",
    "title": "Introduction to R for Clinical Psychology",
    "section": "Textbooks that can be accessed online",
    "text": "Textbooks that can be accessed online\ne-books can be accessed from the library website: https://www.tees.ac.uk/depts/lis/\n\nResearch Methods and Statistics\nCoolican, 2019. Research Methods and Statistics in Psychology. Taylor & Francis Group\nBarker, C., Pistrang, N., & Elliott, R. (2015). Research methods in clinical psychology: An introduction for students and practitioners (3rd ed.). Chichester, West Sussex: Wiley Blackwell.\nWeiner, I. B., Schinka, J. A., & Velicer, W. F. (2012). Handbook of psychology, research methods in psychology (2. Aufl. ed.). Somerset: Wiley.\n\n\nWorking with R and RStudio to do analysis\nNavarro, D. (2017) Learning statistics with R.\nPhillips N. D. (2018) YaRrr! The Pirate’s Guide to R\nHorton, Pruim and Kaplan (2015) A Student’s Guide to R\nMather, M. (2019) R for Academics\nWickham and Grolemund (2019). R For Data Science\nAllaire and Grolemund (2019). R Markdown: The Definitive Guide\nBasics of RStudio\nData Import\nData Transformation\nData Visualisation with GGPlot",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction to R and R Studio",
    "section": "",
    "text": "1.1 What are R and R Studio?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R and R Studio</span>"
    ]
  },
  {
    "objectID": "intro.html#what-are-r-and-r-studio",
    "href": "intro.html#what-are-r-and-r-studio",
    "title": "1  Introduction to R and R Studio",
    "section": "",
    "text": "TipAt the end of this section, you will be able to:\n\n\n\n\nDownload and install R and R Studio\nUnderstand the basic layout of R Studio\nDescribe some of the differences between SPSS and R\n\n\n\n\n1.1.1 Downloading and installing R and R Studio\nTo get started with R Studio, you need to download and install two pieces of software:\n\nR: The base software that you will use to write and run code.\nR Studio: An integrated development environment (IDE) that makes it easier to write and run code in R.\n\n\n\nClick on these links to download:\n\nR project\nRStudio\n\n\n\n1.1.2 The R Studio layout\nWhen you open R Studio, you will see a screen that looks like this:\n\n\n\nR Studio IDE\n\n\nBriefly, the different panes in R Studio are:\n\nConsole: You can write and run code in this pane. However, it is best practice to write code in a script. You will see output from your code in the console.\nEnvironment/History: This pane shows you the objects that you have created in R, and the history of the commands that you have run.\nFiles/Plots/Packages/Help: These panes allow you to navigate your files, view plots, manage packages, and access help documentation.\n\nYou will learn more about these panes as you work through the course.\n\n\n\nR Studio IDE\n\n\n\n\n1.1.3 Differences between SPSS and R\nR is a statistical programming language, while SPSS is a point-and-click software package. This means that in R, you write code to perform tasks, while in SPSS, you click buttons and select options from menus.\nThis can take some getting used to, but there are many advantages to using R:\n\nReproducibility: You can save your code and rerun it at any time, ensuring that your analysis is reproducible.\nFlexibility: You can write code to perform any task you like, rather than being limited to the options available in a menu.\nCommunity: R has a large and active community of users who share code and help each other to solve problems.\n\nWith R, you won’t manipulate your source data files. Instead, you load the data into R and manipulate it in R. This means that you can always go back to your original data and start again if you need to.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R and R Studio</span>"
    ]
  },
  {
    "objectID": "intro.html#no-more-point-and-click---the-r-workflow.",
    "href": "intro.html#no-more-point-and-click---the-r-workflow.",
    "title": "1  Introduction to R and R Studio",
    "section": "1.2 No more “point and click”! - the R workflow.",
    "text": "1.2 No more “point and click”! - the R workflow.\n\n\n\n\n\n\nTipAt the end of this section, you will be able to:\n\n\n\n\nOpen a new script in R Studio\nWrite and run code in a script\nSave a script for later use\n\n\n\n\n1.2.1 Using scripts in R Studio\nWhen you work in R, you will write code in a script. This is a text file that contains the code that you want to run. You can write and run code in the console, but it is best practice to write code in a script. This allows you to save your code and run it again later IT also makes it easier to see what you have done.\nTo open a new script in R Studio, click on File &gt; New File &gt; R Script. This will open a new script in the top-left pane of R Studio.\nYou can write code in the script, and then run it by selecting the code that you want to run and clicking the Run button at the top of the script pane. You can also run code by pressing Ctrl + Enter on your keyboard.\nTo save your script, click on File &gt; Save As... and save the file with a .R extension.\n\n\n\n\n\n\n\n\nImportantOrganising your work\n\n\n\nIt is good practice to keep your work organised by putting your scripts, data, and other files in a folder on your computer, for each project that you work on.\nRStudio also allows the creation of projects. You can create a new project in R Studio by clicking on File &gt; New Project.... This will create a new folder on your computer where you can save your scripts, data, and other files. If you save your script in the project folder, you can easily access it by opening the project in R Studio. If you use projects, be aware that R Studio will load the last project you worked on when you open the software.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R and R Studio</span>"
    ]
  },
  {
    "objectID": "intro.html#objects-functions-and-packages-in-r",
    "href": "intro.html#objects-functions-and-packages-in-r",
    "title": "1  Introduction to R and R Studio",
    "section": "1.3 Objects, functions and packages in R",
    "text": "1.3 Objects, functions and packages in R\n\n\n\n\n\n\nTipAt the end of this section, you will be able to:\n\n\n\n\nCreate objects in R\nUse functions in R to perform tasks\nInstall and load packages in R\n\n\n\n\n1.3.1 What are objects?\nIn R, you can create objects to store data. For example, you can create an object called numbers that contains a set of numbers like this:\nnumbers &lt;- c(1, 2, 3, 4, 5)\nBreaking this code down:\n\nnumbers is the name of the object that you are creating.\n&lt;- is the assignment operator. It assigns the value on the right-hand side of the operator to the object on the left-hand side.\nc(1, 2, 3, 4, 5) is the data that you are assigning to the object. In this case, it is a set of numbers.\n\nWhen you run this code, R will create an object called numbers that contains the numbers 1, 2, 3, 4, and 5. You will be ab` le to see the object in the Environment pane in R Studio.\nYou can then use the object in your code, instead of typing out the data each time (see Section 1.3.2 for example).\n\n\n\n\n1.3.2 What are functions?\nFunctions are code that have been written to perform a specific task. You can use functions in R to perform tasks like reading data into R, summarising data, and creating plots.\nFor example, the mean() function calculates the mean of a set of numbers. You can use the mean() function like this:\nnumbers &lt;- c(1, 2, 3, 4, 5)\n\nmean(numbers)\nFunctions in R have a name, followed by parentheses. You can pass arguments to the function inside the parentheses. In this case, the mean() function takes a set of numbers as an argument, and returns the mean of those numbers.\nTo learn more about a function, you can use the help() function. For example, to learn more about the mean() function, you can run the following code:\nhelp(mean)\nYou can also use the ? operator to get help on a function. For example, to get help on the mean() function, you can run the following code:\n\n?mean\n\n\n\n\n1.3.3 What are packages?\nR has many built-in functions that you can use to perform tasks. However, there are also many packages available that contain additional functions. You can install these packages onto your conputer and then load them into your R session whenever you want to use them.\nTo install a package, you can use the install.packages() function. For example, to install the tidyverse package, you would run the following code:\ninstall.packages(\"tidyverse\")\nTo load a package into your R session, you can use the library() function. For example, to load the tidyverse package, you would run the following code:\nlibrary(tidyverse)\nOnce you have loaded a package, you can use the functions in that package in your code. For example, the tidyverse package contains functions for data manipulation and visualisation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R and R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html",
    "href": "working_with_data.html",
    "title": "2  Working with data in R Studio",
    "section": "",
    "text": "2.1 Importing data into R\nWorking with your data in RStudio is a little bit different from working with data in a spreadsheet, for example. One crucial difference is that you need to be explicit about what you want to do with your data. In a spreadsheet, you can simply click on a cell and start typing. In R, you need to tell the software what you want to do with your data. This can be a bit intimidating at first, but it is also one of the most powerful features of R. It allows you to automate repetitive tasks and perform complex analyses with just a few lines of code.\nThere are a few different ways to load data into R. You can load data from a file on your computer, from a URL, or from a package. You can load data in different file types, such as CSV, Excel, and SPSS files.\nUsing RStudio, you can load data by clicking on File &gt; Import Dataset. This will open a window where you can select the file that you want to load.\nHowever, you can also load data using code. For example, you can use the read_csv() function from the readr package to load a CSV file into R. You can use the readxl package to load an Excel file, and the haven package to load an SPSS file.\nLet’s break this code down:\nWhen you load data into R, it will be stored as a data frame. A data frame is a type of object in R that is used to store tabular data. It is similar to a spreadsheet in Excel, with rows and columns.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#importing-data-into-r",
    "href": "working_with_data.html#importing-data-into-r",
    "title": "2  Working with data in R Studio",
    "section": "",
    "text": "TipAt the end of this section, you will be able to:\n\n\n\n\nLoad data into R from different file types\nUnderstand the structure of data in R\n\n\n\n\n\n\n\n# Load the readr package\n\nlibrary(readr)\n\n# Load a CSV file into R\n\ndata &lt;- read_csv(\"data.csv\")\n\n\nlibrary(readr) loads the readr package into your R session. This package contains the read_csv() function, which you can use to load a CSV file into R.\nread_csv(\"data.csv\") reads the CSV file called data.csv into R. The data will be stored in an object called data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#how-are-data-stored-in-r",
    "href": "working_with_data.html#how-are-data-stored-in-r",
    "title": "2  Working with data in R Studio",
    "section": "2.2 How are data stored in R?",
    "text": "2.2 How are data stored in R?\nIf you worked through the previous section, you should already have some idea how to load data into R. But how are data stored in R? In R, data are stored in objects. An object is a container that holds data. There are several types of objects in R, but the most common ones are:\n\nVectors (e.g., a sequence of numbers)\nMatrices (e.g., a table of rows and colummns, all of the same data type)\nData frames (e.g., a table of data where each column represents a variable and each row represents an observation)\nLists (e.g., a collection of objects)\n\nIn this section, we will focus on data frames, which are the most common way to store data in R. A data frame is a table of data where each column represents a variable and each row represents an observation. You can think of a data frame as having a structure similar to a spreadsheet.\n\n\n\nData frame with 3 variables/columns",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#how-do-we-use-data-frames-in-r",
    "href": "working_with_data.html#how-do-we-use-data-frames-in-r",
    "title": "2  Working with data in R Studio",
    "section": "2.3 How do we use data frames in R?",
    "text": "2.3 How do we use data frames in R?\nTo view the data in a data frame, you can simply type the name of the data frame in the console and press Enter. For example, if you have a data frame called my_data, you can view the data in the data frame by typing my_data in the console and pressing Enter.\n\n## load the tidyverse package\n\nlibrary(tidyverse)\n\n# Create a data frame\nmy_data &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\"),\n  age = c(25, 30, 35, 40, 45, 50),\n  height = c(160, 175, 180, 165, 170, 190),\n  car = c(\"Electric\", \"Petrol\", \"Electric\", \"Petrol\", \"Petrol\", \"Electric\")\n)\n\n# View or refer to the data in the data frame\n\nmy_data\n\n     name age height      car\n1   Alice  25    160 Electric\n2     Bob  30    175   Petrol\n3 Charlie  35    180 Electric\n4   David  40    165   Petrol\n5     Eve  45    170   Petrol\n6   Frank  50    190 Electric\n\n\nIn the code above, we created a data frame called my_data with four variables: name, age, height, and car. We then used the my_data object to view the data in the data frame.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#view-or-refer-to-a-specific-variable-in-a-data-frame",
    "href": "working_with_data.html#view-or-refer-to-a-specific-variable-in-a-data-frame",
    "title": "2  Working with data in R Studio",
    "section": "2.4 View or refer to a specific variable in a data frame",
    "text": "2.4 View or refer to a specific variable in a data frame\nTo view or refer to a specific variable in a data frame, you can use the $ operator. For example, if you want to view the age variable in the my_data data frame, you can type my_data$age in the console and press Enter.\n\n# View or refer to a specific variable in a data frame\n\nmy_data$age\n\n[1] 25 30 35 40 45 50",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#data-types-in-r",
    "href": "working_with_data.html#data-types-in-r",
    "title": "2  Working with data in R Studio",
    "section": "2.5 Data types in R",
    "text": "2.5 Data types in R\nIn R, each variable in a data frame has a data type. The most common data types in R are:\n\nNumeric: for continuous variables (e.g., age, height)\nFactor: for categorical variables\nLogical: for binary variables (TRUE or FALSE)\n\nYou can use the str() function to view the structure of a data frame, including the data types of each variable.\n\n# View the structure of a data frame\n\nstr(my_data)\n\n'data.frame':   6 obs. of  4 variables:\n $ name  : chr  \"Alice\" \"Bob\" \"Charlie\" \"David\" ...\n $ age   : num  25 30 35 40 45 50\n $ height: num  160 175 180 165 170 190\n $ car   : chr  \"Electric\" \"Petrol\" \"Electric\" \"Petrol\" ...\n\n\nIn the code above, we used the str() function to view the structure of the my_data data frame. The output shows the data types of each variable in the data frame.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#convert-data-types-in-r",
    "href": "working_with_data.html#convert-data-types-in-r",
    "title": "2  Working with data in R Studio",
    "section": "2.6 Convert data types in R",
    "text": "2.6 Convert data types in R\nYou can convert the data type of a variable in R using the as. functions. For example, you can convert a character variable to a factor variable using the as.factor() function.\n\n# Convert a character variable to a factor variable\n\nmy_data$name &lt;- as.factor(my_data$name)\n\nmy_data$car &lt;- as.factor(my_data$car)\n\nIn the code above, we converted the name variable in the my_data data frame from a character variable to a factor variable using the as.factor() function.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#subsetting-data-in-r",
    "href": "working_with_data.html#subsetting-data-in-r",
    "title": "2  Working with data in R Studio",
    "section": "2.7 Subsetting data in R",
    "text": "2.7 Subsetting data in R\n\n\n\n\n\n\nTipAt the end of this section, you will be able to:\n\n\n\n\nFilter data in R\nCreate subsets of data in R\n\n\n\nSubsetting data in R means selecting a subset of the data based on certain criteria. For example, you might want to select only the rows where a certain variable is greater than a certain value, or only the columns that contain certain variables.\nIf we use the my_data data frame from the previous section, we can subset the data to select only the rows where the age variable is greater than 30.\n\n# Filter the data frame to select only the rows where the age variable is greater than 30\n\n# this method uses the dplyr package, which is a part of the tidyverse. Be sure to load the tidyverse package if you haven't already.\n\nmy_data %&gt;% filter(age &gt; 30)\n\n     name age height      car\n1 Charlie  35    180 Electric\n2   David  40    165   Petrol\n3     Eve  45    170   Petrol\n4   Frank  50    190 Electric\n\n\nLet’s break this code down:\n\nmy_data is the data frame that we want to subset.\n%&gt;% is the pipe operator, which is used to pass the data frame to the next function. This allows us to link multiple steps together in a single line of code.\nfilter(age &gt; 30) is the function that filters the data frame to select only the rows where the age variable is greater than 30.\n\nThe output of this code will be a new data frame that contains only the rows where the age variable is greater than 30. However, this new data frame will not be saved anywhere, so if you want to save it, you need to assign it to a new object. Tp do this, you can use the assignment operator &lt;-.\n\n# Filter the data frame to select only the rows where the age variable is greater than 30 and save the result to a new data frame called new_data\n\nnew_data &lt;- my_data %&gt;% filter(age &gt; 30)\n\nIn this code, on the left side of the assignment operator &lt;-, we have new_data, which is the name of the new data frame that will contain only the filtered subset of the data (i.e., the values where the age variable is greater than 30). The difference between this code and the previous code is that we are now saving the result to a new data frame called new_data, instead of just printing it to the console.\nWe can also combine multiple conditions when subsetting data. For example, we can select only the rows where the age variable is greater than 25 and the height variable is greater than 175.\n\n# Filter the data frame to select only the rows where the age variable is greater than 25 and the height variable is greater than 175\n\nmy_data %&gt;% filter(age &gt; 25 & height &gt; 175)\n\n     name age height      car\n1 Charlie  35    180 Electric\n2   Frank  50    190 Electric\n\n\nThere are many other ways to subset data in R, depending on the criteria you want to use. For example, you can use the select() function to select specific columns, the arrange() function to sort the data, and the mutate() function to create new variables. We will cover some of these functions in later sections.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "working_with_data.html#grouping-and-summarising-data-in-r",
    "href": "working_with_data.html#grouping-and-summarising-data-in-r",
    "title": "2  Working with data in R Studio",
    "section": "2.8 Grouping and summarising data in R",
    "text": "2.8 Grouping and summarising data in R\n\n\n\n\n\n\nTipAt the end of this section, you will be able to:\n\n\n\n\nGroup data in R\nSummarise data in R\n\n\n\nGrouping and summarising data in R means grouping the data by one or more variables and then calculating summary statistics for each group. For example, you might want to calculate the mean age for each group of people based on their height.\nIf we use the my_data data frame from the previous section, we can group the data by the car variable and then calculate the mean age for each group.\n\n# Group the data frame by the car variable and calculate the mean age for each group\n\nmy_data %&gt;% group_by(car) %&gt;% \n  summarise(mean_age = mean(age)) %&gt;%\n  ungroup()\n\n# A tibble: 2 × 2\n  car      mean_age\n  &lt;fct&gt;       &lt;dbl&gt;\n1 Electric     36.7\n2 Petrol       38.3\n\n\nLet’s break this code down:\n\nmy_data is the data frame that we want to group and summarise.\n%&gt;% is the pipe operator, which is used to pass the data frame to the next function. This allows us to link multiple steps together in a single line of code.\ngroup_by(car) is the function that groups the data frame by the car variable.\nsummarise(mean_age = mean(age)) is the function that calculates the mean age for each group of cars. The mean_age variable is the name of the new variable that will contain the mean age for each group.\nungroup() is the function that removes the grouping from the data frame. This is optional, but it is good practice to ungroup the data frame after you have finished summarising it.\n\nThe output of this code will be a new data frame that contains the mean age for each group of cars. The car variable is the grouping variable, and the mean_age variable is the summary statistic that we calculated for each group.\nYou can also calculate other summary statistics, such as the median, standard deviation, minimum, and maximum, using the summarise() function. You can also calculate multiple summary statistics at the same time by specifying multiple variables inside the summarise() function. For example, you can calculate the mean and standard deviation of the age variable for each group of cars.\n\n# Group the data frame by the car variable and calculate the mean and standard deviation of the age variable for each group\n\nmy_data %&gt;% group_by(car) %&gt;% \n  summarise(mean_age = mean(age), sd_age = sd(age)) %&gt;%\n  ungroup()\n\n# A tibble: 2 × 3\n  car      mean_age sd_age\n  &lt;fct&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n1 Electric     36.7  12.6 \n2 Petrol       38.3   7.64\n\n\nIn this code, we calculated the mean and standard deviation of the age variable for each group of cars. The mean_age and sd_age variables are the names of the new variables that will contain the mean and standard deviation for each group.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with data in R Studio</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html",
    "href": "descriptive_stats.html",
    "title": "3  Exploratory and descriptive analysis",
    "section": "",
    "text": "3.1 Mean, median, and mode\nIn this section, we will cover the basics of exploratory and descriptive analysis in R. We will learn how to conduct some of the most common descriptive statistics, such as mean, median, mode, standard deviation, and variance. We will also look at basic distribution plots, such as histograms and box plots, to visualize the data.\nFor this section we will use the album_sales dataset, which we have already loaded in some of the videos. Let’s start by loading the dataset and displaying the first few rows:\nlibrary(tidyverse)\n\n# Load the album_sales dataset. The location of the dataset will be different based on where you saved it on your computer.\n\nalbum_sales &lt;- read.csv(\"Datasets/album_sales.csv\")\n\n# Display the first few rows of the dataset\n\nhead(album_sales)\n\n   Adverts Sales Airplay Attract   Genre\n1   10.256   330      43      10 Country\n2  985.685   120      28       7     Pop\n3 1445.563   360      35       7  HipHop\n4 1188.193   270      33       7  HipHop\n5  574.513   220      44       5   Metal\n6  568.954   170      19       5 Country\nThe dataset contains 5 variables: Adverts, Sales, Airplay, Attract and Genre. We will focus on the Sales variable for this section.\n# Calculate the mean of the Sales variable\n\nmean_sales &lt;- mean(album_sales$Sales)\n\nmean_sales\n\n[1] 193.2\nLet’s break down the code above:\nNext, let’s calculate the median of the Sales variable:\n# Calculate the median of the Sales variable\n\nmedian_sales &lt;- median(album_sales$Sales)\n\nmedian_sales\n\n[1] 200\nThe code above calculates the median of the Sales variable in the album_sales dataset. The median sales value is stored in the median_sales variable.\nFinally, let’s calculate the mode of the Sales variable. Unfortunately, R does not have a built-in function to calculate the mode. However, we do this in the following way:\n# Calculate the mode of the Sales variable\n\nalbum_sales$Sales %&gt;%\n table() \n\n.\n 10  30  40  50  60  70  80  90 100 110 120 130 140 150 160 170 180 190 200 210 \n  1   1   3   1   5   6   3   4   8   5  10   3  11  12   5   4   8   8   7  13 \n220 230 240 250 260 270 280 290 300 310 320 330 340 360 \n  6  17   7  10   3   3   5   8   6   2   4   2   3   6\nWe can see from the output that the mode of the Sales variable is 210, since that value appears most frequently in the dataset (13 times).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory and descriptive analysis</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#mean-median-and-mode",
    "href": "descriptive_stats.html#mean-median-and-mode",
    "title": "3  Exploratory and descriptive analysis",
    "section": "",
    "text": "TipAt the end of this section, you will be able to:\n\n\n\n\nCalculate the mean, median, and mode of a dataset\n\n\n\n\n\n\n\n\n\nWe used the mean() function to calculate the mean of the Sales variable in the album_sales dataset. To do this, we specified the dataset album_sales and the variable Sales using the $ operator.\nThe mean sales value is stored in the mean_sales variable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory and descriptive analysis</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#standard-deviation-and-variance",
    "href": "descriptive_stats.html#standard-deviation-and-variance",
    "title": "3  Exploratory and descriptive analysis",
    "section": "3.2 Standard deviation and variance",
    "text": "3.2 Standard deviation and variance\n\n\n\n\n\n\nTipAt the end of this section, you will be able to:\n\n\n\n\nCalculate the standard deviation\nCalculate the variance\nCalculate the range of a dataset\nCalculate the interquartile range (IQR)\n\n\n\nNext, let’s calculate the standard deviation and variance of the Sales variable in the album_sales dataset:\n\n# Calculate the standard deviation of the Sales variable\n\nsd_sales &lt;- sd(album_sales$Sales)\n\nsd_sales\n\n[1] 80.69896\n\n\nThe code above calculates the standard deviation of the Sales variable in the album_sales dataset. The standard deviation value is stored in the sd_sales variable.\nNext, let’s calculate the variance of the Sales variable:\n\n# Calculate the variance of the Sales variable\n\nvar_sales &lt;- var(album_sales$Sales)\n\nvar_sales\n\n[1] 6512.322\n\n\nThe code above calculates the variance of the Sales variable in the album_sales dataset. The variance value is stored in the var_sales variable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory and descriptive analysis</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#range-and-interquartile-range",
    "href": "descriptive_stats.html#range-and-interquartile-range",
    "title": "3  Exploratory and descriptive analysis",
    "section": "3.3 Range and interquartile range",
    "text": "3.3 Range and interquartile range\nThe range of a dataset is the difference between the maximum and minimum values. Let’s calculate the range of the Sales variable in the album_sales dataset:\n\n# Calculate the range of the Sales variable\n\nrange_sales &lt;- range(album_sales$Sales)\n\nrange_sales\n\n[1]  10 360\n\n\nThe code above calculates the range of the Sales variable in the album_sales dataset. The range of the sales values is stored in the range_sales variable.\nThe interquartile range (IQR) is the difference between the 75th percentile (Q3) and the 25th percentile (Q1) of a dataset. Let’s calculate the IQR of the Sales variable in the album_sales dataset:\n\n# Calculate the interquartile range of the Sales variable\n\nIQR_sales &lt;- IQR(album_sales$Sales)\n\nIQR_sales\n\n[1] 112.5\n\n\nThe code above calculates the interquartile range (IQR) of the Sales variable in the album_sales dataset. The IQR value is stored in the IQR_sales variable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory and descriptive analysis</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#distribution-plots",
    "href": "descriptive_stats.html#distribution-plots",
    "title": "3  Exploratory and descriptive analysis",
    "section": "3.4 Distribution plots",
    "text": "3.4 Distribution plots\n\n\n\n\n\n\nTipAt the end of this section, you will be able to:\n\n\n\n\nCreate a histogram to visualize the distribution of a dataset\nCreate a box plot to visualize the distribution of a dataset\n\n\n\nNext, let’s create a histogram to visualize the distribution of the Sales variable in the album_sales dataset:\n\n# Create a histogram of the Sales variable\n\nhist(album_sales$Sales, main = \"Histogram of Sales\", xlab = \"Sales\", ylab = \"Frequency\", col = \"lightblue\")\n\n\n\n\n\n\n\n\nThe code above creates a histogram of the Sales variable in the album_sales dataset. The histogram displays the frequency of sales values in the dataset. The only required argument for the hist() function is the variable you want to plot. The main, xlab, ylab, and col arguments are optional and allow you to customize the appearance of the histogram.\nFinally, let’s create a box plot to visualize the distribution of the Sales variable in the album_sales dataset:\n\n# Create a box plot of the Sales variable\n\nboxplot(album_sales$Sales, main = \"Boxplot of Sales\", xlab = \"Sales\", col = \"lightblue\")\n\n\n\n\n\n\n\n\nThe code above creates a box plot of the Sales variable in the album_sales dataset. The box plot displays the distribution of sales values, including the median, quartiles, and outliers. The only required argument for the boxplot() function is the variable you want to plot. The main, xlab, and col arguments are optional and allow you to customize the appearance of the box plot.\n\nWe will learn more about plotting data with ggplot2 in another section. However, for now, we have used the base R functions hist() and boxplot() to create simple distribution plots.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory and descriptive analysis</span>"
    ]
  },
  {
    "objectID": "descriptive_stats.html#assessing-the-normality-of-data",
    "href": "descriptive_stats.html#assessing-the-normality-of-data",
    "title": "3  Exploratory and descriptive analysis",
    "section": "3.5 Assessing the normality of data",
    "text": "3.5 Assessing the normality of data\n\n\n\n\n\n\nTipAt the end of this section, you will be able to:\n\n\n\n\nAssess the skewness and kurtosis of a dataset\nTest the normality of a dataset using the Shapiro-Wilk test\n\n\n\nMany statistical tests assume that the data is normally distributed. When your data sample size is small, violation of the normaility assumption could be an issue. Let’s assess the normality of the Sales variable in the album_sales dataset by calculating the skewness and kurtosis. In order to do this, we will use the psych package, which provides functions for calculating skewness and kurtosis. If you haven’t installed the psych package yet, you can do so by running the following code:\n\n# Install the psych package if you haven't already\n\ninstall.packages(\"psych\")\nNow, let’s calculate the skewness and kurtosis of the Sales variable in the album_sales dataset:\n\n# Load the psych package\n\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\n# Calculate the skewness of the Sales variable\n\nskew_sales &lt;- skew(album_sales$Sales)\n\nskew_sales\n\n[1] 0.0432729\n\n# Calculate the kurtosis of the Sales variable\n\nkurt_sales &lt;- kurtosi(album_sales$Sales)\n\nkurt_sales\n\n[1] -0.7157339\n\n\nWhen interpreting the skewness and kurtosis values, remember that values of 0 indicate a normal distribution.\nWe can also test the normality of the Sales variable using the Shapiro-Wilk test. The null hypothesis of the Shapiro-Wilk test is that the data is normally distributed. Let’s perform the Shapiro-Wilk test on the Sales variable in the album_sales dataset:\n\n# Perform the Shapiro-Wilk test on the Sales variable\n\nshapiro.test(album_sales$Sales)\n\n\n    Shapiro-Wilk normality test\n\ndata:  album_sales$Sales\nW = 0.98479, p-value = 0.02965\n\n\nThe output of the Shapiro-Wilk test includes the test statistic and the p-value. If the p-value is less than 0.05, we reject the null hypothesis and conclude that the data is not normally distributed. If the p-value is greater than 0.05, we fail to reject the null hypothesis and conclude that the data is normally distributed.\n\n\n\n\n\n\n\n\nWarningAssessing normality\n\n\n\nThe shapiro-wilk test is sensitive to sample size. For small sample sizes, the test may be too conservative and reject the null hypothesis too often. For large sample sizes, the test may be too lenient and fail to reject the null hypothesis too often. Therefore, you should not rely solely on the Shapiro-Wilk test to assess the normality of your data. Visual inspection of the data using histograms and Q-Q plots is also recommended. Also remember that the central limit theorem states that the sampling distribution of the mean will be approximately normally distributed for large sample sizes, regardless of the distribution of the original data.\nWe could also use non-parametric bootstrapping methods to deal with non-normal data. We will cover this in a later section.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory and descriptive analysis</span>"
    ]
  },
  {
    "objectID": "power_effectSize.html",
    "href": "power_effectSize.html",
    "title": "4  Sampling, power and effect size",
    "section": "",
    "text": "4.1 Different measures of effect size\nIn this chapter, you will learn how to conduct some analysis on the related concepts of sampling, power, and effect size.\nEffect size is a measure of the strength of the relationship between two variables in a statistical population. It is used to quantify the size of the difference between two groups or the strength of an association between two variables. In this section, we will learn how to calculate the effect size for different types of data.\nThe size of an effect is important when planning a study and trying to determine the sample size required. If an effect is small, we need a larger sample size to detect it. If an effect is large, we can detect it with a smaller sample size.\nThe ability of a study to detect an effect, using its sample, is called statistical power. Power is the probability that a study will correctly reject a false null hypothesis. In other words, power is the probability that a study will find a true effect when there is one (avoiding a Type 2 error).\nThere are different ways to calculate effect size depending on the type of data and the statistical test used. Here are some common effect size measures:\nIn the following sections, we will calculate the effect size for different types of data using some of these measures.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sampling, power and effect size</span>"
    ]
  },
  {
    "objectID": "power_effectSize.html#different-measures-of-effect-size",
    "href": "power_effectSize.html#different-measures-of-effect-size",
    "title": "4  Sampling, power and effect size",
    "section": "",
    "text": "Cohen’s d: This is a measure of the difference between two means in standard deviation units. It is commonly used in t-tests and ANOVA tests.\nEta-squared (\\(\\eta^2\\)): This is a measure of the proportion of variance in the dependent variable that is explained by the independent variable. It is commonly used in ANOVA tests.\nPhi coefficient (\\(\\phi\\)): This is a measure of the association between two binary variables. It is commonly used in chi-square tests.\nCorrelation coefficient (\\(r\\)): This is a measure of the strength and direction of the relationship between two continuous variables. It is commonly used in correlation tests.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sampling, power and effect size</span>"
    ]
  },
  {
    "objectID": "power_effectSize.html#calculating-cohens-d",
    "href": "power_effectSize.html#calculating-cohens-d",
    "title": "4  Sampling, power and effect size",
    "section": "4.2 Calculating Cohen’s d",
    "text": "4.2 Calculating Cohen’s d\nCohen’s d is a measure of the difference between two means in standard deviation units. It is calculated as the difference between the means divided by the pooled standard deviation. The formula for Cohen’s d is:\n\\[ d = \\frac{{\\bar{X}_1 - \\bar{X}_2}}{{s_p}} \\]\nwhere:\n\n\\(\\bar{X}_1\\) and \\(\\bar{X}_2\\) are the means of the two groups.\n\\(s_p\\) is the pooled standard deviation, calculated as:\n\n\\[s_p = \\sqrt{\\frac{{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}}{{n_1 + n_2 - 2}}} \\]\nwhere:\n\n\\(n_1\\) and \\(n_2\\) are the sample sizes of the two groups.\n\\(s_1\\) and \\(s_2\\) are the standard deviations of the two groups.\n\nLet’s calculate Cohen’s d for a hypothetical dataset with two groups. The dataset contains the following information:\n\nGroup 1: Mean = 10, Standard deviation = 2, Sample size = 30\nGroup 2: Mean = 12, Standard deviation = 3, Sample size = 30\n\nTo calculate Cohen’s d, we first need to calculate the pooled standard deviation (\\(s_p\\)) using the formula above. Then, we can calculate Cohen’s d using the formula for Cohen’s d.\nLet’s calculate Cohen’s d for this dataset using R:\n\n# Calculate Cohen's d\n\n# Group 1\n\nmean1 &lt;- 10\n\nsd1 &lt;- 2\n\nn1 &lt;- 30\n\n# Group 2\n\nmean2 &lt;- 12\n\nsd2 &lt;- 3\n\nn2 &lt;- 30\n\n# Calculate pooled standard deviation\n\nsp &lt;- sqrt(((n1 - 1) * sd1^2 + (n2 - 1) * sd2^2) / (n1 + n2 - 2))\n\n# Calculate Cohen's d\n\nd &lt;- (mean1 - mean2) / sp\n\n\nd\n\n[1] -0.7844645\n\n\nThe calculated value of Cohen’s d is -0.7844645. This negative value indicates that the mean of Group 1 is smaller than the mean of Group 2 by approximately 0.78 standard deviations.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sampling, power and effect size</span>"
    ]
  },
  {
    "objectID": "power_effectSize.html#calculating-eta-squared-and-other-effect-size-measures",
    "href": "power_effectSize.html#calculating-eta-squared-and-other-effect-size-measures",
    "title": "4  Sampling, power and effect size",
    "section": "4.3 Calculating Eta-squared and other effect size measures",
    "text": "4.3 Calculating Eta-squared and other effect size measures\nIt is possible to calculate other effect size measures such as Eta-squared, Phi coefficient. However, these measures are most commonly calculated using the output of statistical tests such as ANOVA and chi-square tests etc. To obtain these measures for the purpose of sample size calculation, you would usually look at previous studies or meta analyses to determine the expected effect size. For clinical research, you may also use the minimal clinically important difference (MCID) as a guide to determine the effect size.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sampling, power and effect size</span>"
    ]
  },
  {
    "objectID": "power_effectSize.html#power-analysis",
    "href": "power_effectSize.html#power-analysis",
    "title": "4  Sampling, power and effect size",
    "section": "4.4 Power analysis",
    "text": "4.4 Power analysis\nPower analysis is a method used to determine the sample size required to detect an effect of a given size with a certain level of confidence. It is important to conduct a power analysis before conducting a study to ensure that the sample size is adequate to detect the effect of interest.\nTo conduct a power analysis, you need to specify the following parameters:\n\nThe effect size: The size of the effect you want to detect. This is usually determined based on previous studies, meta-analyses or MCID.\nThe significance level (\\(\\alpha\\)): The probability of rejecting the null hypothesis when it is true (Type 1 error rate). This is commonly set at 0.05.\nThe power (\\(1 - \\beta\\)): The probability of correctly rejecting the null hypothesis when it is false (1 - Type 2 error rate). This is commonly set at 0.80 or 0.90.\nThe number of groups or conditions: The number of groups or conditions in the study.\n\nThe sample size required to achieve a desired power level can be calculated using power analysis functions in R. There are many packages in r for power analysis. The package pwr is one such package that provides functions to calculate the sample size required for different types of statistical tests.\nThe functions for some basic research designs are:\n\npwr.t.test(): For t-tests\npwr.anova.test(): For ANOVA tests\npwr.chisq.test(): For chi-square tests\npwr.f2.test(): For regression models\n\nLet’s calculate the sample size required to achieve a power of 0.80 for a t-test with the example data we used earlier. We will use the pwr.t.test() function from the pwr package to calculate the sample size required to achieve a power of 0.80 for a t-test with the following parameters:\n\nEffect size (Cohen’s d) = -0.7844645\nSignificance level (\\(\\alpha\\)) = 0.05\n\n\n# Load the pwr package\n\n\nlibrary(pwr)\n\n# Calculate the sample size required for a t-test\n# using the d value calculated earlier\n\npwr.t.test(d = d, sig.level = 0.05, power = 0.80)\n\n\n     Two-sample t test power calculation \n\n              n = 26.50429\n              d = 0.7844645\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThe output of the pwr.t.test() function provides the sample size required to achieve a power of 0.80 for a t-test with the specified effect size and significance level. The output includes the following information:\n\nn = The sample size required for each group to achieve a power of 0.80.\nd = The effect size (Cohen’s d) used in the power analysis.\nsig.level = The significance level used in the power analysis.\npower = The power level achieved with the specified sample size.\n\nThe sample size required to achieve a power of 0.80 for a t-test with the specified effect size and significance level is 26.5 for each group. Since the sample size must be a whole number, we would need to round up to the nearest whole number. Therefore, the sample size required for each group is 27.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sampling, power and effect size</span>"
    ]
  },
  {
    "objectID": "power_effectSize.html#more-complex-power-analysis",
    "href": "power_effectSize.html#more-complex-power-analysis",
    "title": "4  Sampling, power and effect size",
    "section": "4.5 More complex power analysis",
    "text": "4.5 More complex power analysis\nFor more complex designs, different approaches to power analysis might be necessary, such as using simulation. This is possible to do in R, but is beyond the scope of this chapter.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sampling, power and effect size</span>"
    ]
  },
  {
    "objectID": "basic_tests.html",
    "href": "basic_tests.html",
    "title": "5  Basic statistical tests",
    "section": "",
    "text": "5.1 Independent t-test\nIn this chapter, we will learn how to perform basic statistical tests in R. These are all tests that you should be familiar with already from your statistics courses. We will cover the following tests:\nThese examples will not be exhaustive, but they should give you a good starting point for performing these tests in R. For theoretical background, you can refer to any standard statistics textbook.\nThe independent t-test is used to compare the means of two independent groups. In R, you can use the t.test() function to perform an independent t-test. Here is an example:\n# This example uses the mtcars dataset, which is a built-in dataset in R that contains data on various car models.\n\n# Load the mtcars dataset\n\ndata(mtcars)\n\n# Independent t-test example: Is there a difference in fuel efficiency between automatic and manual cars?\n# the variable am is a binary variable indicating the type of transmission (0 = automatic, 1 = manual)\n\n# Perform the independent t-test\n\nt_test_result &lt;- t.test(mpg ~ am, data = mtcars)\n\n# Print the result\n\nt_test_result\n\n\n    Welch Two Sample t-test\n\ndata:  mpg by am\nt = -3.7671, df = 18.332, p-value = 0.001374\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -11.280194  -3.209684\nsample estimates:\nmean in group 0 mean in group 1 \n       17.14737        24.39231\nIn this example, we are comparing the fuel efficiency (mpg) of automatic and manual cars in the mtcars dataset. The mpg variable is the dependent variable, and the am variable is the independent variable. The t.test() function is used to perform the independent t-test, and the result is stored in the t_test_result variable.\nIf we look at the output, we can see the following:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#independent-t-test",
    "href": "basic_tests.html#independent-t-test",
    "title": "5  Basic statistical tests",
    "section": "",
    "text": "The t-test result is presented.\nThe alternative hypothesis is that the means are not equal.\nThe 95% confidence interval for the difference in means is presented.\nThe 2 sample means are presented.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#paired-t-test",
    "href": "basic_tests.html#paired-t-test",
    "title": "5  Basic statistical tests",
    "section": "5.2 Paired t-test",
    "text": "5.2 Paired t-test\n\n\nThe paired t-test is used to compare the means of two related groups. In R, you can use the t.test() function with the paired = TRUE argument to perform a paired t-test. Here is an example:\n\n# This example uses the sleep dataset, which is a built-in dataset in R that contains data on the effect of two soporific drugs on sleep duration.\n\n# Load the sleep dataset\n\ndata(sleep)\n\n# Paired t-test example: Is there a difference in sleep duration between the two drugs?\n\n# Perform the paired t-test\n# The paired t-test now requires the data to be in wide format, (data from each group in separate columns). If this is not the case, we need to reshape the data first. \n\nsleep2 &lt;- reshape(sleep, direction = \"wide\",\n                  idvar = \"ID\", timevar = \"group\")\n\npaired_t_test_result &lt;- t.test(sleep2$extra.1, sleep2$extra.2, paired = TRUE)\n\n# Print the result\n\npaired_t_test_result\n\n\n    Paired t-test\n\ndata:  sleep2$extra.1 and sleep2$extra.2\nt = -4.0621, df = 9, p-value = 0.002833\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.4598858 -0.7001142\nsample estimates:\nmean difference \n          -1.58 \n\n\nIn this example, we are comparing the sleep duration (extra) between patients who were given different drugs in the sleep dataset (note: 10 people were each measured twice). The extra variable is the dependent variable, and the group variable is the independent variable. The t.test() function is used to perform the paired t-test, and the result is stored in the paired_t_test_result variable.\nIf we look at the output, we can see the following:\n\nThe t-test result is presented.\nThe alternative hypothesis is that the means are not equal.\nThe 95% confidence interval for the difference in means is presented.\nThe mean difference is presented.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#wilcoxon-signed-rank-test",
    "href": "basic_tests.html#wilcoxon-signed-rank-test",
    "title": "5  Basic statistical tests",
    "section": "5.3 Wilcoxon signed-rank test",
    "text": "5.3 Wilcoxon signed-rank test\nThe Wilcoxon signed-rank test is a non-parametric test used to compare two related groups. In R, you can use the wilcox.test() function to perform a Wilcoxon signed-rank test. Here is an example:\n\n# This example uses the sleep dataset, which is a built-in dataset in R that contains data on the effect of two soporific drugs on sleep duration.\n\n# Load the sleep dataset\n\ndata(sleep)\n\n# Wilcoxon signed-rank test example: Is there a difference in sleep duration between the two drugs?\n\n# Perform the Wilcoxon signed-rank test\n# The wilcoxin now requires the data to be in wide format, (data from each group in separate columns). If this is not the case, we need to reshape the data first. \n\nsleep2 &lt;- reshape(sleep, direction = \"wide\",\n                  idvar = \"ID\", timevar = \"group\")\n\nwilcoxon_test_result &lt;- wilcox.test(sleep2$extra.1, sleep2$extra.2, paired = TRUE)\n\nWarning in wilcox.test.default(sleep2$extra.1, sleep2$extra.2, paired = TRUE):\ncannot compute exact p-value with ties\n\n\nWarning in wilcox.test.default(sleep2$extra.1, sleep2$extra.2, paired = TRUE):\ncannot compute exact p-value with zeroes\n\n# Print the result\n\nwilcoxon_test_result\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  sleep2$extra.1 and sleep2$extra.2\nV = 0, p-value = 0.009091\nalternative hypothesis: true location shift is not equal to 0\n\n\nIn this example, we are comparing the sleep duration (extra) between patients who were given different drugs in the sleep dataset. The extra variable is the dependent variable, and the group variable is the independent variable. The wilcox.test() function is used to perform the Wilcoxon signed-rank test, and the result is stored in the wilcoxon_test_result variable.\nIf we look at the output, we can see the following:\n\nThe Wilcoxon signed-rank test result is presented ( V is the sum of the ranks of the differences between the pairs of observations).\nThe alternative hypothesis (true location shift is not equal to 0) is presented. This means that the medians of the two groups are not equal.\n\nNote: if there are ties in the data, the exact p-value is not calculated. Instead, an approximate p-value is presented.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#mann-whitney-u-test",
    "href": "basic_tests.html#mann-whitney-u-test",
    "title": "5  Basic statistical tests",
    "section": "5.4 Mann-Whitney U test",
    "text": "5.4 Mann-Whitney U test\nThe Mann-Whitney U test is a non-parametric test used to compare the means of two independent groups. In R, you can use the wilcox.test() function to perform a Mann-Whitney U test. Note: we will not include the paired = TRUE argument that we did in the previous example. Here is an example:\n\n# This example uses the mtcars dataset, which is a built-in dataset in R that contains data on various car models.\n\n# Load the mtcars dataset\n\ndata(mtcars)\n\n# Mann-Whitney U test example: Is there a difference in fuel efficiency between automatic and manual cars?\n\n# Perform the Mann-Whitney U test\n\nmann_whitney_test_result &lt;- wilcox.test(mpg ~ am, data = mtcars)\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with ties\n\n# Print the result\n\nmann_whitney_test_result\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  mpg by am\nW = 42, p-value = 0.001871\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe output is interpreted in the same way as the Wilcoxon signed-rank test output.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#chi-squared-test",
    "href": "basic_tests.html#chi-squared-test",
    "title": "5  Basic statistical tests",
    "section": "5.5 Chi-squared test",
    "text": "5.5 Chi-squared test\n\n\nThe chi-squared test is used to test the association between two categorical variables. In R, you can use the chisq.test() function to perform a chi-squared test. Here is an example:\n\n# This example uses the mtcars dataset, which is a built-in dataset in R that contains data on various car models.\n\n# Load the mtcars dataset\n\ndata(mtcars)\n\n# Chi-squared test example: Is there an association between the number of cylinders and the type of transmission?\n\n# Create a contingency table\n\ncontingency_table &lt;- table(mtcars$cyl, mtcars$am)\n\n# Perform the chi-squared test\n\nchi_squared_test_result &lt;- chisq.test(contingency_table)\n\nWarning in chisq.test(contingency_table): Chi-squared approximation may be\nincorrect\n\n# Print the result\n\nchi_squared_test_result\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 8.7407, df = 2, p-value = 0.01265\n\n\nIn this example, we are testing the association between the number of cylinders (cyl) and the type of transmission (am) in the mtcars dataset. The chisq.test() function is used to perform the chi-squared test, and the result is stored in the chi_squared_test_result variable.\nIf we look at the output, we can see the following:\n\nThe chi-squared test result is presented.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#correlation",
    "href": "basic_tests.html#correlation",
    "title": "5  Basic statistical tests",
    "section": "5.6 Correlation",
    "text": "5.6 Correlation\n\n\n\nCorrelation is used to test the relationship between two continuous variables. In R, you can use the cor.test() function to calculate the correlation coefficient with significance test. Here is an example:\n\n# This example uses the mtcars dataset, which is a built-in dataset in R that contains data on various car models.\n\n# Load the mtcars dataset\n\ndata(mtcars)\n\n# Correlation example: Is there a relationship between fuel efficiency and horsepower?\n\n# Calculate the correlation coefficient\n\ncorrelation_result &lt;- cor.test(mtcars$mpg, mtcars$hp)\n\n# Print the result\n\ncorrelation_result\n\n\n    Pearson's product-moment correlation\n\ndata:  mtcars$mpg and mtcars$hp\nt = -6.7424, df = 30, p-value = 1.788e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.8852686 -0.5860994\nsample estimates:\n       cor \n-0.7761684 \n\n\nIn this example, we are testing the relationship between fuel efficiency (mpg) and horsepower (hp) in the mtcars dataset. The cor.test() function is used to calculate the correlation coefficient, and the result is stored in the correlation_result variable.\nIf we look at the output, we can see the following:\n\nThe correlation coefficient is presented in the last line of the output.\nThe significance level of the correlation is tested using a t-test, which is also presented in the output.\nThe confidence interval for the correlation coefficient is presented.\nThe alternative hypothesis is that the correlation is not equal to 0.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#one-way-anova",
    "href": "basic_tests.html#one-way-anova",
    "title": "5  Basic statistical tests",
    "section": "5.7 One-way ANOVA",
    "text": "5.7 One-way ANOVA\nOne-way ANOVA (Analysis of Variance) is used to test the differences between the means of three or more groups. In R, you can use the aov() function to perform an ANOVA. Here is an example:\n\n\n\n# This example uses the mtcars dataset, which is a built-in dataset in R that contains data on various car models.\n\n# Load the mtcars dataset\n\ndata(mtcars)\n\n# One-way ANOVA example: Is there a difference in fuel efficiency between cars with different numbers of cylinders?\n\n# cyl should be a factor\n\nmtcars$cyl &lt;- as.factor(mtcars$cyl)\n\n# Perform the ANOVA\n\nanova_result &lt;- aov(mpg ~ cyl, data = mtcars)\n\n# Print the result\n\nsummary(anova_result)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncyl          2  824.8   412.4    39.7 4.98e-09 ***\nResiduals   29  301.3    10.4                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this example, we are testing the differences in fuel efficiency (mpg) between cars with different numbers of cylinders (cyl) in the mtcars dataset. The aov() function is used to perform the ANOVA, and the result is stored in the anova_result variable.\nIf we look at the output, we can see the following:\n\nThe ANOVA result is within the summary() function. The summary includes the F-statistic, the p-value, and the significance level of the ANOVA test.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#factorial-anova",
    "href": "basic_tests.html#factorial-anova",
    "title": "5  Basic statistical tests",
    "section": "5.8 Factorial ANOVA",
    "text": "5.8 Factorial ANOVA\nFactorial ANOVA is used to test the effects of two or more independent variables on a dependent variable. In R, you can use the aov() function with interaction terms to perform a factorial ANOVA. Here is an example:\n\n# This example uses the mtcars dataset, which is a built-in dataset in R that contains data on various car models.\n\n# Load the mtcars dataset\n\ndata(mtcars)\n\n# Factorial ANOVA example: Is there an interaction effect between the number of cylinders and the type of transmission on fuel efficiency?\n\n# cyl and am should be factors\n\nmtcars$cyl &lt;- as.factor(mtcars$cyl)\nmtcars$am &lt;- as.factor(mtcars$am)\n\n# Perform the factorial ANOVA\n\nfactorial_anova_result &lt;- aov(mpg ~ cyl * am, data = mtcars)\n\n# Print the result\n\nsummary(factorial_anova_result)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncyl          2  824.8   412.4  44.852 3.73e-09 ***\nam           1   36.8    36.8   3.999   0.0561 .  \ncyl:am       2   25.4    12.7   1.383   0.2686    \nResiduals   26  239.1     9.2                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this example, we are testing the interaction effect between the number of cylinders (cyl) and the type of transmission (am) on fuel efficiency (mpg) in the mtcars dataset. The aov() function is used to perform the factorial ANOVA, and the result is stored in the factorial_anova_result variable.\nThe interaction term is specified using the * operator in the formula.\nIf we look at the output, we can see the following:\n\nThe ANOVA result is within the summary() function. The summary includes the F-statistic, the p-value, and the significance level of each term tested in the ANOVA.\nEach independent variable is tested separately (main effects), and the interaction effect is also tested.\nThe interaction effect is denoted by the cyl:am term in the output.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "basic_tests.html#conclusion",
    "href": "basic_tests.html#conclusion",
    "title": "5  Basic statistical tests",
    "section": "5.9 Conclusion",
    "text": "5.9 Conclusion\nIn this chapter, we have covered several basic statistical tests that you can perform in R. These are included for reference and to help you get started with performing statistical tests in R. However, we will be focusing on modelling our data, using different types of regression models, rather than re-learning these tests.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic statistical tests</span>"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "6  Correlation in R",
    "section": "",
    "text": "6.1 What is Correlation?\nCorrelation is a measure of the strength and direction of a relationship between two variables. It is most commonly used when we want to see if there is a relationship between two continuous variables. However, it is possible to run correlations between a continuous and a categorical variable (this is known as point-biserial correlation) or between two categorical variables (this is known as phi coefficient).\nSticking to the more conventional form of correlation; when we calculate correlation, we get an r value of between -1 and 1. This tells us two things:\nWe also often calculate the significance of the correlation, which tests against a null hypothesis that the correlation is 0. This is not part of the correlation per se, but it is often part of correlational research questions.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation in R</span>"
    ]
  },
  {
    "objectID": "correlation.html#what-is-correlation",
    "href": "correlation.html#what-is-correlation",
    "title": "6  Correlation in R",
    "section": "",
    "text": "The closer the value is to 1, the stronger the relationship. The closer the value is to 0, the weaker the relationship.\nThe sign of the value tells us the direction of the relationship. Positive values indicate a positive relationship (i.e. as one variable increases, so does the other). Negative values indicate a negative relationship (i.e. as one variable increases, the other decreases).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation in R</span>"
    ]
  },
  {
    "objectID": "correlation.html#visualising-correlation",
    "href": "correlation.html#visualising-correlation",
    "title": "6  Correlation in R",
    "section": "6.2 Visualising correlation",
    "text": "6.2 Visualising correlation\nWe can visualise correlation using a scatterplot. This is a graph where each data point is plotted on a graph, with one variable on the x axis and the other on the y axis. If the data points form something resembling a straight line, with all of the data points in a consistent pattern, then we have a strong correlation. If the data points are scattered, then we have a weaker correlation. However, if the data points are inconsistent or more diffuse, the correlation is weaker. The direction of the line tells us the direction of the correlation.\n\n\n\n\n\n\n\n\n\nVisualising the data in this way can give us a good idea of the strength and direction of the correlation. However, it is not a substitute for running the correlation itself. At the same time, correlation coefficients can be misleading on their own. It is always a good idea to visualise the data as well.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation in R</span>"
    ]
  },
  {
    "objectID": "correlation.html#how-is-correlation-calculated",
    "href": "correlation.html#how-is-correlation-calculated",
    "title": "6  Correlation in R",
    "section": "6.3 How is correlation calculated?",
    "text": "6.3 How is correlation calculated?\nCorrelation can be thought of as covariance divided by individual variance. Covariance is a measure of how much two variables change together. Variance is a measure of how much a variable changes on its own. When we divide covariance by variance, we get a value that is standardised and can be compared across different data sets.\nIf the changes are consistent with both variables (i.e. the covariance is higher and the individual variance is lower), then the final correlation value will be higher. However, if the changes are inconsistent (i.e. the covariance is lower and the individual variance is higher), then the final correlation value will be lower.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation in R</span>"
    ]
  },
  {
    "objectID": "correlation.html#which-correlation-to-use",
    "href": "correlation.html#which-correlation-to-use",
    "title": "6  Correlation in R",
    "section": "6.4 Which correlation to use?",
    "text": "6.4 Which correlation to use?\nWhen we run correlation in R, we use the cor.test() command. This command will give us the correlation value, the p value and the confidence intervals.\nWe can specify a Pearson correlation (the default) or a Spearman correlation (for non-parametric data).\n\n6.4.1 Running correlation in R\n\nR can run correlations using the cor.test() command\n\n\ncor.test(regression_data$treatment_duration,regression_data$aggression_level) \n\n\n    Pearson's product-moment correlation\n\ndata:  regression_data$treatment_duration and regression_data$aggression_level\nt = -9.5503, df = 98, p-value = 1.146e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7838251 -0.5765006\nsample estimates:\n       cor \n-0.6942996 \n\n\nIn the above example, we are testing the correlation between treatment duration and aggression level. Each variable is separated by a comma.\n\n\n6.4.2 Interpreting the output\n\nThe r value tells us the strength and direction of the relationship\nIn the output it is labelled as “cor” (short for correlation)\n\nCorrelation values can range from -1 to 1. The closer the value is to 1, the stronger the relationship. The closer the value is to 0, the weaker the relationship. Positive values indicate a positive relationship (i.e. as one variable increases, so does the other). Negative values indicate a negative relationship (i.e. as one variable increases, the other decreases).\n\ncor.test(regression_data$treatment_duration,regression_data$aggression_level)\n\n\n    Pearson's product-moment correlation\n\ndata:  regression_data$treatment_duration and regression_data$aggression_level\nt = -9.5503, df = 98, p-value = 1.146e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7838251 -0.5765006\nsample estimates:\n       cor \n-0.6942996 \n\n\n\n\n6.4.3 Check the significance of the correlation\n\nWe can see that the significance by looking at the p value\n\nThe significance is 1.146^-15\nThis means: 0.0000000000000001146\n\nTherefore p value &lt; 0.05\n\n\ncor.test(regression_data$treatment_duration,regression_data$aggression_level)\n\n\n    Pearson's product-moment correlation\n\ndata:  regression_data$treatment_duration and regression_data$aggression_level\nt = -9.5503, df = 98, p-value = 1.146e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7838251 -0.5765006\nsample estimates:\n       cor \n-0.6942996 \n\n\n\n\n\n\n\n\nTipExponent values\n\n\n\nWhen we see a value like 1.146e-15, this is a shorthand way of writing a very small number. The e-15 means that we move the decimal point 15 places to the left. So 1.146e-15 is the same as 0.000000000000001146",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation in R</span>"
    ]
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "7  Simple Regression in R",
    "section": "",
    "text": "8 What is regression analysis?\nRegression analysis is a statistical technique used to model the relationship between an outcome (dependent) variable and one or more predictor (criterion/independent) variables. The goal of regression analysis is to understand how the outcome variable changes as the predictor variable changes.\nWhen we say simple regression, we mean that there is only one predictor variable. This is to distinguish from multiple regression, where there are two or more predictor variables.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simple Regression in R</span>"
    ]
  },
  {
    "objectID": "regression.html#checking-model-assumptions",
    "href": "regression.html#checking-model-assumptions",
    "title": "7  Simple Regression in R",
    "section": "9.1 Checking model assumptions",
    "text": "9.1 Checking model assumptions\nBefore interpreting the results of a regression analysis, it is essential to check the assumptions of the model. The key assumptions of linear regression are:\n\nLinearity: The relationship between the predictor and outcome variables is linear.\nIndependence: The residuals (errors) are independent of each other.\nHomoscedasticity: The residuals have constant variance.\nNormality: The residuals are normally distributed.\n\nWe can check these assumptions using diagnostic plots. The plot() function in R can be used to create diagnostic plots for the regression model.\n\n# Create diagnostic plots\n\n1plot(regression_model)\n\n\n1\n\nWe create diagnostic plots for the regression model using the plot() function. The diagnostic plots include a scatterplot of the residuals against the fitted values, a Q-Q plot of the residuals, a scale-location plot, and a plot of the residuals against the predictor variable.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simple Regression in R</span>"
    ]
  },
  {
    "objectID": "regression.html#check-the-assumptions",
    "href": "regression.html#check-the-assumptions",
    "title": "7  Simple Regression in R",
    "section": "9.2 Check the assumptions",
    "text": "9.2 Check the assumptions\nWe can check the assumptions of linear regression using the plot() function.\n\n# check the assumptions\n\n1plot(model)\n\n\n1\n\nThe plot() function is used to create a plot of the model. It actually shows several plots in sequence.\n\n\n\n\n\nWhen you run the plot function, there will be a message in the console that says “Hit  to see next plot:”. You need to press enter to move through the plots.\n\n\n9.2.1 The assumption of independence\nThis assumption is the idea that each observation is independent of the others. In other words, the value of one observation (e.g. a participant’s score) should not be related to the value of another participant’s score. This is not interpreted from the plots, but is an assumption that should be considered when collecting data. For example, you would not use this approach to analyse data from a repeated measures design, because the observations are not independent.\n\n\n9.2.2 The assumption of linearity\nThe first plot is a plot of the residuals against the fitted values. This plot is used to check the assumption of linearity. The assumption of linearity states that the relationship between the predictor variable and the outcome variable is linear. In other words, the relationship between the predictor variable and the outcome variable can be described by a straight line. If the relationship is not linear, then the model is not a good fit for the data.\n\n\n\nLinear and non-linear data\n\n\n\n\n\n\n\n\nTipLinear and non-linear residuals\n\n\n\nGood: Random scatter around the line. The line is straight.\nBad: Non-random scatter around the line. The line is not straight.\nWhat do we do if this assumption is violated?: Linearity is a very important assumption. If the assumption is violated, then the linear model is likely not a good fit for the data. In this case we should probably try a different model. Data are never perfect, but we shouldn’t ignore a clear violation of linearity.\n\n\n\n\n9.2.3 The assumption of normality\nThe second plot is a normal Q-Q plot of the residuals. This plot is used to check the assumption of normality. The assumption of normality states that the residuals are normally distributed. If the residuals are not normally distributed, then the model is not a good fit for the data.\n\n\n\nqqplot of residuals\n\n\n\n\n\n\n\n\nTipNormal and non-normal residuals\n\n\n\nGood: The points follow the line.\nBad: The points do not follow the line.\nWhat do we do if this assumption is violated?: Normality affects the accuracy of beta values, significance tests and confidence intervals. However, it is most important with small sample sizes. As sample size increases, the assumption of normality becomes less important.\n\n\n\n\n9.2.4 The assumption of homoscedasticity\nThe third plot is a scale-location plot of the residuals against the fitted values. This plot is used to check the assumption of homoscedasticity. The assumption of homoscedasticity states that the residuals have equal variance. If the residuals do not have equal variance, then the model is not a good fit for the data.\n\n\n\nhomoscedasticity\n\n\n\n\n\n\n\n\nTipHomoscedastic and heteroscedastic residuals\n\n\n\nGood: The points are randomly scattered around the line. and the line is horizontal.\nBad: The points are not randomly scattered around the line. and the line is not horizontal.\nWhat do we do if this assumption is violated?: This will affect the accuracy of the beta values, significance tests and confidence intervals. Essentially, it means that conclusions we draw from the model are less accurate. What we do depends on the situation. Transformation of the DV (e.g. log transformation) might help. If not, there are weighted regression models that can be used.\n\n\n\n\n9.2.5 Checking for outliers or influential cases\nThe fourth plot is a plot of Cook’s distance. This plot is used to check for outliers. An outlier is a data point that is very different from the rest of the data. If there are outliers, then they could be affecting the regression model. The threshold for Cook’s distance is 1. If a data point has a Cook’s distance greater than 1, then it is considered an outlier.\nThe fifth plot is a plot of the residuals against the leverage. The sixth plot is a plot of the Cook’s distance against the leverage. They are pretty much the same plots as the plot 4.\n\n\n\nInfluential Cases and Leverage\n\n\nOn the plot above, Cook’s Distance is indicated by a red line. If a data point is outside the red line, then it is considered an outlier.\nLeverage is the idea that a particular outlier might have a lot of influence on the regression model. Look for data points that are outside the red line on the top right or bottom right of the plot. These are data points that have a lot of leverage and might be influencing the regression model.\n\n\n\n\n\n\nTipOutliers and influential cases\n\n\n\nGood: No data points outside the red lines.\nBad: Data points outside the red lines. Outliers with high leverage.\nWhat do we do if this assumption is violated?: Outliers will affect the calculation of variances (e.g. sum of squares or standard deviation) that are used in many calculations related to the regression model. If there are influentual cases, we should consider removing them from the analysis. When doing so, it is important to explain why you removed them, and be transparent about how this affected the model results.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simple Regression in R</span>"
    ]
  },
  {
    "objectID": "regression.html#interpret-the-regression-model-results",
    "href": "regression.html#interpret-the-regression-model-results",
    "title": "7  Simple Regression in R",
    "section": "9.3 Interpret the regression model results",
    "text": "9.3 Interpret the regression model results\nTo view the results of the regression model, we use the summary() function.\n\n# View the results\n\n1summary(regression_model)\n\n\n1\n\nWe view the results of the regression model using the summary() function. The output includes the coefficients, standard errors, t-values, p-values, and R-squared value of the model.\n\n\n\n\n\nCall:\nlm(formula = trust_score ~ treatment_duration, data = regression_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.585 -24.991   0.304  24.285  46.804 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          64.754     14.176   4.568 1.44e-05 ***\ntreatment_duration   -1.130      1.371  -0.824    0.412    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 29.29 on 98 degrees of freedom\nMultiple R-squared:  0.006886,  Adjusted R-squared:  -0.003248 \nF-statistic: 0.6795 on 1 and 98 DF,  p-value: 0.4118\n\n\n\n9.3.1 Call: The regression formula\nThe first line of the output is the regression formula. This is the formula that was used to create the model.\n\n\n9.3.2 Residuals: The residuals\nThe second line of the output is the residuals. The residuals are the difference between the actual values of the outcome variable and the predicted values of the outcome variable. This section is giving us some summary statistics about the residuals. However, we usually check the assumptions using the plots.\n\n\n9.3.3 Coefficients: The beta values\nThe third section of the output is the coefficients. You will see a line of values for the intercept and another line for each of the predictor variables in the model. Estimate is the beta value. Std. Error is the standard error of the beta value. Pr(&gt;|t|) is the p value for the beta value.\n\nIntercept: We are not usually interested in this line by itself. It is the value of the outcome variable when all of the predictor variables are equal to zero. In this case, it is the value of avoidance when depression is equal to zero. However, it might be the case that depression cannot be equal to zero. In this case, the intercept would not be meaningful. If the predictor were a categorical variable, then the intercept would be the value of the outcome variable when the predictor variable is equal to the reference category (i.e. The mean of the outcome for that group).\nDepression: This is the beta value for depression. It is the amount that avoidance changes when depression increases by one unit. What unit means, depends on how the variables were measured, so it is likely to mean one point in the scale used to measure depression, for example.\n\nThe final section in the output shows:\n\nResidual standard error. This is the standard deviation of the residuals. It is the average amount that the actual values of the outcome variable differ from the predicted values of the outcome variable.\nMultiple R-squared. This is the R-squared value. It is the amount of variance in the outcome variable that is explained by the model. We usually talk about this as a percentage value.\nAdjusted R-squared. This is the adjusted R-squared value. It is the amount of variance in the outcome variable that is explained by the model, adjusted for the number of predictor variables in the model. This is to account for the fact that having more predictors in the model will always increase the R-squared value, even if the predictors are not related to the outcome variable. It is relevant when we have more than one predictor variable in the model.\nF Statistic. The F value comes from the ANOVA that is used to test the significance of the model. It tests the null hypothesis that all of the beta values are equal to zero.\np-value. This is the p value for the F statistic (the significance of the overall regression model, with all of the predictors).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simple Regression in R</span>"
    ]
  },
  {
    "objectID": "regression.html#catreg",
    "href": "regression.html#catreg",
    "title": "7  Simple Regression in R",
    "section": "9.4 Regression with a categorical predictor variable",
    "text": "9.4 Regression with a categorical predictor variable\nSo far, we have conducted a regression analysis with a continuous predictor variable. Now, let’s consider a regression analysis with a categorical predictor variable.\n\n\nOur next hypothesis is that the level of depression is different for each treatment group. We can test this hypothesis using a regression model with a categorical predictor variable. Let’s conduct a regression analysis with the treatment variable as the predictor variable. If there are 2 levels of the predictor variable, then the model will compare the two levels. If there are more than 2 levels, then the model will compare each level to the reference level. By default, R uses the first level of the predictor variable as the reference level. Howeverm you can specify a different reference level using the relevel() function.\n\n# specify the reference level - this is more useful when we have more than 2 levels\n\n0regression_data$treatment_group &lt;- as.factor(regression_data$treatment_group)\n1regression_data$treatment &lt;- relevel(regression_data$treatment_group, ref = \"therapy1\")\n\n# Fit a regression model with a categorical predictor variable\n\n2regression_model2 &lt;- lm(aggression_level ~ treatment_group, data = regression_data)\n\n# Summary of the model\n\n3summary(regression_model2)\n\n\n0\n\nWe convert the treatment variable to a factor variable using the as.factor() function. This is necessary for R to recognize the variable as a categorical variable.\n\n1\n\nWe specify the reference level for the treatment variable using the relevel() function. In this case, we set the reference level to “therapy1”. This is likely the default level, but we are specifying it here for clarity. If we had more than 2 levels, we could specify a different reference level using the ref argument. For example, `ref = “control”\n\n2\n\nWe fit a regression model with the treatment_group variable as the predictor variable. The outcome variable is aggression_level. The data are specified using the data argument.\n\n3\n\nWe view the results of the regression model using the summary() function. The output includes the coefficients, standard errors, t-values, p-values, and R-squared value of the model.\n\n\n\n\n\nCall:\nlm(formula = aggression_level ~ treatment_group, data = regression_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.6800 -1.4882 -0.0185  1.3460  4.4131 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               4.6800     0.2843  16.464  &lt; 2e-16 ***\ntreatment_grouptherapy2   1.3201     0.4103   3.217  0.00175 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.05 on 98 degrees of freedom\nMultiple R-squared:  0.09554,   Adjusted R-squared:  0.08631 \nF-statistic: 10.35 on 1 and 98 DF,  p-value: 0.001753\n\n\nThe output tells us that the model is significant, and that the beta values for the treatment_group therapy2 is different from the reference group (therapy1) which is the intercept of this model. The beta values are the difference in the outcome variable between the reference group and the other groups. The beta value for the reference group is the mean of the outcome variable (aggression_level) for that group. The beta values for therapy2 is the difference in the mean outcome variable for that group and the reference group.\n\n\n\n\n\n\nTipInterpreting the results of a regression model with a categorical predictor variable\n\n\n\nWhen we have a categorical predictor variable, the interpretation of the beta values changes. The beta values are the difference in the outcome variable between the reference category and the other categories. The reference category is the category that is not included in the model. The beta value for the reference category is the mean of the outcome variable for that category. The beta values for the other categories are the difference in the outcome variable between that category and the reference category.\nIf you want to see this done another way, if your predictor has 2 levels, you can conduct a t-test and compare the means of the two groups. The t-test will give you the same results as the regression model. The t-test is a simpler way to compare the means of two groups, but the regression model is more flexible and can handle more complex models (by adding more predictor variables).\n\n\n\n\n\n\n\n\nWarningTesting assumptions with a categorical predictor variable\n\n\n\nWhen we have a categorical predictor variable, we need to be careful about how we interpret the assumptions of linear regression. The assumptions of linearity, normality and homoscedasticity are still relevant. However, because we have a categorical predictor, the plots will look different. We need to check the assumptions for each level of the categorical predictor variable. Some of the plots will change to reflect this.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simple Regression in R</span>"
    ]
  },
  {
    "objectID": "regression.html#comparing-multiple-levels-of-predictor-variables-estimated-marginal-means",
    "href": "regression.html#comparing-multiple-levels-of-predictor-variables-estimated-marginal-means",
    "title": "7  Simple Regression in R",
    "section": "9.5 Comparing multiple levels of predictor variables (estimated marginal means)",
    "text": "9.5 Comparing multiple levels of predictor variables (estimated marginal means)\nIf we have more than 2 levels of the predictor variable, we can compare each level to the reference level. We can also compare all levels to each other using the emmeans package. The emmeans package provides estimated marginal means for the levels of a predictor variable. We can use the emmeans() function to calculate the estimated marginal means for the levels of the predictor variable.\nFor this example, let’s use the mtcar dataset, which contains information about cars. We will conduct a regression analysis with the cyl variable as the predictor variable and the mpg variable as the outcome variable. The cyl variable has 3 levels (4, 6, and 8 cylinders). We will compare the mean mpg for each level of the cyl variable.\n\n# Load the mtcars dataset\n\ndata(mtcars)\n\n# Convert the cyl variable to a factor variable\n\nmtcars$cyl &lt;- as.factor(mtcars$cyl)\n\n# Fit a regression model with the cyl variable as the predictor variable\n\nregression_model3 &lt;- lm(mpg ~ cyl, data = mtcars)\n\n# Summary of the model\n\nsummary(regression_model3)\n\n# Compare the levels of the predictor variable with pairwise comparisons\n\nlibrary(emmeans)\n\n\n3\n\nWe use the emmeans() function from the emmeans package to calculate the estimated marginal means for the levels of the cyl variable. The pairwise ~ cyl argument specifies that we want to compare the levels of the cyl variable with pairwise comparisons.\n\n\n\n\nWelcome to emmeans.\nCaution: You lose important information if you filter this package's results.\nSee '? untidy'\n\n3emmeans(regression_model3, pairwise ~ cyl)\n\n\nCall:\nlm(formula = mpg ~ cyl, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2636 -1.8357  0.0286  1.3893  7.2364 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  26.6636     0.9718  27.437  &lt; 2e-16 ***\ncyl6         -6.9208     1.5583  -4.441 0.000119 ***\ncyl8        -11.5636     1.2986  -8.905 8.57e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.223 on 29 degrees of freedom\nMultiple R-squared:  0.7325,    Adjusted R-squared:  0.714 \nF-statistic:  39.7 on 2 and 29 DF,  p-value: 4.979e-09\n\n$emmeans\n cyl emmean    SE df lower.CL upper.CL\n 4     26.7 0.972 29     24.7     28.7\n 6     19.7 1.220 29     17.3     22.2\n 8     15.1 0.861 29     13.3     16.9\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast    estimate   SE df t.ratio p.value\n cyl4 - cyl6     6.92 1.56 29   4.441  0.0003\n cyl4 - cyl8    11.56 1.30 29   8.905  &lt;.0001\n cyl6 - cyl8     4.64 1.49 29   3.112  0.0112\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nThe output tells us that the mean mpg for the 4-cylinder cars is significantly different from the mean mpg for the 6-cylinder cars and the 8-cylinder cars. The mean mpg for the 6-cylinder cars is also significantly different from the mean mpg for the 8-cylinder cars. The estimated marginal means provide a way to compare the levels of the predictor variable and determine if there are significant differences between them.\n\n\n\n\n\n\nTipWhat are estimated marginal means?\n\n\n\nEstimated marginal means are the predicted means of the outcome variable for each level of the predictor variable. They are calculated by averaging the predicted values of the outcome variable for each level of the predictor variable, while holding all other variables constant. Estimated marginal means provide a way to compare the levels of the predictor variable and determine if there are significant differences between them.\n\n9.5.1 What is the difference between estimated marginal means and actual means - how should I report them?\nEstimated marginal means are the predicted means of the outcome variable for each level of the predictor variable. They are calculated by averaging the predicted values of the outcome variable for each level of the predictor variable, while holding all other variables constant. Actual means are the observed means of the outcome variable for each level of the predictor variable. The difference between estimated marginal means and actual means is that estimated marginal means are based on the regression model, while actual means are based on the observed data. This allows the estimated marginal means to take into account other variables in the model, for example.\nWhen reporting the results of a regression analysis, it is common to report the estimated marginal means rather than the actual means. This is because the estimated marginal means take into account the effects of other variables in the model, while the actual means do not. Reporting the estimated marginal means can give a more accurate representation of the relationship between the predictor variable and the outcome variable.\n\n\n9.5.2 What are Tukey corrected p-values?\nTukey corrected p-values are used to adjust for multiple comparisons in a pairwise comparison analysis. When conducting multiple comparisons between the levels of a predictor variable, there is an increased risk of making a Type I error (false positive) due to the number of comparisons being made. Tukey corrected p-values adjust for this increased risk. This means that the p-values are adjusted to account for the number of comparisons being made, reducing the likelihood of making a false positive conclusion.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simple Regression in R</span>"
    ]
  },
  {
    "objectID": "multipleRegression.html",
    "href": "multipleRegression.html",
    "title": "8  Multiple Regression and Heirarchical Regression",
    "section": "",
    "text": "8.1 What is multiple regression?\nMultiple regression ia an extension of simple regression that allows us to predict an outcome variable (Y) based on multiple predictors (X1, X2 …).\n\\[ Y = b_1X_1 + b_2X_2 + b_0 \\]\n(The constant can be referred to in the equation as c or b0 )",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multiple Regression and Heirarchical Regression</span>"
    ]
  },
  {
    "objectID": "multipleRegression.html#what-are-the-assumptions-of-multiple-regression",
    "href": "multipleRegression.html#what-are-the-assumptions-of-multiple-regression",
    "title": "8  Multiple Regression and Heirarchical Regression",
    "section": "8.2 What are the assumptions of Multiple Regression?",
    "text": "8.2 What are the assumptions of Multiple Regression?\nThey are primarily the same as simple regression, so look back at the assumptions of simple regression in that section. The additional assumption of no multicollinearity applies, due to having multiple predictors. The multicollinearity assumption means that predictors should not be highly correlated. If they were, it would be difficult to determine the unique contribution of each predictor to the model.\n\n8.2.1 What is multicollinearity?\nMulticollinearity means that predictors correlated highly with each other. This is not good because:\n\nIt makes it difficult to determine the role of individual predictors\nIncreases the error of the model (higher standard errors)\nDifficult to identify significant predictors\nWider confidence interval\n\n\n\n8.2.2 Testing multicollinearity\nTo test for multicollinearity, we can use the mctest() function in R. This function is part of the mctest package. It performs several tests for multicollinearity, including the Variance Inflation Factor (VIF) and the Condition Index (CI).\nTo run the test, you pass the regression model to the mctest() function. The function will then return the results of the tests.\n\n## use the mctest package\n# install.packages(‘mctest’)\n\nlibrary(mctest)\n\n1m1 &lt;- lm(aggression_level ~ treatment_group + treatment_duration + trust_score, data=regression_data)\n\n2mctest(m1)\n\n\n1\n\nIn this code, we are running a regression model with three predictors (treatment group, treatment duration, and trust score) and the outcome variable aggression level.\n\n2\n\nWe then pass the regression model to the mctest() function to test for multicollinearity.\n\n\n\n\n\nCall:\nomcdiag(mod = mod, Inter = TRUE, detr = detr, red = red, conf = conf, \n    theil = theil, cn = cn)\n\n\nOverall Multicollinearity Diagnostics\n\n                       MC Results detection\nDeterminant |X'X|:         0.9229         0\nFarrar Chi-Square:         7.7960         0\nRed Indicator:             0.1547         0\nSum of Lambda Inverse:     3.1728         0\nTheil's Method:           -0.8800         0\nCondition Number:         13.6549         0\n\n1 --&gt; COLLINEARITY is detected by the test \n0 --&gt; COLLINEARITY is not detected by the test\n\n\nThe mctest() function also takes an additional argument, type, which specifies whether you want, the main tests, each individual predictor, or both. The default is type = \"main\", which will run the main tests.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multiple Regression and Heirarchical Regression</span>"
    ]
  },
  {
    "objectID": "multipleRegression.html#sample-size-for-multiple-regression",
    "href": "multipleRegression.html#sample-size-for-multiple-regression",
    "title": "8  Multiple Regression and Heirarchical Regression",
    "section": "8.3 Sample size for multiple regression",
    "text": "8.3 Sample size for multiple regression\nAs the number of predictors increases, the sample size needed to run a multiple regression analysis also increases. A common rule of thumb is to have at least 10-15 participants per predictor. However, this is a loose rule, and the actual number of participants needed will depend on the complexity of the model and the effect size. Always run a power analysis to determine the sample size needed for your study.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multiple Regression and Heirarchical Regression</span>"
    ]
  },
  {
    "objectID": "multipleRegression.html#approaches-to-multiple-regression-all-predictors-at-once",
    "href": "multipleRegression.html#approaches-to-multiple-regression-all-predictors-at-once",
    "title": "8  Multiple Regression and Heirarchical Regression",
    "section": "8.4 Approaches to multiple regression: All predictors at once",
    "text": "8.4 Approaches to multiple regression: All predictors at once\nThere are different ways we could run a multiple regression analysis depending on our research question and how we conceptualise the relationship between the predictors and the outcome. One way is to include all predictors at the same time. This is useful when we want to know the combined predictive power of all the predictors at the same time.\n\nResearch question: Do a client’s treatment duration and treatment group predict aggression level?\n\n\n1model1 &lt;- lm(data = regression_data, aggression_level ~ treatment_duration + treatment_group)\n\n\n1\n\nHere we are including all of the predictors at the same time. Note that we are using a plus sign + between each predictor - this means that no interactions will be tested.\n\n\n\n\n\n8.4.1 Using categorical predictors in R\n\nTreatment group is a categorical (also called “nominal” or “factor”) variable\nNo special “dummy coding” is required in R to use categorical predictors in regression\nR will use the first group as the reference category and test whether being in another group shows a significant difference\nR chooses the reference group based on numerical value or alphabetical order\nIf you want you can change the reference category or “force” it using the relevel function:\n\n\nregression_data$treatment_group &lt;- relevel(regression_data$treatment_group, ref = \"therapy1\")\n\nMore information in categorical predictors in section @ref(catreg)\n\n\n8.4.2 Reviewing the output\nThe output from this approach will look the same as simple regression, except there will be an additional row for each predictor in the model in the coefficients table.\n\nsummary(model1)\n\n\nCall:\nlm(formula = aggression_level ~ treatment_duration + treatment_group, \n    data = regression_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9468 -1.1104  0.0205  0.9621  3.4481 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             11.58713    0.77331  14.984  &lt; 2e-16 ***\ntreatment_duration      -0.66024    0.07119  -9.274 4.96e-15 ***\ntreatment_grouptherapy2  0.85032    0.30449   2.793   0.0063 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.5 on 97 degrees of freedom\nMultiple R-squared:  0.5206,    Adjusted R-squared:  0.5107 \nF-statistic: 52.67 on 2 and 97 DF,  p-value: 3.267e-16\n\n\nAs a reminder of what the output tells us:\n\nMultiple \\(R^2\\) = Total variance in outcome that is explained by the model\np-value = Statistical significance of the model\nCoefficients = Contribution of each predictor to the model\n\nPr = Significance of the individual predictor\nEstimate = Change in the outcome level that occurs when the predictor increases by 1 unit of measurement (for continuous predictors) or the difference in the mean outcome level between the predictor and the reference category (for categorical predictors)\n\n\n\n\n8.4.3 All predictors at once (testing interactions)\nIt might be the case that we are interested in whether the predictors interact with each other to predict the outcome. For example, we might want to know if the effect of treatment duration on aggression level depends on the treatment group.\n\nResearch questions:\n\n\nDo a client’s treatment duration and treatment group predict aggression level\nDo the predictors interact?\n\n\n1model2 &lt;- lm(data = regression_data, aggression_level ~ treatment_duration * treatment_group)\n\n\n1\n\nHere we are including all of the predictors at the same time. Note that we are using an asterisk * between each predictor. This means that interactions will be tested.\n\n\n\n\nReviewing the output\n\n\nsummary(model2) %&gt;% coefficients\n\n                                             Estimate Std. Error    t value\n(Intercept)                                12.3529190  1.1006127 11.2236751\ntreatment_duration                         -0.7334435  0.1033086 -7.0995381\ntreatment_grouptherapy2                    -0.5615517  1.4753596 -0.3806202\ntreatment_duration:treatment_grouptherapy2  0.1394649  0.1425977  0.9780305\n                                               Pr(&gt;|t|)\n(Intercept)                                3.599000e-19\ntreatment_duration                         2.166226e-10\ntreatment_grouptherapy2                    7.043260e-01\ntreatment_duration:treatment_grouptherapy2 3.305175e-01\n\n\n\nIn the output, we get additional information in the coefficients table about the interaction between variables. We can see from the output that none of the interactions are significant. This being the case, we would not interpret the main effects of the predictors in the model, and instead, we would interpret the results based on the interaction between the predictors.\n\nThere is more information on interactions in regression in the section on moderation.\n\n\n\n8.4.4 Hierarchical multiple regression: Theory driven “blocks” of variables\nWhen we have multiple predictors, it might be the case that we have previous research or theory to guide how we run the analysis. For example, we might know that treatment duration and therapy group are likely to predict the outcome, based on the results of previous studies. However, we might also want to check whether client’s level of trust in the clinician has any additional impact on our ability to predict the outcome (aggression level).\nThis being the case, we could run a hierarchical multiple regression analysis. This is where we run the regression in “blocks” (groups) of variables. We start with a baseline model (Model 0) and then add additional predictors in each subsequent model.\nHow we group our predictors will depend on our research question and the theory behind the relationship between the predictors and the outcome. As such, we should have a clear rationale for how we group our predictors.\nTo do this, we run three regression models:\n\nModel 0: the constant (intercept)\nModel 1: treatment duration and therapy group\nModel 2: treatment duration and therapy group and trust score\n\nWe then compare the two regression models to see if:\n\nModel 1 is better than Model 0 (the intercept)\nModel 2 is better than Model 1\n\nThe intercept (null) model is the simplest model we can run. It is a model that only includes the constant (intercept) and no predictors. This model is used as a baseline to compare the other models to. Therefore, we don’t really interpret the null model in isolation, rather, we examine whether another model (with predictors) is more useful in predicting the outcome than the null model.\nHierarchical multiple regression: Running and comparing 2 models\n\n## run regression using the same method as above\n1model0 &lt;- lm(data = regression_data, aggression_level ~ 1)\n2model1 &lt;- lm(data = regression_data, aggression_level ~ treatment_duration + treatment_group)\n3model2 &lt;- lm(data = regression_data, aggression_level ~ treatment_duration + treatment_group + trust_score)\n\n\n## use the aov() command to compare the models\n4anova(model0,model1,model2)\n\n\n1\n\nHere we are running the null model (Model 0) with only the intercept\n\n2\n\nHere we are running Model 1 with treatment duration and treatment group\n\n3\n\nHere we are running Model 2 with treatment duration, treatment group, and trust score\n\n4\n\nWe then use the anova() command to compare the models\n\n\n\n\nAnalysis of Variance Table\n\nModel 1: aggression_level ~ 1\nModel 2: aggression_level ~ treatment_duration + treatment_group\nModel 3: aggression_level ~ treatment_duration + treatment_group + trust_score\n  Res.Df    RSS Df Sum of Sq       F    Pr(&gt;F)    \n1     99 455.27                                   \n2     97 218.26  2   237.013 52.2195 4.507e-16 ***\n3     96 217.86  1     0.399  0.1757     0.676    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWhen we run the anova() command, we get an output that tells us whether the additional predictors in each model significantly change the R^2 value of the model (reducing the residual variance of the model). We can see from the output that Model 1 is significantly better than the null model (Model 0), and Model 2 is not significantly better than Model 1.\n\n\n8.4.5 Model performance metrics\nWhen we run a hierarchical multiple regression analysis, we are interested in whether the additional predictors in each model significantly change the R^2 value of the model. However, adding additional predictors to the model will always increase the R^2 value of the model somewhat, even if the predictors are not useful This is because the R^2 value is based on the amount of variance in the outcome that is explained by the predictors in the model.\nBecause of this, we should also consider other metrics of model performance, such as the AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion). These metrics take into account the number of predictors in the model and penalise the model for having too many predictors. A lower AIC or BIC value indicates a better model. The AIC() and BIC() functions in R can be used to calculate these metrics.\n\n1AIC(model0,model1,model2)\n\n2BIC(model0,model1,model2)\n\n\n1\n\nHere we are using the AIC() function to calculate the AIC value for each model\n\n2\n\nHere we are using the BIC() function to calculate the BIC value for each model\n\n\n\n\n       df      AIC\nmodel0  2 439.3603\nmodel1  4 369.8394\nmodel2  5 371.6566\n       df      BIC\nmodel0  2 444.5707\nmodel1  4 380.2601\nmodel2  5 384.6825\n\n\nWe can see from the output that Model 1 has the lowest AIC and BIC values, indicating that it is the best model of the three. We phrase this has having the best fit to the data, given the number of predictors in the model. Although, in this example, model 2 is not significantly better than model 1, it could be the case that you have a different research question or theory where the anova() test would show that the model with the additional predictors is significantly better. You could then use the AIC and BIC values to determine which model is actually the best fit to the data.\nRemember that the accuracy of your models will depend on the quality of your data and the assumptions of regression being met. How you interpret the results will depend on your research question and the theory behind the relationship between the predictors and the outcome. The analysis cannot prove causation, only association. It is up to you to design your study and analysis to best answer your research question.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multiple Regression and Heirarchical Regression</span>"
    ]
  },
  {
    "objectID": "moderation.html",
    "href": "moderation.html",
    "title": "9  Moderation",
    "section": "",
    "text": "9.1 What is moderation?\nTheoretically, we can think of moderation as a relationship between a predictor (X) and an outcome (Y) that is affected by a third variable (M). This means that the relationship between X and Y exists, but the strength of this relationship is affected by the level of M. We could also phrase this as the relationship between X and Y being different, depending on the level of M.\nA moderated relationship can be visualised as follows:\nIn the above model, we theorise that Time in counselling predicts General Wellbeing but the strength of the relationship is affected by the level of Rapport with psychologist. As with all of our hypotheses, the moderated relationship is theorised based on previous research evidence and/or strong theoretical reasoning.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Moderation</span>"
    ]
  },
  {
    "objectID": "moderation.html#what-is-moderation",
    "href": "moderation.html#what-is-moderation",
    "title": "9  Moderation",
    "section": "",
    "text": "A moderated relationship",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Moderation</span>"
    ]
  },
  {
    "objectID": "moderation.html#what-packages-do-we-need",
    "href": "moderation.html#what-packages-do-we-need",
    "title": "9  Moderation",
    "section": "9.2 What packages do we need?",
    "text": "9.2 What packages do we need?\nWe can do a lot of the moderation analysis with just the lm() function in R. However, there are some packages that can make the process easier and help us understand the moderation better. These include:\n\ngvlma (for checking assumptions)\ninteractions (for generating interaction plot)\nRockchalk (for testing simple slopes)\ncar (includes a Boot() function to bootstrap regression models )",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Moderation</span>"
    ]
  },
  {
    "objectID": "moderation.html#moderation-as-an-interaction",
    "href": "moderation.html#moderation-as-an-interaction",
    "title": "9  Moderation",
    "section": "9.3 Moderation as an interaction",
    "text": "9.3 Moderation as an interaction\nWhen we say that a variable moderates the relationship between two other variables, we are saying that the relationship between the two variables is not the same at different levels of the moderator. This is similar to an interaction in standard regression analysis. In fact, we can tell if there is a moderation effect by looking at the interaction term in a regression model.\nlm(Y ~ X + M + X*M)\nIn the above formula, the interaction term is X*M. This term is the product of the two variables (X and M). If the interaction term is significant, this suggests that there is a moderation effect.\n\n\n\nA moderated relationship\n\n\nIf the interaction term is significant, this suggests that there is a moderation effect.\nHowever, a moderator can effect the direction and/or strength of a relationship between X and Y. The interaction effect might only be significant when the moderator is at a certain level. The below plot shows a moderated relationship:\n\n\n\n\n\n\n\n\n\nIn the plot above:\n\nThe blue line is the “standard” regression line\nThe black line is when the moderator is “low” (-1sd)\nThe dotted line is when the moderator is “high” (+1sd)\n\nTo understand the moderation effect fully, we need some additional analysis.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Moderation</span>"
    ]
  },
  {
    "objectID": "moderation.html#moderation-step-by-step",
    "href": "moderation.html#moderation-step-by-step",
    "title": "9  Moderation",
    "section": "9.4 Moderation: step-by-step",
    "text": "9.4 Moderation: step-by-step\n\n9.4.1 Run your model\nAs with our previous models, we start by running a regression model. In this case, we are interested in the interaction between our predictor and moderator. The model is named fitMod and is run using the lm() function. The name is not important, but it is good practice to name your models in a way that makes sense to you.\n\nfitMod &lt;- lm(generalWellbeing ~ timeInCounselling *rapportLevel , data = Moddata) #Model interacts IV & moderator\n\n\n\n\n\n\n\nTipGrand Mean Centering\n\n\n\nIn many moderation models, people use grand mean centering. This is a way to make the results of the model easier to interpret. Especially if you have a continuous moderator, it can be helpful to center the data around the grand mean. This means that the mean of the data is subtracted from each value. This makes the mean of the data 0.\nThis is becuase regression coefficients (b values) are based on predicting Y when X = 0, but not all measures actually have a zero value. To make results easier to interpret, we can centre our data around the grand mean of the data (making the mean 0). The mean of the full sample is subtracted from the value. This is similar to z-score (i.e. a standardised score)\nTo do this in R, we can use the scale() function:\n\n    timeInCounselling_centred    &lt;- scale(timeInCounselling, center=TRUE, scale=FALSE) #Centering X; \n    rapportLevel_centred    &lt;- scale(rapportLevel,  center=TRUE, scale=FALSE) #Centering M;\n\nWe then use the centred data in our analysis.\nWe can see that the difference between the original data is the mean of the data.\n\n  timeInCounselling_centred    &lt;- scale(timeInCounselling, center=TRUE, scale=FALSE) #Centering X; \n  \n  timeInCounselling\n\n  [1]  3.7580974  5.0792900 12.2348333  6.2820336  6.5171509 12.8602599\n  [7]  7.8436648  0.9397551  3.2525886  4.2173521 10.8963272  7.4392553\n [13]  7.6030858  6.4427309  3.7766355 13.1476525  7.9914019  1.8664686\n [19]  8.8054236  4.1088344  1.7287052  5.1281003  1.8959822  3.0844351\n [25]  3.4998429  0.7467732  9.3511482  6.6134925  1.4474523 11.0152597\n [31]  7.7058569  4.8197141  9.5805026  9.5125340  9.2863243  8.7545610\n [37]  8.2156706  5.7523532  4.7761493  4.4781160  3.2211721  5.1683309\n [43]  0.9384146 14.6758239 10.8318480  1.5075657  4.3884607  4.1333786\n [49]  9.1198605  5.6665237  7.0132741  5.8858130  5.8285182 11.4744091\n [55]  5.0969161 12.0658824  0.1950112  8.3384550  6.4954170  6.8637663\n [61]  7.5185579  3.9907062  4.6671705  1.9256985  1.7128351  7.2141146\n [67]  7.7928391  6.2120169  9.6890699 14.2003387  4.0358753  3.2366755\n [73] 10.0229541  3.1631969  3.2479655 10.1022855  4.8609080  1.1171292\n [79]  6.7252139  5.4444346  6.0230567  7.5411216  4.5173599  8.5775062\n [85]  5.1180538  7.3271279 10.3873561  7.7407260  4.6962737 10.5952305\n [91]  9.9740154  8.1935878  6.9549269  3.4883757 11.4426098  3.5989617\n [97] 14.7493320 12.1304425  5.0571986  1.8943164\n\n  head(timeInCounselling_centred)\n\n            [,1]\n[1,] -2.72442479\n[2,] -1.40323216\n[3,]  5.75231105\n[4,] -0.20048864\n[5,]  0.03462873\n[6,]  6.37773774\n\n   mean(timeInCounselling)\n\n[1] 6.482522\n\n  timeInCounselling[1]-timeInCounselling_centred[1]\n\n[1] 6.482522\n\n\n\n#Centering Data\nModdata$timeInCounselling_centred    &lt;- c(scale(timeInCounselling, center=TRUE, scale=FALSE)) \n\n#Centering IV; \nModdata$rapportLevel_centred    &lt;- c(scale(rapportLevel,  center=TRUE, scale=FALSE)) #Centering moderator; \n\n#Moderation \"By Hand\" with centred data\nlibrary(gvlma)\nfitMod &lt;- lm(generalWellbeing ~ timeInCounselling_centred *rapportLevel_centred  , data = Moddata) #Model interacts IV & moderator\n\nlibrary(interactions)\n ip &lt;- interact_plot(fitMod, pred = timeInCounselling_centred, modx = rapportLevel_centred)\n ip\n\n\n\n\n\n\n\n\n\n9.4.1.1 Do I need to mean centre my data?\nIt’s really about ease of interpretation. It is worth noting:\n\nIt does not change the results of your interaction (coefficient, standard error or significance tests).\nIt will change the results of the direct effects (the individual predictors in your model).\nIt is a step that tries to ensure that the coefficients of the predictor and moderator are meaningful to interpret, in relation to each other.\nIn some cases, it might not be necessary to mean centre at all. However, there is no harm in doing so, and it could potentially be helpful.\n\nHayes (2013) discusses mean centering, pp. 282-290.\n\n\n\n\n\n9.4.2 Step 2: Check assumptions\nWe can use the gvlma function to check regression assumptions for moderation analysis.\n\nlibrary(gvlma)\ngvlma(fitMod)\n\n\nCall:\nlm(formula = generalWellbeing ~ timeInCounselling_centred * rapportLevel_centred, \n    data = Moddata)\n\nCoefficients:\n                                   (Intercept)  \n                                       21.1851  \n                     timeInCounselling_centred  \n                                        0.8971  \n                          rapportLevel_centred  \n                                        0.5842  \ntimeInCounselling_centred:rapportLevel_centred  \n                                        0.1495  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = fitMod) \n\n                    Value p-value                   Decision\nGlobal Stat        9.6949 0.04589 Assumptions NOT satisfied!\nSkewness           7.7571 0.00535 Assumptions NOT satisfied!\nKurtosis           1.2182 0.26972    Assumptions acceptable.\nLink Function      0.5287 0.46716    Assumptions acceptable.\nHeteroscedasticity 0.1910 0.66207    Assumptions acceptable.\n\n\nThe “global stat” is an attempt to check multiple assumptions of linear model: Pena, E. A., & Slate, E. H. (2006). Global validation of linear model assumptions. Journal of the American Statistical Association, 101(473), 341-354.\nSince one of the underlying assumptions is violated, the overall stat is also not acceptable.\nThe data looks skewed, we should transform it or perhaps use bootstrapping (we will do this later).\n\n\n9.4.3 Multicollinearity?\nFor more information on multicollinearity in moderation, see Clelland, G. H., Irwin, J. R., Disatnik, D., & Sivan, L. (2017). Multicollinearity is a red herring in the search for moderator variables: A guide to interpreting moderated multiple regression models and a critique of Iacobucci, Schneider, Popovich, and Bakamitsos (2016). Behavior research methods, 49(1), 394-402.\n\n\n9.4.4 Step 3: Moderation Analysis\nNow that we have run our model and checked the assumptions, we can interpret the results. So we run the summary() function on our model.\n\nfitMod &lt;- lm(generalWellbeing ~ timeInCounselling_centred *rapportLevel_centred  , data = Moddata) #Model interacts IV & moderator\n #Model interacts IV & moderator\nsummary(fitMod)\n\n\nCall:\nlm(formula = generalWellbeing ~ timeInCounselling_centred * rapportLevel_centred, \n    data = Moddata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.121  -8.938  -0.670   5.840  37.396 \n\nCoefficients:\n                                               Estimate Std. Error t value\n(Intercept)                                    21.18508    1.14115  18.565\ntimeInCounselling_centred                       0.89707    0.33927   2.644\nrapportLevel_centred                            0.58416    0.15117   3.864\ntimeInCounselling_centred:rapportLevel_centred  0.14948    0.04022   3.716\n                                               Pr(&gt;|t|)    \n(Intercept)                                     &lt; 2e-16 ***\ntimeInCounselling_centred                      0.009569 ** \nrapportLevel_centred                           0.000203 ***\ntimeInCounselling_centred:rapportLevel_centred 0.000340 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.33 on 96 degrees of freedom\nMultiple R-squared:  0.2737,    Adjusted R-squared:  0.251 \nF-statistic: 12.06 on 3 and 96 DF,  p-value: 9.12e-07\n\n\nThe results above show that there is a moderated effect (the interaction term is significant). When the interaction term is significant, this suggests that there is a moderation effect (i.e. the relationship between the predictor and outcome is different at different levels of the moderator). So we do not need to interpret the main effects (timeInCounselling and rapportLevel).\n\n9.4.4.1 Visualising the moderation effect\nWe use an approach called simple slopes to visualise the moderation effect. This is a way to see how the relationship between the predictor and outcome changes at different levels of the moderator.\ninteract_plot(fitMod, pred = timeInCounselling_centred, modx = rapportLevel_centred)\n\n\n\n\n\n\n\n\n\nIn the case of the above plot, we can see that the relationship between timeInCounselling and generalWellbeing is stronger when rapportLevel is high (+1sd) compared to when it is low (-1sd).\nThe rockchalk package includes useful functions for visualising simple slopes\n\nlibrary(rockchalk)\n\nfitMod &lt;- lm(generalWellbeing ~ timeInCounselling *rapportLevel  , data = Moddata)\nsummary(fitMod)\n\n\nslopes &lt;- plotSlopes(fitMod, modx = \"rapportLevel\", plotx = \"timeInCounselling\")\n\n\n2\n\nThe testSlopes() function tests the simple slopes. This is to see if the slopes are significant.\n\n3\n\nThe plot() function is used to plot the results of the simple slopes test. This tells us at which level of the moderator the slopes are significant.\n\n\n\n\n\n\n\n\n\n\n2testSlopes &lt;- testSlopes(slopes)\n3plot(testSlopes)\n\n\n\n\n\n\n\n\n\nCall:\nlm(formula = generalWellbeing ~ timeInCounselling * rapportLevel, \n    data = Moddata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.121  -8.938  -0.670   5.840  37.396 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                    17.28006    3.17944   5.435 4.15e-07 ***\ntimeInCounselling               0.15510    0.42033   0.369  0.71296    \nrapportLevel                   -0.38484    0.29916  -1.286  0.20140    \ntimeInCounselling:rapportLevel  0.14948    0.04022   3.716  0.00034 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.33 on 96 degrees of freedom\nMultiple R-squared:  0.2737,    Adjusted R-squared:  0.251 \nF-statistic: 12.06 on 3 and 96 DF,  p-value: 9.12e-07\n\nValues of rapportLevel OUTSIDE this interval:\n        lo         hi \n-11.580166   3.634439 \ncause the slope of (b1 + b2*rapportLevel)timeInCounselling to be statistically significant\n\n\nIn the example above, we can see that the slope of the relationship between timeInCounselling and generalWellbeing is significant when the moderator is between -11.58 and 3.63. We can look at the simple slopes plot to see how the relationship changes at these levels. We can also check whether -11.58 and 3.63 are within the range of the moderator variable (rapportLevel) - if this is not the case, we only consider the values that are within the possible range of the moderator variable.\n\n\n\n9.4.5 Step 4: Bootstrapping\nWe saw in the regression diagnostics that the data was skewed. We can use bootstrapping to get a more accurate estimate of the confidence intervals. The car package includes a function to bootstrap any regression\n\nlibrary(car)\n\n\n1\n\nThe Boot() function is used to bootstrap the model. We save the results as an object called bootstrapModel.\n\n2\n\nWe can use the confint() function to get the confidence intervals of the original model.\n\n3\n\nWe can use the confint() function to get the confidence intervals of the bootstrapped model.\n\n4\n\nWe can use the summary() function to get a summary of the bootstrapped model.\n\n5\n\nWe can use the hist() function to plot a histogram of the bootstrapped model.\n\n\n\n\nLoading required package: carData\n\n1bootstrapModel &lt;- Boot(fitMod, R=999)\n\n2confint(fitMod)\n3confint(bootstrapModel)\n4summary(bootstrapModel)\n5hist(bootstrapModel)\n\n\n\n\n\n\n\n\n                                     2.5 %     97.5 %\n(Intercept)                    10.96891826 23.5912086\ntimeInCounselling              -0.67926290  0.9894532\nrapportLevel                   -0.97866229  0.2089882\ntimeInCounselling:rapportLevel  0.06963667  0.2293205\nBootstrap bca confidence intervals\n\n                                     2.5 %     97.5 %\n(Intercept)                    11.57230420 23.7222700\ntimeInCounselling              -0.61780918  1.0397199\nrapportLevel                   -0.90786799  0.2558502\ntimeInCounselling:rapportLevel  0.05806412  0.2146814\n\nNumber of bootstrap replications R = 999 \n                               original    bootBias   bootSE  bootMed\n(Intercept)                    17.28006 -0.13667103 3.165301 17.05431\ntimeInCounselling               0.15510  0.01637117 0.399550  0.15929\nrapportLevel                   -0.38484  0.00716631 0.294061 -0.38218\ntimeInCounselling:rapportLevel  0.14948 -0.00052838 0.038516  0.14974\n\n\nComparing the confidence intervals of the original model and the bootstrapped model can give us an idea of how accurate our original model is. If the confidence intervals are very different, this suggests that the original model is not very accurate. In this case, we might want to use the bootstrapped model instead.\nThe bootBias and bootSE columns in the summary of the bootstrapped model can also give us an idea of how much the bootstrapped model differs from the original model. If the bootBias is very different from 0, this suggests that the original model is biased. If the bootSE is very different from the original SE, this suggests that the original model is not very accurate.\nIn the data above, we can see that the bootBias and bootSE are very close to 0 and the original SE, respectively. This suggests that the deviation from normality is not a big problem in this case.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Moderation</span>"
    ]
  },
  {
    "objectID": "mediation.html",
    "href": "mediation.html",
    "title": "10  Mediation analysis",
    "section": "",
    "text": "10.1 Overview\nThis example borrows heavily from Demos & Salas (2019). A Language, not a Letter: Learning Statistics in R (Chapter 14)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Mediation analysis</span>"
    ]
  },
  {
    "objectID": "mediation.html#overview",
    "href": "mediation.html#overview",
    "title": "10  Mediation analysis",
    "section": "",
    "text": "TipAt the end of this chapter, you will be able to:\n\n\n\n\nExplain the difference between mediation and moderation.\nApply the Baron and Kenny approach to mediation analysis in R.\nApply the Preacher & Hayes (2004) mediation approach in R.\nInterpret the output of mediation analysis.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Mediation analysis</span>"
    ]
  },
  {
    "objectID": "mediation.html#what-is-mediation",
    "href": "mediation.html#what-is-mediation",
    "title": "10  Mediation analysis",
    "section": "10.2 What is mediation?",
    "text": "10.2 What is mediation?\nThe basic premise of mediation is that the relationship between a predictor (X) and an outcome (Y) is mediated by another variable (M). That is to say, X predicts M, which in turn predicts Y.\nWhen thinking about mediation, form a research perspective, we could be saying that the relationship between X and Y is explained by M. That is to say, M is the mechanism through which X influences Y.\nTo use an example, consider the relationship between internet usage and self-esteem. We might know from previous findings that internet usage predicts self-esteem. Perhaps higher internet usage is associated with lower self-esteem.\nHowever, we might theorise that the relationship between internet usage and self-esteem is mediated by social media usage. That is to say, internet usage predicts social media usage, which in turn predicts self-esteem. If this turns out to be the case, then we might say that social media usage is the mechanism through which internet usage influences self-esteem. That would mean that it is not internet usage per se that influences self-esteem, but rather the way in which people use the internet (i.e., through social media) that explains the relationship.\nImportantly, theoerising mediation (like all models) requires a strong rationale that is based on previous research evidence or a well-developed theoretical model. We need good reason to believe that the relationship between X and Y is mediated by M, we cannot simply test for mediation with no basis and use the result to justify a post facto theory.\n\nIn the above model, we theorise that the relationship between internet use and self-esteem is mediated by social media usage.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Mediation analysis</span>"
    ]
  },
  {
    "objectID": "mediation.html#what-is-the-difference-between-mediation-and-moderation",
    "href": "mediation.html#what-is-the-difference-between-mediation-and-moderation",
    "title": "10  Mediation analysis",
    "section": "10.3 What is the difference between mediation and moderation?",
    "text": "10.3 What is the difference between mediation and moderation?\nThe key difference between mediation and moderation is that mediation is about the third variable (M) explaining the relationship between X and Y, whereas moderation is about the third variable (M) changing the relationship between X and Y.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Mediation analysis</span>"
    ]
  },
  {
    "objectID": "mediation.html#mediation-analysis",
    "href": "mediation.html#mediation-analysis",
    "title": "10  Mediation analysis",
    "section": "10.4 Mediation analysis",
    "text": "10.4 Mediation analysis\nMediation analysis is based on regression analysis. In the past it was tested using the Baron and Kenny (1986) approach, but more recently the Preacher & Hayes (2004) bootstrapping approach has become more popular.\nBaron and Kenny (1986) originally used a 4-step regression model to test each of these relationships. The Sobel test is then used to test the significance of mediation.\nA summary of the logic of mediation (mirroring the Baron and Kenny approach) is as follows:\n\nThe direct relationship between X and Y should be significant\nThe relationship between X and M should be significant\nThe relationship between M and Y (controlling for X) should be significant\nWhen controlling for M, the strength of the relationship between X and Y decreases and is not significant\n\n\n10.4.1 What packages do we need?\n\n    library(mediation) #Mediation package\n    \n    library(multilevel) #Sobel Test\n    \n    library(bda) #Another Sobel Test option\n    \n    library(gvlma) #Testing Model Assumptions \n    \n    library(stargazer) #Handy regression tables",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Mediation analysis</span>"
    ]
  },
  {
    "objectID": "mediation.html#mediation-analysis-the-baron-and-kenny-approach",
    "href": "mediation.html#mediation-analysis-the-baron-and-kenny-approach",
    "title": "10  Mediation analysis",
    "section": "10.5 Mediation analysis (the Baron and Kenny Approach)",
    "text": "10.5 Mediation analysis (the Baron and Kenny Approach)\n\n10.5.1 Conducting mediation analysis (the Baron and Kenny Approach)\nBaron & Kenny (1986) originally used a 4-step regression model to test each of these relationships. The sobel test is then used to test the significance of mediation. The idea with this approach is that you can test each of the paths in the mediation model to see if they are significant. If they are, you can then test the significance of the indirect effect.\nA crucial part of this process is the idea that the significance of the total effect is no longer present when the mediator is included in the model. This is because the mediator is now explaining the relationship between the IV and DV.\nThis highlights one of the issues with this approach: looking for a lack of significance as indirect evidence of an effect. In addition, the assumption that the total effect is no longer significant when the mediator is included does not always hold true.\nAnother issue with this approach is that it does not test the significance of the indirect effect. This is where the Sobel test comes in. The Sobel test checks the significance of the indirect effect. However, the Sobel test is less reliable with smaller sample sizes and when the data is not normally distributed.\nNevertheless, it is still a useful approach to understand the logic of mediation analysis and to be able to interpret the output of mediation analysis in published papers.\nFor this example, we will use variables named X, M, and Y. This is just for the purposes of this example. In your own research, you would replace these with your own variables. Similarly, the names of the models are named fit, fita, fitb, and fitc. These are just for the purposes of this example. In your own research, you can name these models however you like.\n\n\n10.5.2 Step 1: Total Effect\nThe first model we run is the total effect model. This model tests the relationship between the Precictor (X) and the Outcome (Y). This is before we include the mediator in the model. We need this model to be significant to move on to the next steps. If it is not, then we cannot test for mediation.\n\n#1. Total Effect\nfit &lt;- lm(Y ~ X, data=Meddata)\nsummary(fit)\n\n\nCall:\nlm(formula = Y ~ X, data = Meddata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-10.917  -3.738  -0.259   2.910  12.540 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 19.88368   14.26371   1.394   0.1665  \nX            0.16899    0.08116   2.082   0.0399 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.16 on 98 degrees of freedom\nMultiple R-squared:  0.04237,   Adjusted R-squared:  0.0326 \nF-statistic: 4.336 on 1 and 98 DF,  p-value: 0.03993\n\n\n\n\n10.5.3 Step 2: Path A (X on M)\nThe second model we run is the relationship between the Predictor (X) and the Mediator (M). This is to test if the IV predicts the mediator. This is important because if the IV does not predict the mediator, then the M variable cannot be a mediator.\n\n#2. Path A (X on M)\nfita &lt;- lm(M ~ X, data=Meddata)\nsummary(fita)\n\n\nCall:\nlm(formula = M ~ X, data = Meddata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5367 -3.4175 -0.4375  2.9032 16.4520 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.04494   13.41692   0.451    0.653    \nX            0.66252    0.07634   8.678 8.87e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.854 on 98 degrees of freedom\nMultiple R-squared:  0.4346,    Adjusted R-squared:  0.4288 \nF-statistic: 75.31 on 1 and 98 DF,  p-value: 8.872e-14\n\n\n\n\n10.5.4 Step 3: Path B (M on Y, controlling for X)\nThe third model we run is the relationship between the Mediator (M) and the Outcome (Y), controlling for the Predictor (X). This is to test if the M variable predicts the Outcome, controlling for the Predictor. This is important because if the M variable does not predict the Outcome, then it cannot be a mediator.\n\n#3. Path B (M on Y, controlling for X)\nfitb &lt;- lm(Y ~ M + X, data=Meddata)\nsummary(fitb)\n\n\nCall:\nlm(formula = Y ~ M + X, data = Meddata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3651 -3.3037 -0.6222  3.1068 10.3991 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.32177   13.16216   1.316    0.191    \nM            0.42381    0.09899   4.281 4.37e-05 ***\nX           -0.11179    0.09949  -1.124    0.264    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.756 on 97 degrees of freedom\nMultiple R-squared:  0.1946,    Adjusted R-squared:  0.1779 \nF-statistic: 11.72 on 2 and 97 DF,  p-value: 2.771e-05\n\n\n\n\n10.5.5 Step 4: Reversed Path C (Y on X, controlling for M)\nThe fourth model we run is the relationship between the Outcome (Y) and the Predictor (X), controlling for the Mediator (M). This is to test if the relationship between the Predictor and the Outcome is no longer significant when the Mediator is included in the model. This is important because if the relationship between the Predictor and the Outcome is still significant when the Mediator is included, then the Mediator is not mediating the relationship.\nAt least, this is the logic of the Baron and Kenny approach. However, as we mentioned earlier, this assumption does not always hold true. The significance of the total effect is not always lost when the mediator is included in the model - it could be that the mediator is not a full mediator, but a partial mediator, for example.\n\n#4. Reversed Path C (Y on X, controlling for M)\nfitc &lt;- lm(X ~ Y + M, data=Meddata)\nsummary(fitc)\n\n\nCall:\nlm(formula = X ~ Y + M, data = Meddata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.438  -2.573  -0.030   3.010  11.779 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 96.11234    9.27663  10.361  &lt; 2e-16 ***\nY           -0.11493    0.10229  -1.124    0.264    \nM            0.69619    0.08356   8.332 5.27e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.823 on 97 degrees of freedom\nMultiple R-squared:  0.4418,    Adjusted R-squared:  0.4303 \nF-statistic: 38.39 on 2 and 97 DF,  p-value: 5.233e-13\n\n\n\n\n10.5.6 Viewing output\nWe can use the stargazer package to view the output of the models in a nice table. This is optional.\nSummary Table\nstargazer(fit, fita, fitb, fitc, type = \"text\", title = \"Baron and Kenny Method\")\n\n\n\n\n\n\n\n\n\n\n\n10.5.7 Interpreting Baron and Kenny approach\nA reminder of the logic of mediation:\n\nThe direct relationship between X and Y should be significant\nThe relationship between X and M should be significant\nThe relationship between M and Y (controlling for X) should be significant\nWhen controlling for M, the strength of the relationship between X and Y decreases and is not significant\n\nFrom the output of the models, we can see that:\n\nThe total effect is significant (model 1)\nThe relationship between X and M is significant (model 2)\nThe relationship between M and Y is significant (model 3)\nThe relationship between X and Y is no longer significant when M is included in the model (model 4)\n\nThis suggests that the relationship between X and Y is mediated by M.\n\n\n10.5.8 Running the Sobel test\nHowever, the lack of significance of the total effect when the mediator is included in the model is not enough to test for mediation. We need to test the significance of the indirect (mediated) effect. This is where the Sobel test comes in. The Sobel test checks the significance of the mediated effect.\n\nlibrary(bda)\n\nLoading required package: boot\n\n\nbda - 19.0.0\n\nmediation.test(Meddata$M, Meddata$X, Meddata$Y)\n\n               Sobel       Aroian      Goodman\nz.value 3.8393902040 3.8190525305 3.8600562907\np.value 0.0001233403 0.0001339652 0.0001133609\n\n\nThe output shows that the p value of the Sobel test is significant. This suggests that the relationship between X and Y is mediated by M.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Mediation analysis</span>"
    ]
  },
  {
    "objectID": "mediation.html#mediation-analysis-the-mediation-package",
    "href": "mediation.html#mediation-analysis-the-mediation-package",
    "title": "10  Mediation analysis",
    "section": "10.6 Mediation analysis (the Mediation package)",
    "text": "10.6 Mediation analysis (the Mediation package)\n\n10.6.1 Preacher & Hayes (2004) mediation approach\nThe Mediation package in R uses the Preacher & Hayes (2004) bootstrapping approach. They argue that in practice, when using the Baron and Kenny approach, few people actually test the significance of the indirect effect. Instead, they simply look for a lack of significance of the total effect when the mediator is included in the model.\n\n\n“Baron and Kenny simply state that perfect mediation has occurred if c’ becomes nonsignificant after controlling for M, so researchers have focused on that requirement.” (Preacher & Hayes, 2004, p. 719)\n\n\nMoreover, they argue that the Sobel test is not reliable with smaller sample sizes and when the data is not normally distributed. The bootstrapping approach is more reliable in these cases.\nIn practice, the newer approach is also simpler. The mediation package in R allows you to run the mediation analysis in fewer steps and gives you more information.\n\n\n10.6.2 What is bootstrapping?\nTheir approach uses bootstrapping to estimate the indirect effect.\n\n\n“Bootstrapping is a nonparametric approach to effect-size estimation and hypothesis testing that makes no assumptions about the shape of the distributions of the variables or the sampling distribution of the statistic” (Preacer & Hayes, 2004, p. 722)\n\n\nBootstrapping takes a large number of samples from our data and runs the analysis on each of these samples. The sampling is done randomly with replacement, and each sample in the bootstrap is the same size as our dataset. Using this method, we can create estimates with that fall within a narrower confidence interval (since we have now run the analysis on 100’s of samples). Bootstrapping overcomes concerns about the distribution of our original dataset.\nBootstrapping is not confined to mediation analysis. It is a useful tool in many analysis approaches.\n\n\n10.6.3 Mediation example\nIs the relationship between Internet Use and Self-esteem mediated by Social Media Usage?\n\n\n\n10.6.4 Step 1: Run the models\nUsing this approach, we only need to run two models, rather than four. They are as follows:\n\n#Mediate package\nlibrary(mediation)\n\n1fitM &lt;- lm(M ~ X,     data=Meddata)\n2fitY &lt;- lm(Y ~ X + M, data=Meddata)\n\n\n1\n\nThe first model is the relationship between the Predictor (X) and the Mediator (M). This is to test if the IV predicts the mediator. This is important because if the IV does not predict the mediator, then the M variable cannot be a mediator.\n\n2\n\nThe second model is the relationship between the Predictor (X) and the Outcome (Y), controlling for the Mediator (M). This is to test if the relationship between the Predictor and the Outcome is no longer significant when the Mediator is included in the model. This is important because if the relationship between the Predictor and the Outcome is still significant when the Mediator is included, then the Mediator is not mediating the relationship.\n\n\n\n\nYou can see that these models are the same as the first and fourth models in the Baron and Kenny approach.\n\n\n10.6.5 Step 2: Check assumptions\nRemember that bootstrapping is a non-parametric approach. This means that it makes no assumptions about the shape of the distributions of the variables or the sampling distribution of the statistic. However, it is still important to check other assumptions of the model, such as linearity. You can use the gvlma function or the plot function to check these assumptions.\n\ngvlma(fitM) \n\n\nCall:\nlm(formula = M ~ X, data = Meddata)\n\nCoefficients:\n(Intercept)            X  \n     6.0449       0.6625  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = fitM) \n\n                   Value p-value                   Decision\nGlobal Stat        8.833 0.06542    Assumptions acceptable.\nSkewness           6.314 0.01198 Assumptions NOT satisfied!\nKurtosis           1.219 0.26949    Assumptions acceptable.\nLink Function      1.076 0.29959    Assumptions acceptable.\nHeteroscedasticity 0.223 0.63674    Assumptions acceptable.\n\ngvlma(fitY)\n\n\nCall:\nlm(formula = Y ~ X + M, data = Meddata)\n\nCoefficients:\n(Intercept)            X            M  \n    17.3218      -0.1118       0.4238  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = fitY) \n\n                     Value p-value                Decision\nGlobal Stat        3.41844  0.4904 Assumptions acceptable.\nSkewness           1.85648  0.1730 Assumptions acceptable.\nKurtosis           0.77788  0.3778 Assumptions acceptable.\nLink Function      0.71512  0.3977 Assumptions acceptable.\nHeteroscedasticity 0.06896  0.7929 Assumptions acceptable.\n\n\nWe can see that there are issues with normality of data, but because we are bootstrapping, this is not a problem.\n\n\n10.6.6 Step 3.1: Run the mediation analysis on the models\nThe mediate function will run the mediation analysis on the models we have created. It will give us the following estimates:\n\nAverage Causal Mediation Effects (ACME)\nAverage Direct Effects (ADE)\ncombined indirect and direct effects (Total Effect)\nthe ratio of these estimates (Prop. Mediated).\n\nFirstly, we need the Total Effect to be significant. This is the relationship between X and Y (direct and indirect).\nSecondly, we need the ACME to be significant. This is the indirect effect of M (total effect - direct effect) and thus this value tells us if our mediation effect is significant.\nWe can look at the ADE to see if the relationship between X and Y is direct. If this is not significant, then the relationship between X and Y is mediated by M. If it is significant (and the ACME is also significant), then the relationship between X and Y is both direct and indirect (partial mediation).\nThe Prop Mediated value tells us the proportion of the total effect that is mediated by M. It is calculated by dividing the ACME by the Total Effect. The closer this value is to 1, the more of the total effect is mediated by M. We can usually read it like a percentage (e.g., 0.87 means 87% of the total effect is mediated). If your ACME is greater than 1, then it could be because the ACME and ADE are in opposite directions (i.e., one is positive and the other is negative). This would lead to a value more than 1 when calculating the proportion, but for practical purposes, we could interpret the ACME as if it were 1 (i.e., all of the effect is mediated by M).\n\nfitMed &lt;- mediate(fitM, fitY, treat=\"X\", mediator=\"M\")\nsummary(fitMed)\n\n\nCausal Mediation Analysis \n\nQuasi-Bayesian Confidence Intervals\n\n                Estimate 95% CI Lower 95% CI Upper p-value    \nACME            0.280779     0.143728     0.424325  &lt;2e-16 ***\nADE            -0.113330    -0.311648     0.086472   0.258    \nTotal Effect    0.167450     0.020768     0.335915   0.028 *  \nProp. Mediated  1.642775     0.563148     8.444815   0.028 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSample Size Used: 100 \n\n\nSimulations: 1000 \n\n\n\n\n10.6.7 Step 3.2: Plot the mediation analysis of the models\nWe can plot the mediation analysis to visualise the results. The plot below reiterates what was on the output of the summary. We can see that the confidence intervals of Total Effect and ACME are significant, but the confidence interval of ADE is not significant.\nTranslation:\n\nTotal effect is signficant: there is a relationship between X and Y (direct and indirect)\nADE is not significant: the relationship between X and Y is not direct\nACME is significant: the relationship between X and Y is mediated by M\n\n\nplot(fitMed)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Mediation analysis</span>"
    ]
  },
  {
    "objectID": "mediation.html#reporting-mediation-analysis",
    "href": "mediation.html#reporting-mediation-analysis",
    "title": "10  Mediation analysis",
    "section": "10.7 Reporting mediation analysis",
    "text": "10.7 Reporting mediation analysis\nWhen reporting mediation analysis, the indirect (ACME) result is most important. However you should make it clear that the total effect is significant. You could also report the Prop Mediated values.\nFor example (us:\n\nThe mediating effect was using a bootstrapping approach (1000 samples) with the mediation package (REF) in R. The relationship between Internet Use and Self-esteem was mediated by Social Media Usage (ACME = 0.28, 95% CI [0.14, 0.42]). The total effect was significant (0.167, 95% CI [0.02, 0.34]). The proportion mediated was 1, suggesting that all of the total effect of relationship between Internet Usage and Self-esteem was mediated by Social Media Usage.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Mediation analysis</span>"
    ]
  },
  {
    "objectID": "mediation.html#references",
    "href": "mediation.html#references",
    "title": "10  Mediation analysis",
    "section": "10.8 References",
    "text": "10.8 References\nDemos & Salas (2019). A Language, not a Letter: Learning Statistics in R (Chapter 14). https://ademos.people.uic.edu/ Accessed Jan 2020.\nPreacher, K. J., & Hayes, A. F. (2004). SPSS and SAS procedures for estimating indirect effects in simple mediation models. Behavior research methods, instruments, & computers, 36(4), 717-731.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Mediation analysis</span>"
    ]
  },
  {
    "objectID": "ggplot.html",
    "href": "ggplot.html",
    "title": "11  Creating plots with ggplot2 in R",
    "section": "",
    "text": "11.1 The “grammar of visualisation” with ggplot\nWith ggplot, graphs are made up of 3 components: - A dataset - A coordinate system (e.g., the X and Y axes) - Visual marks to represent data (geoms)\nIn the above example, the dataset is the studentData that we used previously. The grades variable is mapped to the X axis. The hoursOfStudy variable is mapped to the Y axis. The graph is created using the following code:\nlibrary(ggplot2)\n\nggplot(data=studentData, aes(x=grades,y=hoursOfStudy)) +\n  geom_point()\nIn this code:",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Creating plots with ggplot2 in R</span>"
    ]
  },
  {
    "objectID": "ggplot.html#the-grammar-of-visualisation-with-ggplot",
    "href": "ggplot.html#the-grammar-of-visualisation-with-ggplot",
    "title": "11  Creating plots with ggplot2 in R",
    "section": "",
    "text": "We specify the dataset and the variables for the X and Y axes.\nWe specify the geom that will represent the data points visually (in this case, each datum is a point).\n\n\n\n\n\n\n\n\nImportantRemember the + operator\n\n\n\n\nThe layers of a ggplot are separated using the + operator. That is, we first specify the dataset and the variables for the X and Y axes, and then we add a +, then the geom that will represent the data points visually.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Creating plots with ggplot2 in R</span>"
    ]
  },
  {
    "objectID": "ggplot.html#adding-more-variables-to-a-plot",
    "href": "ggplot.html#adding-more-variables-to-a-plot",
    "title": "11  Creating plots with ggplot2 in R",
    "section": "11.2 Adding more variables to a plot",
    "text": "11.2 Adding more variables to a plot\nOne of the things that makes ggplot so powerful is that you can add more variables to a plot. You can map these variables to different aesthetic attributes of the plot, such as colour, shape, or size.\nThis can be done either in the ggplot() function or in the geom_point() function.\n\n11.2.1 Example: assigning a variable to colour (2 different approaches)\n\n1ggplot(data=studentData, aes(x=grades,y=hoursOfStudy, color = route)) +\n2  geom_point()\n\n\n1\n\nThe color aesthetic is mapped to the route variable in the main ggplot() function.\n\n2\n\nThe geom_point() function is used to represent the data points visually and it takes its colour from the main ggplot() function.\n\n\n\n\n\n\n\n\n\n\n\n\n1ggplot(data=studentData, aes(x=grades,y=hoursOfStudy)) +\n2  geom_point(aes(color = route))\n\n\n1\n\nThe color aesthetic is not mentioned in the main ggplot() function.\n\n2\n\nThe color aesthetic is mapped to the route variable in the geom_point() function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n11.2.2 Example: assigning a variable to size\n\n1ggplot(data=studentData, aes(x=grades,y=hoursOfStudy, size = satisfactionLevel, colour = route)) +\n2  geom_point()\n\n\n1\n\nThe size aesthetic is mapped to the satisfactionLevel variable in the main ggplot() function.\n\n2\n\nThe geom_point() function is used to represent the data points visually and it takes its size from the main ggplot() function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n11.2.3 Example: assigning a variable to shape\n\n1ggplot(data=studentData, aes(x=grades,y=hoursOfStudy, size = satisfactionLevel, colour = route, shape = hasDepdendants)) +\n2  geom_point()\n\n\n1\n\nThe shape aesthetic is mapped to the hasDepdendants variable in the main ggplot() function.\n\n2\n\nThe geom_point() function is used to represent the data points visually and it takes its shape from the main ggplot() function.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Creating plots with ggplot2 in R</span>"
    ]
  },
  {
    "objectID": "ggplot.html#different-types-of-geoms-bar-charts",
    "href": "ggplot.html#different-types-of-geoms-bar-charts",
    "title": "11  Creating plots with ggplot2 in R",
    "section": "11.3 Different types of geoms: bar charts",
    "text": "11.3 Different types of geoms: bar charts\nThe geom_point() function is just one of many geoms that you can use in ggplot. Another common geom is geom_col(), which is used to create bar charts.\nWhen creating a bar chart there are 2 approaches:\n\nsummarise the data before plotting\nlet ggplot do the summarising for you\n\n\n11.3.1 Example: summarising the data before plotting\n\nlibrary(dplyr)\n\nstudentData %&gt;%\n  group_by(route) %&gt;%\n  summarise(meanHours = mean(hoursOfStudy)) %&gt;%\n  ggplot(aes(x = route, y = meanHours)) +\n  geom_col()\n\n\n\n\n\n\n\n\nIn the above code, we first use the group_by() and summarise() functions from the dplyr package to calculate the mean number of hours of study for each route (see previous lessons for this). We then use the ggplot() function to create a plot with the route variable on the X axis and the meanHours variable on the Y axis. Finally, we use the geom_col() function to create the bar chart.\n\n\n\n\n\n\nImportantRemember to refer to the summarised variable in the ggplot() function\n\n\n\nWhen you summarise the data, remember to refer to the summarised variable in the ggplot() function, not the original variable. In the above example, we use meanHours in the ggplot() function, not hoursOfStudy.\nIf you find it easier, you can save the summarised data as a new object and then use this in the ggplot() function.\n\n\n\n\n11.3.2 Example: letting ggplot do the summarising for you\n\nggplot(data=studentData, aes(x = route, y = hoursOfStudy)) + \n  stat_summary(fun = mean, geom = \"col\")\n\n\n\n\n\n\n\n\nIn the above code, we use the stat_summary() function to calculate the mean number of hours of study for each route. We specify that we want to calculate the mean using the fun = mean argument, and we specify that we want to create a bar chart using the geom = \"col\" argument.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Creating plots with ggplot2 in R</span>"
    ]
  },
  {
    "objectID": "ggplot.html#changing-the-titles-and-legends-of-a-plot",
    "href": "ggplot.html#changing-the-titles-and-legends-of-a-plot",
    "title": "11  Creating plots with ggplot2 in R",
    "section": "11.4 Changing the titles and legends of a plot",
    "text": "11.4 Changing the titles and legends of a plot\nYou can change the titles and legends of a plot using the labs() function. This function allows you to change the title of the plot, the titles of the X and Y axes, and the legend title.\n\nstudentData %&gt;%\n  group_by(route) %&gt;%\n  summarise(meanHours = mean(hoursOfStudy)) %&gt;%\n  ggplot(aes(x = route, y = meanHours)) +\n  geom_col() +\n  labs(title = \"Mean hours of study by route\",\n       x = \"Route\",\n       y = \"Mean hours of study\",\n       caption = \"Source: Student data\")\n\n\n\n\n\n\n\n\nIn the above code, we use the labs() function to change the title of the plot to “Mean hours of study by route”, the title of the X axis to “Route”, the title of the Y axis to “Mean hours of study”, and the caption to “Source: Student data”.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Creating plots with ggplot2 in R</span>"
    ]
  },
  {
    "objectID": "ggplot.html#themes-in-ggplot",
    "href": "ggplot.html#themes-in-ggplot",
    "title": "11  Creating plots with ggplot2 in R",
    "section": "11.5 Themes in ggplot",
    "text": "11.5 Themes in ggplot\nYou can change all of the colours and other appearance elements in ggplot. However, it can be easier to change the appearance of a plot using themes. Themes allow you to change the background colour, the colour of the text, the size of the text, and other aspects of the appearance of the plot.\n\nstudentData %&gt;%\n  group_by(route) %&gt;%\n  summarise(meanHours = mean(hoursOfStudy)) %&gt;%\n  ggplot(aes(x = route, y = meanHours)) +\n  geom_col() +\n  labs(title = \"Mean hours of study by route\",\n       x = \"Route\",\n       y = \"Mean hours of study\",\n       caption = \"Source: Student data\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn the above code, we use the theme_minimal() function to change the appearance of the plot to a minimal theme. There are many other themes that you can use, such as theme_light(), theme_dark(), and theme_bw().",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Creating plots with ggplot2 in R</span>"
    ]
  },
  {
    "objectID": "ggplot.html#saving-plots-in-ggplot",
    "href": "ggplot.html#saving-plots-in-ggplot",
    "title": "11  Creating plots with ggplot2 in R",
    "section": "11.6 Saving plots in ggplot",
    "text": "11.6 Saving plots in ggplot\nThe best way to save plots (and get reliable results) is to save them using the ggsave() function. This function allows you to save plots in a variety of formats, such as PDF, PNG, and JPEG. For publication-quality plots, it is best to save them in PDF format, because this format is scalable and resolution independent.\n\nstudentData %&gt;%\n  group_by(route) %&gt;%\n  summarise(meanHours = mean(hoursOfStudy)) %&gt;%\n  ggplot(aes(x = route, y = meanHours)) +\n  geom_col() +\n  labs(title = \"Mean hours of study by route\",\n       x = \"Route\",\n       y = \"Mean hours of study\",\n       caption = \"Source: Student data\") +\n  theme_minimal()\n\nggsave(\"studentData_plot.pdf\", width = 6, height = 4, dpi = 300)\n\nIn the above code, we use the ggsave() function to save the plot as a PDF file called “studentData_plot.pdf”. We specify the width of the plot as 6 inches, the height of the plot as 4 inches, and the resolution as 300 dots per inch (dpi). dpi is not actually used for PDF files, but it is useful for other file formats, such as PNG and eps.\nIn terms of specifying the width and height of the plot, it can take some fiddling around to get the right dimensions. You can specify the width and height in pixels, but it is generally better to specify them in inches, because this is a more standard unit of measurement for plots. I recommend thinking in terms of the aspect ratio of the plot, rather than the exact dimensions. For example, if you want a wide plot, you might specify the width as 6 inches and the height as 4 inches. If you want a square plot, you might specify the width and height as 4 inches each. If you want a tall plot, you might specify the width as 4 inches and the height as 6 inches.\nOnce the aspect ratio feels right, you can adjust the width and height to get the exact right dimensions for your plot.\n\n\n\n\n\n\nImportantPlots will save into the working directory\n\n\n\nWhen you save a plot using the ggsave() function, the plot will be saved into the working directory. You can specify a different directory by specifying the full path to the file, such as “C:/Users/username/Documents/studentData_plot.pdf”.\nIt’s easier just to set the working directory to the folder where you want to save the plots!",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Creating plots with ggplot2 in R</span>"
    ]
  },
  {
    "objectID": "ggplot.html#summary",
    "href": "ggplot.html#summary",
    "title": "11  Creating plots with ggplot2 in R",
    "section": "11.7 Summary",
    "text": "11.7 Summary\nIn this chapter, you have learned how to create plots with ggplot in R. You have learned about the “grammar of visualisation” with ggplot, including the dataset, the coordinate system, and the visual marks that represent the data. You have learned how to write a graph function to display multiple variables on a plot, how to amend the titles and legends of a plot, and how to save plots in PDF or image formats.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Creating plots with ggplot2 in R</span>"
    ]
  }
]